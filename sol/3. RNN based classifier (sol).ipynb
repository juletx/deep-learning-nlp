{"cells":[{"cell_type":"markdown","metadata":{"id":"YxcogNx1spwp"},"source":["# Lab3: Sentiment, but slower!"]},{"cell_type":"markdown","metadata":{"id":"w7huCqsRspww"},"source":["In this assignment, you'll implement an **RNN-based sentence classifier**. Plain ol' RNNs aren't very good at sentiment classification, and they're very picky about things like learning rates. However, they're the foundation for things like LSTMs, which we'll learn about next week, and which *are* quite useful."]},{"cell_type":"markdown","metadata":{"id":"tcEY8ElAspw4"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"v2LXIRspspw_"},"source":["First, let's load the data as before."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22652,"status":"ok","timestamp":1642427153304,"user":{"displayName":"Ander Barrena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjgRxXTKl_X28G_aTFKEgcKsBAE_r5TE9B5ziwl2w=s64","userId":"06771498905541655767"},"user_tz":-60},"id":"MvjTs7K1t17g","outputId":"909ac50a-1f55-4345-fe4c-6a7f378ff4b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4070,"status":"ok","timestamp":1642427157368,"user":{"displayName":"Ander Barrena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjgRxXTKl_X28G_aTFKEgcKsBAE_r5TE9B5ziwl2w=s64","userId":"06771498905541655767"},"user_tz":-60},"id":"WWRmPqvTspxB","outputId":"515c3230-073c-4792-b060-a89816bbe3b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training size: 6920\n","Dev size: 872\n","Test size: 1821\n"]}],"source":["import re\n","import random\n","\n","# Let's do 2-way positive/negative classification instead of 5-way\n","def load_sst_data(path,\n","                  easy_label_map={0:0, 1:0, 2:None, 3:1, 4:1}):\n","    data = []\n","    with open(path) as f:\n","        for i, line in enumerate(f): \n","            example = {}\n","            example['label'] = easy_label_map[int(line[1])]\n","            if example['label'] is None:\n","                continue\n","            \n","            # Strip out the parse information and the phrase labels--\n","            # ---we don't need those here\n","            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n","            example['text'] = text[1:]\n","            data.append(example)\n","    \n","    random.seed(1)\n","    random.shuffle(data)\n","    return data\n","   \n","sst_home = 'drive/My Drive/Colab Notebooks/dl4nlp_labs/data/trees/'\n","training_set = load_sst_data(sst_home + '/train.txt')\n","dev_set = load_sst_data(sst_home + '/dev.txt')\n","test_set = load_sst_data(sst_home + '/test.txt')\n","\n","print('Training size: {}'.format(len(training_set)))\n","print('Dev size: {}'.format(len(dev_set)))\n","print('Test size: {}'.format(len(test_set)))"]},{"cell_type":"markdown","metadata":{"id":"vaoqE0D7spxL"},"source":["Next, we'll convert the data to __index vectors__.\n","\n","To simplify your implementation, we'll use a __fixed unrolling length of 20__. In the conversion process, we'll cut off excess words (towards the left/start end of the sentence), pad short sentences (to the left) with a special word symbol `<PAD>`, and mark out-of-vocabulary words with `<UNK>`, for unknown. As in the previous assignment, we'll use a very small vocabulary for this assignment, so you'll see `<UNK>` often."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L7d0AMI6spxQ"},"outputs":[],"source":["import collections\n","import numpy as np\n","\n","def sentence_to_padded_index_sequence(datasets):\n","    '''Annotates datasets with feature vectors.'''\n","    \n","    PADDING = \"<PAD>\"\n","    UNKNOWN = \"<UNK>\"\n","    SEQ_LEN = 20\n","    \n","    # Extract vocabulary\n","    def tokenize(string):\n","        return string.lower().split()\n","    \n","    word_counter = collections.Counter()\n","    for example in datasets[0]:\n","        word_counter.update(tokenize(example['text']))\n","    \n","    vocabulary = set([word for word in word_counter if word_counter[word] > 10])\n","    vocabulary = list(vocabulary)\n","    vocabulary = [PADDING, UNKNOWN] + vocabulary\n","        \n","    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n","    indices_to_words = {v: k for k, v in word_indices.items()}\n","        \n","    for i, dataset in enumerate(datasets):\n","        for example in dataset:\n","            example['index_sequence'] = np.zeros((SEQ_LEN), dtype=np.int32)\n","            \n","            token_sequence = tokenize(example['text'])\n","            padding = SEQ_LEN - len(token_sequence)\n","            \n","            for i in range(SEQ_LEN):\n","                if i >= padding:\n","                    if token_sequence[i - padding] in word_indices:\n","                        index = word_indices[token_sequence[i - padding]]\n","                    else:\n","                        index = word_indices[UNKNOWN]\n","                else:\n","                    index = word_indices[PADDING]\n","                example['index_sequence'][i] = index\n","    return indices_to_words, word_indices\n","    \n","indices_to_words, word_indices = sentence_to_padded_index_sequence([training_set, dev_set, test_set])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1642427157370,"user":{"displayName":"Ander Barrena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjgRxXTKl_X28G_aTFKEgcKsBAE_r5TE9B5ziwl2w=s64","userId":"06771498905541655767"},"user_tz":-60},"id":"Apdx7iJospxW","outputId":"0b42f546-80bb-4e54-b7f2-1efee38e30b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'label': 1, 'text': 'As the dominant Christine , Sylvie Testud is icily brilliant .', 'index_sequence': array([   0,    0,    0,    0,    0,    0,    0,    0,    0, 1008,  647,\n","          1,    1,  402,    1,    1,  617,    1,  288,  322], dtype=int32)}\n","1250\n"]}],"source":["print (training_set[18])\n","print (len(word_indices))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0_qTijrGspxf"},"outputs":[],"source":["def evaluate_classifier(classifier, eval_set):\n","    correct = 0\n","    hypotheses = classifier(eval_set)\n","    for i, example in enumerate(eval_set):\n","        hypothesis = hypotheses[i]\n","        if hypothesis == example['label']:\n","            correct += 1        \n","    return correct / float(len(eval_set))"]},{"cell_type":"markdown","metadata":{"id":"cLcqDcjvspxm"},"source":["## Assignments: Building the RNN"]},{"cell_type":"markdown","metadata":{"id":"_uC9N01Mspxq"},"source":["Replace the TODOs in the code below to make RNN work. If it's set up properly, it should reach dev set accuracy of about 0.7 within 500 epochs with the given hyperparameters.\n","\n","You will find 3 TODOs in the code.\n","\n","### TODO 1:\n","\n","- You have to define the RNN parameters (attribute *self.dim* sets dimmension of hidden state). \n","\n","- (Hint) The paremters take input's embedding (*self.embedding_dim*) and the previous hidden state (*self.dim*) and provides the current hidden state (*self.dim*).\n","\n","### TODO 2:\n","\n","- Write a (very short) Python function that defines one step of an RNN. (Hint) In each step current input and previous hidden states are involved. \n","\n","- Recall from slides: $f(h_{t-1}, p_t) = tanh(W[h_{t-1};p_t])$. Note that input $x$ at time step $t$ is *translated* to its embedding representation. \n","\n","\n","![](https://drive.google.com/uc?id=1VNI--El3renuefGD0R7AOlcxI4ycLj4V)\n","\n","\n","### TODO 3:\n","\n","- Unroll the RNN using a *for* loop, and obtain the sentence representation with the final hidden state.\n","\n","- (Hint) Note that we are vectorizing the whole minibatch. That is, in each step we are processing all the examples in the batch together in one go. Try to understand the following two code lines:\n","\n","   $\\rightarrow$ ``self.x_slices = tf.split(self.x, self.sequence_length, 1)``\n","   \n","   $\\rightarrow$ ``self.h_zero = tf.zeros([self.batch_size, self.dim])``\n","   \n","- (Hint) It might be a good idea to reshape (tf.reshape) the tensor at step t in a single tensor. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":2537,"status":"ok","timestamp":1642427159900,"user":{"displayName":"Ander Barrena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjgRxXTKl_X28G_aTFKEgcKsBAE_r5TE9B5ziwl2w=s64","userId":"06771498905541655767"},"user_tz":-60},"id":"tG8H7bYgspxw","outputId":"6fdbbd85-0129-4898-b2be-f47303e1f3b0"},"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.7.0'"]},"metadata":{},"execution_count":6}],"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","tf.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"piE_Cr6zspx6"},"outputs":[],"source":["class RNNSentimentClassifier:\n","    def __init__(self, vocab_size, sequence_length):\n","        # Define the hyperparameters\n","        self.learning_rate = 0.2  # Should be about right\n","        self.training_epochs = 500  # How long to train for - chosen to fit within class time\n","        self.display_epoch_freq = 5  # How often to test and print out statistics\n","        self.dim = 24  # The dimension of the hidden state of the RNN\n","        self.embedding_dim = 8  # The dimension of the learned word embeddings\n","        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n","        self.vocab_size = vocab_size  # Defined by the file reader above\n","        self.sequence_length = sequence_length  # Defined by the file reader above\n","        self.l2_lambda = 0.001\n","        \n","        self.trainable_variables = []\n","\n","        # Define the parameters\n","        self.E = tf.Variable(tf.random.normal([self.vocab_size, self.embedding_dim], stddev=0.1))\n","        self.trainable_variables.append(self.E)\n","        \n","        self.W_cl = tf.Variable(tf.random.normal([self.dim, 2], stddev=0.1))\n","        self.b_cl = tf.Variable(tf.random.normal([2], stddev=0.1))\n","        self.trainable_variables.append(self.W_cl)\n","        self.trainable_variables.append(self.b_cl)\n","        \n","        # TODO 1: Define the RNN parameters\n","        self.W_rnn = tf.Variable(tf.random.normal([self.dim+self.embedding_dim, self.dim], stddev=0.1))\n","        self.b_rnn = tf.Variable(tf.random.normal([self.dim], stddev=0.1))\n","        self.trainable_variables.append(self.W_rnn)\n","        self.trainable_variables.append(self.b_rnn)\n","        \n","        # Define the placeholders\n","        #self.x = tf.placeholder(tf.int32, [None, self.sequence_length])\n","        #self.y = tf.placeholder(tf.int32, [None])\n","    \n","    def model(self,x):\n","        # Split up the inputs into individual tensors\n","        # print(tf.shape(x)) (256,20)\n","        self.x_slices = tf.split(x, self.sequence_length, 1)\n","        # print(tf.shape(self.x_slices)) (20,256,1)\n","\n","        # Define the start state of the RNN\n","        self.h_zero = tf.zeros([self.batch_size, self.dim])\n","        # print(tf.shape(self.h_zero)) (256,24)\n","        \n","        # TODO 2: Write a (very short) Python function that defines one step of an RNN\n","        def step(x, h_prev):\n","            # print(tf.shape(x)) (256)\n","            # print(tf.shape(h_prev)) (256,24)\n","            emb = tf.nn.embedding_lookup(params=self.E,ids=x)\n","            # print(tf.shape(emb)) (256,8)\n","            conc = tf.concat([emb,h_prev],1)\n","            # print(tf.shape(conc)) (256,32)\n","            h = tf.tanh(tf.matmul(conc,self.W_rnn)+self.b_rnn)\n","            # print(tf.shape(h)) (256,24)\n","            return h\n","        \n","        # TODO 3: Unroll the RNN using a for loop, and obtain the sentence representation with the final hidden state        \n","        h_prev = self.h_zero\n","        for i in range(self.sequence_length):\n","            # print(tf.shape(self.x_slices[i])) (256,1)\n","            x_t = tf.reshape(self.x_slices[i],[-1])\n","            # x_t = tf.reshape(self.x_slices[i],[tf.shape(x_slices[i])[0],tf.shape(x_slices[i])[2]])\n","            # print(tf.shape(x_t)) (256)\n","            h_prev = step(x_t,h_prev)\n","        sentence_representation = h_prev\n","        # Compute the logits using one last linear layer\n","        logits = tf.matmul(sentence_representation, self.W_cl) + self.b_cl\n","        return logits\n","\n","    def train(self, training_data, dev_set):\n","        def get_minibatch(dataset, start_index, end_index):\n","            indices = range(start_index, end_index)\n","            vectors = np.vstack([dataset[i]['index_sequence'] for i in indices])\n","            labels = [dataset[i]['label'] for i in indices]\n","            return vectors, labels\n","      \n","        print('Training.')\n","\n","        # Training cycle\n","        for epoch in range(self.training_epochs):\n","            random.shuffle(training_set)\n","            avg_cost = 0.\n","            total_batch = int(len(training_set) / self.batch_size)\n","            \n","            # Loop over all batches in epoch\n","            for i in range(total_batch):\n","                # Assemble a minibatch of the next B examples\n","                minibatch_vectors, minibatch_labels = get_minibatch(training_set, \n","                                                                    self.batch_size * i, \n","                                                                    self.batch_size * (i + 1))\n","\n","                # Run the optimizer to take a gradient step, and also fetch the value of the \n","                # cost function for logging\n","                with tf.GradientTape() as tape:\n","                  logits = self.model(minibatch_vectors)\n","                \n","                  # Define the L2 cost\n","                  self.l2_cost = self.l2_lambda * (tf.reduce_sum(tf.square(self.W_rnn)) +\n","                                                  tf.reduce_sum(tf.square(self.W_cl)))\n","\n","                  # Define the cost function (here, the softmax exp and sum are built in)\n","                  total_cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=minibatch_labels, logits=logits) + self.l2_cost)\n","        \n","                # This  performs the main SGD update equation with gradient clipping\n","                optimizer = tf.keras.optimizers.SGD(self.learning_rate)\n","                gradients = tape.gradient(total_cost, self.trainable_variables)\n","                gvs = zip(gradients, self.trainable_variables)\n","                capped_gvs = [(tf.clip_by_norm(grad, 5.0), var) for grad, var in gvs if grad is not None]\n","                optimizer.apply_gradients(capped_gvs)\n","                                                                            \n","                # Compute average loss\n","                avg_cost += total_cost / total_batch\n","                \n","            # Display some statistics about the step\n","            # Evaluating only one batch worth of data -- simplifies implementation slightly\n","            if (epoch+1) % self.display_epoch_freq == 0:\n","                tf.print(\"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \\\n","                    \"Dev acc:\", evaluate_classifier(self.classify, dev_set[0:256]), \\\n","                    \"Train acc:\", evaluate_classifier(self.classify, training_set[0:256]))  \n","    \n","    def classify(self, examples):\n","        # This classifies a list of examples\n","        vectors = np.vstack([example['index_sequence'] for example in examples])\n","        logits = self.model(vectors)\n","        return np.argmax(logits, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"26SBs4iespyE","outputId":"64b0610d-b327-4fa3-86e0-c86eca8a54cc","executionInfo":{"status":"ok","timestamp":1642429434721,"user_tz":-60,"elapsed":939162,"user":{"displayName":"Ander Barrena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjgRxXTKl_X28G_aTFKEgcKsBAE_r5TE9B5ziwl2w=s64","userId":"06771498905541655767"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Training.\n","Epoch: 5 Cost: 0.699208498 Dev acc: 0.5546875 Train acc: 0.54296875\n","Epoch: 10 Cost: 0.698272 Dev acc: 0.5625 Train acc: 0.49609375\n","Epoch: 15 Cost: 0.697723508 Dev acc: 0.5546875 Train acc: 0.53125\n","Epoch: 20 Cost: 0.697201729 Dev acc: 0.5546875 Train acc: 0.58984375\n","Epoch: 25 Cost: 0.696648 Dev acc: 0.5546875 Train acc: 0.52734375\n","Epoch: 30 Cost: 0.696155488 Dev acc: 0.5546875 Train acc: 0.5234375\n","Epoch: 35 Cost: 0.695750654 Dev acc: 0.5625 Train acc: 0.5546875\n","Epoch: 40 Cost: 0.69528085 Dev acc: 0.5546875 Train acc: 0.53515625\n","Epoch: 45 Cost: 0.694870114 Dev acc: 0.5625 Train acc: 0.5390625\n","Epoch: 50 Cost: 0.694575191 Dev acc: 0.56640625 Train acc: 0.52734375\n","Epoch: 55 Cost: 0.694262 Dev acc: 0.55078125 Train acc: 0.5234375\n","Epoch: 60 Cost: 0.693910182 Dev acc: 0.5703125 Train acc: 0.54296875\n","Epoch: 65 Cost: 0.693611681 Dev acc: 0.5546875 Train acc: 0.51171875\n","Epoch: 70 Cost: 0.693315685 Dev acc: 0.5546875 Train acc: 0.5078125\n","Epoch: 75 Cost: 0.693208396 Dev acc: 0.546875 Train acc: 0.5\n","Epoch: 80 Cost: 0.692625105 Dev acc: 0.55078125 Train acc: 0.546875\n","Epoch: 85 Cost: 0.692182302 Dev acc: 0.546875 Train acc: 0.5234375\n","Epoch: 90 Cost: 0.691912353 Dev acc: 0.54296875 Train acc: 0.58984375\n","Epoch: 95 Cost: 0.691490114 Dev acc: 0.546875 Train acc: 0.55078125\n","Epoch: 100 Cost: 0.691019773 Dev acc: 0.49609375 Train acc: 0.484375\n","Epoch: 105 Cost: 0.691022813 Dev acc: 0.55078125 Train acc: 0.53515625\n","Epoch: 110 Cost: 0.690428078 Dev acc: 0.51953125 Train acc: 0.53515625\n","Epoch: 115 Cost: 0.689853847 Dev acc: 0.55078125 Train acc: 0.53125\n","Epoch: 120 Cost: 0.689380884 Dev acc: 0.55859375 Train acc: 0.5234375\n","Epoch: 125 Cost: 0.688539922 Dev acc: 0.53515625 Train acc: 0.5234375\n","Epoch: 130 Cost: 0.687245309 Dev acc: 0.55859375 Train acc: 0.53125\n","Epoch: 135 Cost: 0.686300397 Dev acc: 0.6015625 Train acc: 0.48046875\n","Epoch: 140 Cost: 0.683305442 Dev acc: 0.5859375 Train acc: 0.5625\n","Epoch: 145 Cost: 0.679361 Dev acc: 0.53515625 Train acc: 0.55859375\n","Epoch: 150 Cost: 0.674680889 Dev acc: 0.546875 Train acc: 0.58984375\n","Epoch: 155 Cost: 0.668043613 Dev acc: 0.51953125 Train acc: 0.5859375\n","Epoch: 160 Cost: 0.657648563 Dev acc: 0.56640625 Train acc: 0.6328125\n","Epoch: 165 Cost: 0.656775415 Dev acc: 0.57421875 Train acc: 0.62109375\n","Epoch: 170 Cost: 0.646144927 Dev acc: 0.58984375 Train acc: 0.6015625\n","Epoch: 175 Cost: 0.639371216 Dev acc: 0.578125 Train acc: 0.6328125\n","Epoch: 180 Cost: 0.643181443 Dev acc: 0.578125 Train acc: 0.58203125\n","Epoch: 185 Cost: 0.635439575 Dev acc: 0.56640625 Train acc: 0.5703125\n","Epoch: 190 Cost: 0.631743252 Dev acc: 0.54296875 Train acc: 0.6796875\n","Epoch: 195 Cost: 0.627774417 Dev acc: 0.5390625 Train acc: 0.703125\n","Epoch: 200 Cost: 0.622527301 Dev acc: 0.5 Train acc: 0.59765625\n","Epoch: 205 Cost: 0.616708815 Dev acc: 0.609375 Train acc: 0.6875\n","Epoch: 210 Cost: 0.613937199 Dev acc: 0.6171875 Train acc: 0.72265625\n","Epoch: 215 Cost: 0.620772243 Dev acc: 0.61328125 Train acc: 0.6171875\n","Epoch: 220 Cost: 0.594281375 Dev acc: 0.625 Train acc: 0.6953125\n","Epoch: 225 Cost: 0.606495261 Dev acc: 0.6328125 Train acc: 0.69140625\n","Epoch: 230 Cost: 0.597836196 Dev acc: 0.60546875 Train acc: 0.7578125\n","Epoch: 235 Cost: 0.585219622 Dev acc: 0.5859375 Train acc: 0.703125\n","Epoch: 240 Cost: 0.587043941 Dev acc: 0.65625 Train acc: 0.72265625\n","Epoch: 245 Cost: 0.570151746 Dev acc: 0.5703125 Train acc: 0.66796875\n","Epoch: 250 Cost: 0.573565722 Dev acc: 0.66796875 Train acc: 0.71484375\n","Epoch: 255 Cost: 0.568519175 Dev acc: 0.625 Train acc: 0.74609375\n","Epoch: 260 Cost: 0.563801885 Dev acc: 0.6015625 Train acc: 0.69921875\n","Epoch: 265 Cost: 0.567888856 Dev acc: 0.73046875 Train acc: 0.78515625\n","Epoch: 270 Cost: 0.550542057 Dev acc: 0.64453125 Train acc: 0.7578125\n","Epoch: 275 Cost: 0.552043796 Dev acc: 0.625 Train acc: 0.78125\n","Epoch: 280 Cost: 0.551811695 Dev acc: 0.61328125 Train acc: 0.78515625\n","Epoch: 285 Cost: 0.535427749 Dev acc: 0.625 Train acc: 0.7578125\n","Epoch: 290 Cost: 0.543756247 Dev acc: 0.6484375 Train acc: 0.74609375\n","Epoch: 295 Cost: 0.539776146 Dev acc: 0.58984375 Train acc: 0.66015625\n","Epoch: 300 Cost: 0.530901968 Dev acc: 0.60546875 Train acc: 0.74609375\n","Epoch: 305 Cost: 0.53297472 Dev acc: 0.671875 Train acc: 0.8125\n","Epoch: 310 Cost: 0.519183517 Dev acc: 0.65234375 Train acc: 0.765625\n","Epoch: 315 Cost: 0.518166423 Dev acc: 0.5546875 Train acc: 0.72265625\n","Epoch: 320 Cost: 0.502527416 Dev acc: 0.640625 Train acc: 0.8203125\n","Epoch: 325 Cost: 0.511802912 Dev acc: 0.62109375 Train acc: 0.78515625\n","Epoch: 330 Cost: 0.506900668 Dev acc: 0.5703125 Train acc: 0.7265625\n","Epoch: 335 Cost: 0.513343453 Dev acc: 0.671875 Train acc: 0.80078125\n","Epoch: 340 Cost: 0.499458164 Dev acc: 0.64453125 Train acc: 0.8359375\n","Epoch: 345 Cost: 0.474160194 Dev acc: 0.62109375 Train acc: 0.8125\n","Epoch: 350 Cost: 0.485567123 Dev acc: 0.64453125 Train acc: 0.8203125\n","Epoch: 355 Cost: 0.501259267 Dev acc: 0.609375 Train acc: 0.7109375\n","Epoch: 360 Cost: 0.488469511 Dev acc: 0.640625 Train acc: 0.81640625\n","Epoch: 365 Cost: 0.471523434 Dev acc: 0.65625 Train acc: 0.7578125\n","Epoch: 370 Cost: 0.501855493 Dev acc: 0.61328125 Train acc: 0.76953125\n","Epoch: 375 Cost: 0.469405562 Dev acc: 0.5703125 Train acc: 0.74609375\n","Epoch: 380 Cost: 0.44187519 Dev acc: 0.65234375 Train acc: 0.85546875\n","Epoch: 385 Cost: 0.4936845 Dev acc: 0.609375 Train acc: 0.74609375\n","Epoch: 390 Cost: 0.454760462 Dev acc: 0.65625 Train acc: 0.83203125\n","Epoch: 395 Cost: 0.483965456 Dev acc: 0.60546875 Train acc: 0.69140625\n","Epoch: 400 Cost: 0.474416316 Dev acc: 0.58984375 Train acc: 0.74609375\n","Epoch: 405 Cost: 0.450934 Dev acc: 0.67578125 Train acc: 0.8671875\n","Epoch: 410 Cost: 0.478355467 Dev acc: 0.6796875 Train acc: 0.84375\n","Epoch: 415 Cost: 0.456589609 Dev acc: 0.65234375 Train acc: 0.84765625\n","Epoch: 420 Cost: 0.427624434 Dev acc: 0.6171875 Train acc: 0.75390625\n","Epoch: 425 Cost: 0.427359 Dev acc: 0.66015625 Train acc: 0.890625\n","Epoch: 430 Cost: 0.416459769 Dev acc: 0.61328125 Train acc: 0.875\n","Epoch: 435 Cost: 0.398227096 Dev acc: 0.6015625 Train acc: 0.7578125\n","Epoch: 440 Cost: 0.405231535 Dev acc: 0.60546875 Train acc: 0.8046875\n","Epoch: 445 Cost: 0.4139373 Dev acc: 0.65625 Train acc: 0.921875\n","Epoch: 450 Cost: 0.385404974 Dev acc: 0.69140625 Train acc: 0.86328125\n","Epoch: 455 Cost: 0.403523803 Dev acc: 0.671875 Train acc: 0.84375\n","Epoch: 460 Cost: 0.378939718 Dev acc: 0.6640625 Train acc: 0.88671875\n","Epoch: 465 Cost: 0.375042528 Dev acc: 0.6640625 Train acc: 0.8828125\n","Epoch: 470 Cost: 0.378449678 Dev acc: 0.5703125 Train acc: 0.85546875\n","Epoch: 475 Cost: 0.39475733 Dev acc: 0.640625 Train acc: 0.82421875\n","Epoch: 480 Cost: 0.374957174 Dev acc: 0.62109375 Train acc: 0.8046875\n","Epoch: 485 Cost: 0.380916834 Dev acc: 0.65625 Train acc: 0.87890625\n","Epoch: 490 Cost: 0.372150481 Dev acc: 0.671875 Train acc: 0.875\n","Epoch: 495 Cost: 0.377899736 Dev acc: 0.66796875 Train acc: 0.890625\n","Epoch: 500 Cost: 0.40041256 Dev acc: 0.65234375 Train acc: 0.86328125\n"]}],"source":["classifier = RNNSentimentClassifier(len(word_indices), 20)\n","classifier.train(training_set, dev_set)"]},{"cell_type":"markdown","metadata":{"id":"r0EeyXVCyjaG"},"source":["# Atribution:\n","Adapted by Oier Lopez de Lacalle, Olatz Perez de Vi√±aspre and Ander Barrena, based on a notebook by Sam Bowman at NYU"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"3. RNN based classifier (sol).ipynb","provenance":[{"file_id":"1N3d6R4lnmdNEIh3VWdAAOeP6bQc0kSbQ","timestamp":1545915164296},{"file_id":"1K4mLe1n4WyLsDu43z3dAQlYxTec9TENC","timestamp":1545914730104}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}