{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. MLP and Dropout (sol).ipynb","provenance":[{"file_id":"1mkhICIQba8Oz0Iu3FKw1EGZTIzdFHsXv","timestamp":1541614347511}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"HzzMSlqjSGBg"},"source":["# Lab2: MLPs and Dropout"]},{"cell_type":"markdown","metadata":{"id":"oJof5oljSGBk"},"source":["First, let's load the data as before."]},{"cell_type":"code","metadata":{"id":"e8fdLvE4Ygyq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641463158523,"user_tz":-60,"elapsed":13734,"user":{"displayName":"Ander Barrena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjgRxXTKl_X28G_aTFKEgcKsBAE_r5TE9B5ziwl2w=s64","userId":"06771498905541655767"}},"outputId":"ecabbc72-b3b3-4034-b715-62be796e2391"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"OoRAgDVzSGBn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641463159909,"user_tz":-60,"elapsed":1391,"user":{"displayName":"Ander Barrena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjgRxXTKl_X28G_aTFKEgcKsBAE_r5TE9B5ziwl2w=s64","userId":"06771498905541655767"}},"outputId":"ba93bfee-2bb0-44ff-e9ab-a657e4166807"},"source":["# Load the data\n","import re\n","import random\n","\n","# Let's do 2-way positive/negative classification instead of 5-way    \n","def load_sst_data(path,\n","                  easy_label_map={0:0, 1:0, 2:None, 3:1, 4:1}):\n","    data = []\n","    with open(path) as f:\n","        for i, line in enumerate(f): \n","            example = {}\n","            example['label'] = easy_label_map[int(line[1])]\n","            if example['label'] is None:\n","                continue\n","            \n","            # Strip out the parse information and the phrase labels--\n","            # ---we don't need those here\n","            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n","            example['text'] = text[1:]\n","            data.append(example)\n","    random.seed(1)\n","    random.shuffle(data)\n","    return data\n","\n","sst_home = 'drive/My Drive/Colab Notebooks/dl4nlp_labs/data/trees/'\n","training_set = load_sst_data(sst_home + 'train.txt')\n","dev_set = load_sst_data(sst_home + 'dev.txt')\n","test_set = load_sst_data(sst_home + 'test.txt')\n","\n","print('Training size: {}'.format(len(training_set)))\n","print('Dev size: {}'.format(len(dev_set)))\n","print('Test size: {}'.format(len(test_set)))"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Training size: 6920\n","Dev size: 872\n","Test size: 1821\n"]}]},{"cell_type":"markdown","metadata":{"id":"RvlDOvE9SGBy"},"source":["And extract bag-of-words feature vectors. For speed, we'll only use words that appear at least 10 times in the training set, leaving us with $|V|=1254$."]},{"cell_type":"code","metadata":{"id":"ZzJYCAobSGB0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641463160273,"user_tz":-60,"elapsed":366,"user":{"displayName":"Ander Barrena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjgRxXTKl_X28G_aTFKEgcKsBAE_r5TE9B5ziwl2w=s64","userId":"06771498905541655767"}},"outputId":"68d4ac34-65e3-42a1-f01c-8832d2db08ee"},"source":["import collections\n","import numpy as np\n","\n","def feature_function(datasets):\n","    '''Annotates datasets with feature vectors.'''\n","    \n","    # Extract vocabulary\n","    def tokenize(string):\n","        return string.split()\n","    \n","    word_counter = collections.Counter()\n","    for example in datasets[0]:\n","        word_counter.update(tokenize(example['text']))\n","    \n","    vocabulary = set([word for word in word_counter if word_counter[word] > 10])\n","                                \n","    feature_names = set()\n","    for i, dataset in enumerate(datasets):\n","        for example in dataset:\n","            example['features'] = collections.defaultdict(float)\n","            \n","            # Extract features (by name) for one example\n","            word_counter = collections.Counter(tokenize(example['text']))\n","            for x in word_counter.items():\n","                if x[0] in vocabulary:\n","                    example[\"features\"][\"word_count_for_\" + x[0]] = x[1]\n","            \n","            feature_names.update(example['features'].keys())\n","                            \n","    # By now, we know what all the features will be, so we can\n","    # assign indices to them.\n","    feature_indices = dict(zip(feature_names, range(len(feature_names))))\n","    indices_to_features = {v: k for k, v in feature_indices.items()}\n","    dim = len(feature_indices)\n","                \n","    # Now we create actual vectors from those indices.\n","    for dataset in datasets:\n","        for example in dataset:\n","            example['vector'] = np.zeros((dim))\n","            for feature in example['features']:\n","                example['vector'][feature_indices[feature]] = example['features'][feature]\n","    return indices_to_features, dim\n","    \n","indices_to_features, dim = feature_function([training_set, dev_set, test_set])\n","\n","print('Vocabulary size: {}'.format(dim))\n","\n"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary size: 1254\n"]}]},{"cell_type":"markdown","metadata":{"id":"gUlD7zgkSGB7"},"source":["And define a batch evalution function."]},{"cell_type":"code","metadata":{"id":"A533GAkvSGB9","executionInfo":{"status":"ok","timestamp":1641463160274,"user_tz":-60,"elapsed":6,"user":{"displayName":"Ander Barrena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjgRxXTKl_X28G_aTFKEgcKsBAE_r5TE9B5ziwl2w=s64","userId":"06771498905541655767"}}},"source":["def evaluate_classifier(classifier, eval_set):\n","    correct = 0\n","    hypotheses = classifier(eval_set)\n","    for i, example in enumerate(eval_set):\n","        hypothesis = hypotheses[i]\n","        if hypothesis == example['label']:\n","            correct += 1        \n","    return correct / float(len(eval_set))"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H0Qn58-YSGCD"},"source":["## Assignments\n","\n","Now for the fun part! The below should be a working implementation of logistic regression in TensorFlow.\n","\n","### Part One:\n","\n","Modify it to turn it into an MLP with two ReLU hidden layers of 50 dimensions.\n","\n","Keep in mind that initializing weight matrices with zeros causes problems in deep neural networks trained by SGD. (Why?) You should use tf.random_normal instead, with stddev=0.1.\n","\n","If your model works, it should be able to overfit, reaching about 90% accuracy *on the training set* in the first 100 epochs.\n","\n","### Part Two:\n","\n","After each hidden layer, add dropout with a 80% keep rate (20% drop rate). You're welcome to use `tf.nn.dropout`.\n","\n","Remember that dropout behaves differently at training time and at test time. This is not automatic. You can implement in various ways, but an easy way can be this:\n","\n","- Hint: Treat the keep rate as an input to the model, just like `x`. At training time, feed it a value of `0.2`, at test time, feed it a value of `0.0`. You can explore different dropout values.\n","\n","If dropout works, your model should overfit less, but should still perform about as well (or, hopefully, better) on the dev set."]},{"cell_type":"code","metadata":{"id":"CMJeulPiSGCF","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1641463163291,"user_tz":-60,"elapsed":3022,"user":{"displayName":"Ander Barrena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjgRxXTKl_X28G_aTFKEgcKsBAE_r5TE9B5ziwl2w=s64","userId":"06771498905541655767"}},"outputId":"cc0fc143-0dc1-4cc9-cfe1-0108fb4c2889"},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","tf.__version__"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.7.0'"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"EyS-IkAYSGCL","executionInfo":{"status":"ok","timestamp":1641463248875,"user_tz":-60,"elapsed":232,"user":{"displayName":"Ander Barrena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjgRxXTKl_X28G_aTFKEgcKsBAE_r5TE9B5ziwl2w=s64","userId":"06771498905541655767"}}},"source":["class logistic_regression_classifier:\n","    def __init__(self, dim):\n","        # Define the hyperparameters\n","        self.learning_rate = 0.3  # Should be about right\n","        self.training_epochs = 100  # How long to train for - chosen to fit within class time\n","        self.display_epoch_freq = 1  # How often to test and print out statistics\n","        self.dim = dim  # The number of features\n","        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n","        \n","        # TODO: Use these.\n","        self.hidden_layer_sizes = [50, 50]\n","        self.rate = 0.2\n","\n","        # TODO: Overwrite this section\n","        ### Start of model definition ###\n","\n","        self.trainable_variables = []\n","         # Define (most of) the model\n","        '''Variables'''\n","        #Hidden RELU layer 0\n","        self.W0 = tf.Variable(tf.random.normal([self.dim, self.hidden_layer_sizes[0]],stddev=0.1), dtype='float32')\n","        self.b0 = tf.Variable(tf.zeros([self.hidden_layer_sizes[0]]), dtype='float32')\n","        self.trainable_variables.append(self.W0)\n","        self.trainable_variables.append(self.b0)\n","        \n","        #Hidden RELU layer 1\n","        self.W1 = tf.Variable(tf.random.normal([self.hidden_layer_sizes[0],self.hidden_layer_sizes[1]],stddev=0.1), dtype='float32')\n","        self.b1 = tf.Variable(tf.zeros([self.hidden_layer_sizes[1]]), dtype='float32')\n","        self.trainable_variables.append(self.W1)\n","        self.trainable_variables.append(self.b1)\n","        \n","        #Output layer\n","        self.W2 = tf.Variable(tf.random.normal([self.hidden_layer_sizes[1], 2],stddev=0.1), dtype='float32')\n","        self.b2 = tf.Variable(tf.zeros([2]), dtype='float32')\n","        self.trainable_variables.append(self.W2)\n","        self.trainable_variables.append(self.b2)\n","\n","    def model(self,x,rate):\n","        '''Training Computation'''\n","        #Hidden RELU layer 0 activation\n","        self.logits0 = tf.matmul(x, self.W0) + self.b0\n","        self.h0 = tf.nn.relu(self.logits0)\n","        self.h0 = tf.nn.dropout(self.h0,rate)\n","        \n","        #Hidden RELU layer 1 activation\n","        self.logits1 = tf.matmul(self.h0, self.W1) + self.b1\n","        self.h1 = tf.nn.relu(self.logits1)\n","        self.h1 = tf.nn.dropout(self.h1,rate)\n","        \n","        #Output layer activation\n","        logits = tf.matmul(self.h1, self.W2) + self.b2\n","        ### End of model definition ###\n","        return logits\n","     \n","\n","    def train(self, training_data, dev_set):\n","        def get_minibatch(dataset, start_index, end_index):\n","            indices = range(start_index, end_index)\n","            vectors = np.float32(np.vstack([dataset[i]['vector'] for i in indices]))\n","            labels = [dataset[i]['label'] for i in indices]\n","            return vectors, labels\n","      \n","        print ('Training.')\n","\n","        # Training cycle\n","        for epoch in range(self.training_epochs):\n","            random.shuffle(training_set)\n","            avg_cost = 0.\n","            total_batch = int(len(training_set) / self.batch_size)\n","            \n","            # Loop over all batches in epoch\n","            for i in range(total_batch):\n","                # Assemble a minibatch of the next B examples\n","                minibatch_vectors, minibatch_labels = get_minibatch(training_set, \n","                                                                    self.batch_size * i, \n","                                                                    self.batch_size * (i + 1))\n","\n","                # Run the optimizer to take a gradient step, and also fetch the value of the \n","                # cost function for logging\n","                with tf.GradientTape() as tape:\n","                  logits = self.model(minibatch_vectors,self.rate)\n","                  # Define the cost function (here, the exp and sum are built in)\n","                  cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=minibatch_labels))\n","                  #cost = tf.keras.losses.SparseCategoricalCrossentropy(logits,minibatch_labels,from_logits=True)\n","                  \n","                # Optionally you could add L2 regularization term\n","        \n","                # This library call performs the main SGD update equation\n","                #self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.cost)\n","                \n","                gradients = tape.gradient(cost, self.trainable_variables)\n","                optimizer = tf.keras.optimizers.SGD(self.learning_rate)\n","                optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n","   \n","                # Compute average loss\n","                avg_cost += cost / total_batch\n","                \n","                # Display some statistics about the step\n","            if (epoch+1) % self.display_epoch_freq == 0:\n","                tf.print (\"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \\\n","                    \"Dev acc:\", evaluate_classifier(self.classify, dev_set[0:500]), \\\n","                    \"Train acc:\", evaluate_classifier(self.classify, training_set[0:500]))\n","    \n","    def classify(self, examples):\n","        # This classifies a list of examples\n","        vectors = np.float32(np.vstack([example['vector'] for example in examples]))\n","        logits = self.model(vectors,0.0)\n","        return np.argmax(logits, axis=1)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sU7c7S6rSGCT"},"source":["Now let's train it."]},{"cell_type":"code","metadata":{"id":"kymCD3LkSGCW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641463295679,"user_tz":-60,"elapsed":43951,"user":{"displayName":"Ander Barrena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjgRxXTKl_X28G_aTFKEgcKsBAE_r5TE9B5ziwl2w=s64","userId":"06771498905541655767"}},"outputId":"c8108a13-0472-4495-db29-5b691500426b"},"source":["classifier = logistic_regression_classifier(dim)\n","classifier.train(training_set, dev_set)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Training.\n","Epoch: 1 Cost: 0.692445576 Dev acc: 0.562 Train acc: 0.584\n","Epoch: 2 Cost: 0.686476648 Dev acc: 0.602 Train acc: 0.586\n","Epoch: 3 Cost: 0.679325342 Dev acc: 0.636 Train acc: 0.596\n","Epoch: 4 Cost: 0.670828223 Dev acc: 0.63 Train acc: 0.618\n","Epoch: 5 Cost: 0.667153478 Dev acc: 0.642 Train acc: 0.624\n","Epoch: 6 Cost: 0.657815397 Dev acc: 0.66 Train acc: 0.636\n","Epoch: 7 Cost: 0.654222 Dev acc: 0.668 Train acc: 0.698\n","Epoch: 8 Cost: 0.642935634 Dev acc: 0.692 Train acc: 0.666\n","Epoch: 9 Cost: 0.633350194 Dev acc: 0.682 Train acc: 0.676\n","Epoch: 10 Cost: 0.62144649 Dev acc: 0.692 Train acc: 0.676\n","Epoch: 11 Cost: 0.608016551 Dev acc: 0.694 Train acc: 0.732\n","Epoch: 12 Cost: 0.596590638 Dev acc: 0.716 Train acc: 0.724\n","Epoch: 13 Cost: 0.586231411 Dev acc: 0.716 Train acc: 0.75\n","Epoch: 14 Cost: 0.571956515 Dev acc: 0.71 Train acc: 0.744\n","Epoch: 15 Cost: 0.557533503 Dev acc: 0.688 Train acc: 0.698\n","Epoch: 16 Cost: 0.543159187 Dev acc: 0.724 Train acc: 0.742\n","Epoch: 17 Cost: 0.532618 Dev acc: 0.714 Train acc: 0.818\n","Epoch: 18 Cost: 0.520263374 Dev acc: 0.708 Train acc: 0.778\n","Epoch: 19 Cost: 0.511670887 Dev acc: 0.726 Train acc: 0.778\n","Epoch: 20 Cost: 0.515954316 Dev acc: 0.74 Train acc: 0.794\n","Epoch: 21 Cost: 0.485247493 Dev acc: 0.718 Train acc: 0.798\n","Epoch: 22 Cost: 0.498650312 Dev acc: 0.714 Train acc: 0.768\n","Epoch: 23 Cost: 0.477414191 Dev acc: 0.712 Train acc: 0.778\n","Epoch: 24 Cost: 0.480162472 Dev acc: 0.736 Train acc: 0.846\n","Epoch: 25 Cost: 0.459277928 Dev acc: 0.744 Train acc: 0.808\n","Epoch: 26 Cost: 0.438598841 Dev acc: 0.754 Train acc: 0.842\n","Epoch: 27 Cost: 0.439209968 Dev acc: 0.744 Train acc: 0.852\n","Epoch: 28 Cost: 0.440693498 Dev acc: 0.734 Train acc: 0.878\n","Epoch: 29 Cost: 0.427380532 Dev acc: 0.718 Train acc: 0.794\n","Epoch: 30 Cost: 0.442566603 Dev acc: 0.744 Train acc: 0.866\n","Epoch: 31 Cost: 0.413498193 Dev acc: 0.742 Train acc: 0.85\n","Epoch: 32 Cost: 0.408846766 Dev acc: 0.73 Train acc: 0.796\n","Epoch: 33 Cost: 0.391743183 Dev acc: 0.738 Train acc: 0.886\n","Epoch: 34 Cost: 0.408125222 Dev acc: 0.702 Train acc: 0.816\n","Epoch: 35 Cost: 0.37053135 Dev acc: 0.7 Train acc: 0.784\n","Epoch: 36 Cost: 0.372467488 Dev acc: 0.73 Train acc: 0.802\n","Epoch: 37 Cost: 0.406395048 Dev acc: 0.746 Train acc: 0.898\n","Epoch: 38 Cost: 0.344174981 Dev acc: 0.73 Train acc: 0.874\n","Epoch: 39 Cost: 0.34226644 Dev acc: 0.74 Train acc: 0.902\n","Epoch: 40 Cost: 0.332315624 Dev acc: 0.746 Train acc: 0.934\n","Epoch: 41 Cost: 0.32837528 Dev acc: 0.706 Train acc: 0.896\n","Epoch: 42 Cost: 0.340151042 Dev acc: 0.738 Train acc: 0.912\n","Epoch: 43 Cost: 0.322656691 Dev acc: 0.756 Train acc: 0.878\n","Epoch: 44 Cost: 0.315287858 Dev acc: 0.654 Train acc: 0.724\n","Epoch: 45 Cost: 0.316652596 Dev acc: 0.694 Train acc: 0.796\n","Epoch: 46 Cost: 0.264223963 Dev acc: 0.678 Train acc: 0.796\n","Epoch: 47 Cost: 0.350555748 Dev acc: 0.73 Train acc: 0.936\n","Epoch: 48 Cost: 0.2619524 Dev acc: 0.724 Train acc: 0.93\n","Epoch: 49 Cost: 0.294853032 Dev acc: 0.708 Train acc: 0.8\n","Epoch: 50 Cost: 0.291174024 Dev acc: 0.746 Train acc: 0.974\n","Epoch: 51 Cost: 0.223053098 Dev acc: 0.734 Train acc: 0.942\n","Epoch: 52 Cost: 0.306613177 Dev acc: 0.736 Train acc: 0.948\n","Epoch: 53 Cost: 0.272709042 Dev acc: 0.726 Train acc: 0.936\n","Epoch: 54 Cost: 0.223911285 Dev acc: 0.754 Train acc: 0.956\n","Epoch: 55 Cost: 0.300717354 Dev acc: 0.728 Train acc: 0.902\n","Epoch: 56 Cost: 0.202957556 Dev acc: 0.69 Train acc: 0.902\n","Epoch: 57 Cost: 0.241605446 Dev acc: 0.748 Train acc: 0.95\n","Epoch: 58 Cost: 0.286931336 Dev acc: 0.718 Train acc: 0.962\n","Epoch: 59 Cost: 0.29042834 Dev acc: 0.722 Train acc: 0.944\n","Epoch: 60 Cost: 0.197139233 Dev acc: 0.704 Train acc: 0.914\n","Epoch: 61 Cost: 0.246584535 Dev acc: 0.738 Train acc: 0.956\n","Epoch: 62 Cost: 0.172744945 Dev acc: 0.734 Train acc: 0.962\n","Epoch: 63 Cost: 0.246135682 Dev acc: 0.708 Train acc: 0.946\n","Epoch: 64 Cost: 0.236982793 Dev acc: 0.704 Train acc: 0.89\n","Epoch: 65 Cost: 0.148881599 Dev acc: 0.744 Train acc: 0.98\n","Epoch: 66 Cost: 0.127675548 Dev acc: 0.732 Train acc: 0.984\n","Epoch: 67 Cost: 0.123513535 Dev acc: 0.744 Train acc: 0.962\n","Epoch: 68 Cost: 0.123643398 Dev acc: 0.704 Train acc: 0.942\n","Epoch: 69 Cost: 0.157013714 Dev acc: 0.638 Train acc: 0.762\n","Epoch: 70 Cost: 0.166577592 Dev acc: 0.74 Train acc: 0.974\n","Epoch: 71 Cost: 0.114187412 Dev acc: 0.744 Train acc: 0.98\n","Epoch: 72 Cost: 0.273597479 Dev acc: 0.718 Train acc: 0.946\n","Epoch: 73 Cost: 0.157189175 Dev acc: 0.728 Train acc: 0.982\n","Epoch: 74 Cost: 0.116554089 Dev acc: 0.73 Train acc: 0.99\n","Epoch: 75 Cost: 0.245684013 Dev acc: 0.738 Train acc: 0.938\n","Epoch: 76 Cost: 0.227375537 Dev acc: 0.728 Train acc: 0.976\n","Epoch: 77 Cost: 0.11388579 Dev acc: 0.73 Train acc: 0.976\n","Epoch: 78 Cost: 0.109767631 Dev acc: 0.728 Train acc: 0.992\n","Epoch: 79 Cost: 0.0919797644 Dev acc: 0.746 Train acc: 0.988\n","Epoch: 80 Cost: 0.0843453929 Dev acc: 0.734 Train acc: 0.988\n","Epoch: 81 Cost: 0.0822933465 Dev acc: 0.746 Train acc: 0.992\n","Epoch: 82 Cost: 0.0881204158 Dev acc: 0.724 Train acc: 0.99\n","Epoch: 83 Cost: 0.0873079896 Dev acc: 0.746 Train acc: 0.982\n","Epoch: 84 Cost: 0.0783360824 Dev acc: 0.74 Train acc: 0.994\n","Epoch: 85 Cost: 0.0795385614 Dev acc: 0.746 Train acc: 0.984\n","Epoch: 86 Cost: 0.279441 Dev acc: 0.664 Train acc: 0.858\n","Epoch: 87 Cost: 0.184424073 Dev acc: 0.744 Train acc: 0.98\n","Epoch: 88 Cost: 0.107181355 Dev acc: 0.752 Train acc: 0.992\n","Epoch: 89 Cost: 0.0984310731 Dev acc: 0.746 Train acc: 0.984\n","Epoch: 90 Cost: 0.0946909189 Dev acc: 0.748 Train acc: 0.992\n","Epoch: 91 Cost: 0.281871945 Dev acc: 0.68 Train acc: 0.858\n","Epoch: 92 Cost: 0.196844533 Dev acc: 0.716 Train acc: 0.946\n","Epoch: 93 Cost: 0.2925542 Dev acc: 0.712 Train acc: 0.922\n","Epoch: 94 Cost: 0.182710439 Dev acc: 0.726 Train acc: 0.976\n","Epoch: 95 Cost: 0.136649534 Dev acc: 0.694 Train acc: 0.934\n","Epoch: 96 Cost: 0.336955369 Dev acc: 0.736 Train acc: 0.98\n","Epoch: 97 Cost: 0.217813343 Dev acc: 0.712 Train acc: 0.884\n","Epoch: 98 Cost: 0.133295685 Dev acc: 0.742 Train acc: 0.988\n","Epoch: 99 Cost: 0.121678062 Dev acc: 0.738 Train acc: 0.964\n","Epoch: 100 Cost: 0.105094701 Dev acc: 0.734 Train acc: 0.984\n"]}]},{"cell_type":"markdown","metadata":{"id":"F_6kMcOhSGCe"},"source":["And evaluate it."]},{"cell_type":"code","metadata":{"id":"ZkTmNJpCSGCf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641463295679,"user_tz":-60,"elapsed":21,"user":{"displayName":"Ander Barrena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjgRxXTKl_X28G_aTFKEgcKsBAE_r5TE9B5ziwl2w=s64","userId":"06771498905541655767"}},"outputId":"69425124-52e2-4c16-8d7d-4127aba99f05"},"source":["evaluate_classifier(classifier.classify, test_set)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7138934651290499"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"kjBUT-D2spyT"},"source":["# Atribution:\n","Adapted by Oier Lopez de Lacalle, Olatz Perez de Viñaspre and Ander Barrena, based on a notebook by Sam Bowman at NYU"]}]}