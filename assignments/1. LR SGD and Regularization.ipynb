{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1. LR SGD and Regularization.ipynb","provenance":[{"file_id":"12rSs7Qd9pqemBK65VS1AMM6E3eIP7ANG","timestamp":1546968666539},{"file_id":"1z5m_m7x6J1bIx0dNnaJ5cKqkhJbUGvxh","timestamp":1546968599919}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"CUp73cFyg35T"},"source":["# Assignment1: Logistic Regression, SGD, and Regularization"]},{"cell_type":"markdown","metadata":{"id":"D1rdWNfAGonU"},"source":["In this lab session we will implement a Logistic Regression model for __sentence classification__ using Tensorflow. Given a sentence our model will predict if it is a positive or negative piece of text. The dataset we are going to use ranges the polarity annotation from 0 to 5, where 0 denotes extremely negative sentiment,  and 5  is the most  positive. \n","\n","Nevertheless, for this lab we'll  simplify the task, and we will translate the 5-way classification task into 2-way classification task (0 $\\rightarrow$ _negative,_ ;1 $\\rightarrow$ positive),\n","\n","\n","All in all, the main __objectives__ of this first assignment are the following: \n","- Learn how to build, train and evaluate a Logistic Regression Model in Tensorflow.\n","- Implement L2 reguralization (see Assignment section on the bottom)\n","- Preprocessing of the input (e.g. one-hot-encoding)\n","- Explore hyperparameters like:\n","  - Learning Rates\n","  - Regularization weight\n","- Plot learning curves for model selection"]},{"cell_type":"markdown","metadata":{"id":"uUSf1xrF4GeJ"},"source":["## Load data\n","We'll use the same data used in previous lab 1. You need to follow the same steps specified in lab1."]},{"cell_type":"code","metadata":{"id":"W5DV5ozMho_T"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HlYEMAWSg35d"},"source":["# Load the data\n","\n","import re\n","import random\n","\n","# Let's do 2-way positive/negative classification instead of 5-way\n","easy_label_map = {0:0, 1:0, 2:None, 3:1, 4:1}\n","\n","def load_sst_data(path):\n","    data = []\n","    with open(path) as f:\n","        for i, line in enumerate(f): \n","            example = {}\n","            example['label'] = easy_label_map[int(line[1])]\n","            if example['label'] is None:\n","                continue\n","            \n","            # Strip out the parse information and the phrase labels---we don't need those here\n","            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n","            example['text'] = text[1:]\n","            data.append(example)\n","\n","    random.seed(1)\n","    random.shuffle(data)\n","    return data\n","\n","sst_home = 'drive/My Drive/Colab Notebooks/dl4nlp_labs/data/trees/'  \n","training_set = load_sst_data(sst_home + '/train.txt')\n","dev_set = load_sst_data(sst_home + '/dev.txt')\n","test_set = load_sst_data(sst_home + '/test.txt')\n","\n","# Note: Unlike with feature based classifiers, evaluation here should be fast, \n","# and we don't need to trim down the dev and test sets. "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TnPdmJDTg35o"},"source":["And extract bag-of-words feature vectors."]},{"cell_type":"code","metadata":{"id":"fmawYyqtg35t"},"source":["import collections\n","import numpy as np\n","\n","def feature_function(datasets):\n","    '''Annotates datasets with feature vectors.'''\n","    \n","    # Extract vocabulary\n","    def tokenize(string):\n","        return string.split()\n","    \n","    word_counter = collections.Counter()\n","    for example in datasets[0]:\n","        word_counter.update(tokenize(example['text']))\n","    \n","    vocabulary = set([word for word in word_counter])\n","                                \n","    feature_names = set()\n","    for i, dataset in enumerate(datasets):\n","        for example in dataset:\n","            example['features'] = collections.defaultdict(float)\n","            \n","            # Extract features (by name) for one example\n","            word_counter = collections.Counter(tokenize(example['text']))\n","            for x in word_counter.items():\n","                if x[0] in vocabulary:\n","                    example[\"features\"][\"word_count_for_\" + x[0]] = x[1]\n","            \n","            feature_names.update(example['features'].keys())\n","                            \n","    # By now, we know what all the features will be, so we can\n","    # assign indices to them.\n","    feature_indices = dict(zip(feature_names, range(len(feature_names))))\n","    indices_to_features = {v: k for k, v in feature_indices.items()}\n","    dim = len(feature_indices)\n","                \n","    # Now we create actual vectors from those indices.\n","    for dataset in datasets:\n","        for example in dataset:\n","            example['vector'] = np.zeros((dim))\n","            for feature in example['features']:\n","                example['vector'][feature_indices[feature]] = example['features'][feature]\n","    return indices_to_features, dim\n","    \n","indices_to_features, dim = feature_function([training_set, dev_set, test_set])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GNkPQ2Eog351"},"source":["And define an evalution function. This is a bit different, since it's designed to let us test an entire big batch of examples at once with the classifier, rather than passing them in one by one. (For larger models or larger training sets, this could run out of memory, but it should be fine for now.)"]},{"cell_type":"code","metadata":{"id":"f0bMBijzg353"},"source":["def evaluate_classifier(classifier, eval_set):\n","    correct = 0\n","    hypotheses = classifier(eval_set)\n","    for i, example in enumerate(eval_set):\n","        hypothesis = hypotheses[i]\n","        if hypothesis == example['label']:\n","            correct += 1        \n","    return correct / float(len(eval_set))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"peSwoZIY4Zju"},"source":["## Define Logistic Regression model"]},{"cell_type":"markdown","metadata":{"id":"Snsdr1tpg35-"},"source":["Now for the fun part! The below should be a working implementation of logistic regression in TensorFlow."]},{"cell_type":"code","metadata":{"id":"3erTlngig36A"},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","tf.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xwuDSo6pg36G"},"source":["class logistic_regression_classifier:\n","    def __init__(self, dim):\n","        # Define the hyperparameters\n","        self.learning_rate = 1.0  # Maybe? Let's tune this\n","        self.reg_weight = 0.0  # Regularization weight (lambda)\n","        self.training_epochs = 50  # How long to train for - chosen to fit within class time\n","        self.display_epoch_freq = 1  # How often to test and print out statistics\n","        self.dim = dim  # The number of features\n","        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n","        \n","        self.trainable_variables = []\n","        # Define (most of) the model\n","        self.W = tf.Variable(tf.zeros([self.dim, 2]))\n","        self.b = tf.Variable(tf.zeros([2]))\n","        self.trainable_variables.append(self.W)\n","        self.trainable_variables.append(self.b)\n","    def model(self,x):\n","        logits = tf.matmul(x, self.W) + self.b\n","        return logits\n","        \n","    def train(self, training_data, dev_set):\n","        def get_minibatch(dataset, start_index, end_index):\n","            indices = range(start_index, end_index)\n","            vectors = np.float32(np.vstack([dataset[i]['vector'] for i in indices]))\n","            labels = [dataset[i]['label'] for i in indices]\n","            return vectors, labels\n","\n","        print('Training.')\n","\n","        # Training cycle\n","        for epoch in range(self.training_epochs):\n","            random.shuffle(training_set)\n","            avg_cost = 0.\n","            total_batch = int(len(training_set) / self.batch_size)\n","            \n","            # Loop over all batches in epoch\n","            for i in range(total_batch):\n","                # Assemble a minibatch of the next B examples\n","                minibatch_vectors, minibatch_labels = get_minibatch(training_set, \n","                                                                    self.batch_size * i, \n","                                                                    self.batch_size * (i + 1))\n","\n","                # Run the optimizer to take a gradient step, and also fetch the value of the \n","                # cost function for logging\n","                with tf.GradientTape() as tape:\n","                  logits = self.model(minibatch_vectors)\n","                  # Define the cost function (here, the exp and sum are built in)\n","                  cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=minibatch_labels))\n","              \n","                # This performs the SGD update equation\n","                gradients = tape.gradient(cost, self.trainable_variables)\n","                optimizer = tf.optimizers.SGD(self.learning_rate)\n","                optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n","                \n","                # Compute average loss\n","                avg_cost += cost / (total_batch * self.batch_size)\n","                \n","            # Display some statistics about the step\n","            if (epoch+1) % self.display_epoch_freq == 0:\n","                tf.print(\"Epoch:\", (epoch+1), \"Cost:\", avg_cost,\n","                      \"Dev acc:\", evaluate_classifier(self.classify, dev_set[0:500]), \n","                      \"Train acc:\", evaluate_classifier(self.classify, training_set[0:500]))\n","    \n","    def classify(self, examples):\n","        # This classifies a list of examples\n","        vectors = np.float32(np.vstack([example['vector'] for example in examples]))\n","        logits = self.model(vectors)\n","        return np.argmax(logits, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uYpnOwr5g36Q"},"source":["Now let's train it."]},{"cell_type":"code","metadata":{"id":"umNrB1Lig36T"},"source":["classifier = logistic_regression_classifier(dim)\n","classifier.train(training_set, dev_set)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s182Zr3Rg36d"},"source":["And evaluate it."]},{"cell_type":"code","metadata":{"id":"PZ2hGBe2g36h"},"source":["evaluate_classifier(classifier.classify, test_set)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ATQjXPR7g36s"},"source":["## Assignments\n","\n","### Our goals\n","  1. **Pick an effective learning rate**:\n","      - You could set up the learning rate value by passing it as argument (e.g. in `__init__ (self, dim, lr=1.0, ...)` )\n","      - Try small and larger values to see the behavior of the model.\n","  \n","  2. **Implement L2 regularization:**\n","      - Hint: Add regularization term to overal cost (`self.cost`)\n","      - Tensorflow already built in method for this. Check the API to find out. \n","      - (Optionaly) Code it without using the built in tool for it\n","\n","  3. **Pick an effective L2 weight:**\n","      - You could set up the learning rate value by passing it as argument (e.g. in `__init__ (self, dim, lw=1.0, ...)` )\n","      - Try small and larger values to see the behavior of the model.\n","  \n","  4. **Look at some learning curves:**\n","      - This code might be helpful: http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n"]},{"cell_type":"markdown","metadata":{"id":"eTv7CLOmCyKW"},"source":["# Atribution:\n","Adapted by Oier Lopez de Lacalle, Olatz Perez de Vi√±aspre and Ander Barrena, based on a notebook by Sam Bowman at NYU"]}]}