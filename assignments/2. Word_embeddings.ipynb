{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. Word_embeddings.ipynb","provenance":[{"file_id":"1Uwy0A8cePkzzVF_2Sx9jjhX-ffSaCNP-","timestamp":1547030890981}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"I3xC3wqVOVPd"},"source":["# Assignment 2: Working with Word Embeddings"]},{"cell_type":"markdown","metadata":{"id":"H_QGNa-QOVPj"},"source":["In this assigment will learn how to work with word embeddings, and that with simple techniques we can implement cool things.\n","\n","The assignment is divided in many parts that in most of the cases do not need code implementation. Hopefully, you would complete the assignment in short time and have great fun too. \n","\n","- __Part 0__: It is for setting up every thing, like load embeddings, normalize them etc.\n","- __Part 1__: We will use word embeddings for finding similar and relatead words.\n","- __Part 2__: We will learn scoring semantically words.\n","- __Part 3__: We will learn doing analogies, like `man-king` and `woman-queen`\n","- __Part 4__: We will learn visualizing embeddings."]},{"cell_type":"markdown","metadata":{"id":"ckVu0WXBOVPm"},"source":["## Part 0: Set up"]},{"cell_type":"markdown","metadata":{"id":"NF7yzVGFOVPr"},"source":["We first define some helper functions for printing results and reading embeddings."]},{"cell_type":"code","metadata":{"id":"no8wyWxKOVPt"},"source":["def print_header(title):\n","    print('┌───────────────────────────────────────────────────────────────┐')\n","    print('│{0:^63}│'.format(title))\n","    print('├──────────┬─────────────────────────────┬──────────┬───────────┤')\n","\n","def print_footer():\n","    print('└──────────┴─────────────────────────────┴──────────┴───────────┘')\n","\n","def print_oov(oov):\n","    if len(oov) > 0:\n","        print('OOV: {0}'.format(', '.join(oov)))\n","\n","def print_row(index, last, trg_words, knn, sim):\n","    if last >= index or index < 0:\n","        return last\n","    if last < index - 1:\n","        print('│  {0:>5}  │  {0:^25}  │  {0:>5}   │  {0:^7} │'.format('⋮'))\n","    word = trg_words[knn[index]]\n","    word = ('{0:^25}').format(word)\n","    print('│  {0:>6}  │  {1}  │  {2:>6}  │  {3:7.4f}  │'.format(index + 1, word, knn[index], sim[knn[index]]))\n","    return index"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l5Hy_t60OVP1"},"source":["import numpy as np\n","\n","def read(file, threshold=0, dim=50, vocabulary=None):\n","    count = 400000 if threshold <= 0 else min(threshold, 400000)\n","    words = []\n","    matrix = np.empty((count, dim)) if vocabulary is None else []\n","    for i in range(count):\n","        word, vec = file.readline().decode('utf-8').split(' ', 1)\n","        if vocabulary is None:\n","            words.append(word)\n","            matrix[i] = np.fromstring(vec, sep=' ')\n","        elif word in vocabulary:\n","            words.append(word)\n","            matrix.append(np.fromstring(vec, sep=' '))\n","    return (words, matrix) if vocabulary is None else (words, np.array(matrix))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1BZHGNh0OVP_"},"source":["def length_normalize(matrix):\n","    norms = np.sqrt(np.sum(matrix**2, axis=1))\n","    norms[norms == 0] = 1\n","    return matrix / norms[:, np.newaxis]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bv6y1OJvOVQG"},"source":["__Load data__\n","\n","First let's load a set of 50D word vectors from GloVe. If you want to run local you can download them at the following [url](http://nlp.stanford.edu/data/glove.6B.zip) (1GB). The zip file includes embeddings of different dimensionality (50d, 100d, 200d, 300d) for a vocabulary of 400000 words. Decompress them and place somewhere, for example in `./embeddings/` folder\n","\n","`glove_home` below specifies the location of the unzipped file. \n","\n","Variables like `matrix` and `word2ind` are used below in the notebook by different functions, so you need first load data in order to make everything work."]},{"cell_type":"code","metadata":{"id":"56Z64wWAO-Vu"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"etU5jndDOVQI"},"source":["import bz2\n","\n","# Read input embeddings\n","glove_home = 'drive/My Drive/Colab Notebooks/dl4nlp_labs/data/embeddings/glove.6B.50d.txt.bz2'\n","embsfile = bz2.open(glove_home)\n","words, matrix = read(embsfile)\n","\n","# Length normalize embeddings so their dot product effectively computes the cosine similarity\n","matrix = length_normalize(matrix)\n","\n","# Build word to index map\n","word2ind = {word: i for i, word in enumerate(words)}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JOXJM7cPOVQO"},"source":["## Part 1: Semantically similar/related words"]},{"cell_type":"code","metadata":{"id":"3DeSt-eZOVQS"},"source":["def knn(word, k=10):\n","    try:\n","        i = word2ind[word]\n","        sim = matrix[i].dot(matrix.T)\n","        knn = np.argsort(-sim)\n","    except KeyError:\n","        print_header('{0} (OOV)'.format(word))\n","        print_footer()\n","        print()\n","        return\n","    print_header('{0} ({1})'.format(word, i + 1))\n","    last = -1\n","    for j in range(k):\n","        word = words[knn[j]]\n","        last = print_row(j, last=last, trg_words=words, knn=knn, sim=sim)\n","    last = print_row(len(knn)-1, last=last, trg_words=words, knn=knn, sim=sim)\n","    print_footer()\n","    print()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KWIlW_V_OVQb"},"source":["\n","`knn` function retrieve the _k_ most similar/related words of the target word according to the given embedding space. The output looks like the following:\n","\n","   - fist column for nearest neighbour index\n","   - second column for nearest neighbour word\n","   - third column for index of word in frequency ranking (1 is most frequent)\n","   - last column for cosine (1 for perfect similarity)\n","\n"]},{"cell_type":"code","metadata":{"id":"_GRrVy1tOVQe"},"source":["# Show the 30 nearest neighbors\n","knn('pound', k=30)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ioOM7k14OVQn"},"source":["### TODO1: \n","Check the results for the words below. List which words you think are working well, and which ones it is failing. You can use the example in the slide (page 27) as reference.\n","\n","- france, jesus, xbox, reddish, scratched, megabits \n","\n","Try any other word you fancy, and write any comment you might have. Keep a copy of the output including 30 nearest neighbours."]},{"cell_type":"code","metadata":{"id":"_MKrSbCPOVQo"},"source":["knn('jesus',k=30)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"etAplYH6OVQz"},"source":["## Part 2: Semantic orientation"]},{"cell_type":"markdown","metadata":{"id":"jgLwuxJuOVQ1"},"source":["The __semantic orientation__ method of [Turney and Littman 2003](http://doi.acm.org/10.1145/944012.944013) is a method for automatically scoring words along some single semantic dimension like sentiment. It works from a pair of small seed sets of words that represent two opposing points on that dimension."]},{"cell_type":"code","metadata":{"id":"EiYnFZ3POVQ4"},"source":["seed_pos = ['good', 'great', 'awesome', 'like', 'love']\n","seed_neg = ['bad', 'awful', 'terrible', 'hate', 'dislike']\n","\n","def determine_coefficient(candidate_word, seed_pos, seed_neg):\n","    pos_ind = np.array([word2ind[word] for word in seed_pos])\n","    pos_mat = matrix[pos_ind]\n","\n","    neg_ind = np.array([word2ind[word] for word in seed_neg])\n","    neg_mat = matrix[neg_ind]\n","\n","    i = word2ind[candidate_word]\n","\n","    pos_sim = np.sum(matrix[i].dot(pos_mat.T))\n","    neg_sim = np.sum(matrix[i].dot(neg_mat.T))\n","\n","    return pos_sim - neg_sim\n","\n","print(determine_coefficient('abhorrent', seed_pos, seed_neg))\n","print(determine_coefficient('vacations', seed_pos, seed_neg))\n","print(determine_coefficient('hunger', seed_pos, seed_neg))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"56z0v2RVOVQ_"},"source":["And sort our vocabulary by its score along the axis. For now, we're only scoring frequent words, since this process can be slow."]},{"cell_type":"code","metadata":{"id":"3KBRp2jEOVRD"},"source":["from operator import itemgetter\n","\n","scored_words = [(word, determine_coefficient(word, seed_pos, seed_neg)) for word in words[1:5000]]\n","sorted_words = sorted(scored_words, key=itemgetter(1), reverse=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ynMv8NVOOVRK"},"source":["import pprint\n","pp = pprint.PrettyPrinter(indent=4)\n","\n","pp.pprint(sorted_words[:10])\n","pp.pprint(sorted_words[-10:])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cEi-wheaOVRU"},"source":["### TODO 2\n","Spend a few minutes exploring possible seed sets for semantic dimensions other than sentiment (e.g. \"animals\" vs \"tools\"). \n","\n","- Define your semantic orientation with the two sets of seeds.\n","- Report (write it here in the notebook) what works, what doesn't, and why?"]},{"cell_type":"markdown","metadata":{"id":"oUIcT4oUOVRV"},"source":["## Part 3: Analogy"]},{"cell_type":"markdown","metadata":{"id":"glmE-OhPOVRX"},"source":["Next, let's try to build a similar function for determining which words are likely to be good completions for an analogy. Our inputs will be a pair and a singleton word that together represent an analogy problem.\n","\n","- Analogy pair:  good $\\rightarrow$ best,  man $\\rightarrow$ king\n","- Analogy problem: bad $\\rightarrow$ ??,  woman $\\rightarrow$ ??\n","\n","\n","Remenber from slides:\n","\n","- Task: _a is to b as c is to?_\n","    + $a-b \\approx c-d$\n","    + $c-a+b \\approx d$\n","    + $argmax_{d\\in V} (cos(d , c−a+b))$"]},{"cell_type":"code","metadata":{"id":"PoI1QpJsOVRY"},"source":["def analogy(pront_pair, pront_seed, k=10):\n","    # The function make use of embedding matrix and word indices.\n","    # Recall to load data and inialize matrix and word2ind variables.\n","    try:\n","        i = word2ind[pront_pair[0]]\n","        w1v = matrix[i]\n","    except KeyError:\n","        print_header('{0} (OOV)'.format(pront_pair[0]))\n","        print_footer()\n","        print()\n","        return\n","    try:\n","        i = word2ind[pront_pair[1]]\n","        w2v = matrix[i]\n","    except KeyError:\n","        print_header('{0} (OOV)'.format(pront_pair[1]))\n","        print_footer()\n","        print()\n","        return\n","    try:\n","        i = word2ind[pront_seed]\n","        w3v = matrix[i]\n","    except KeyError:\n","        print_header('{0} (OOV)'.format(pront_seed))\n","        print_footer()\n","        print()\n","        return\n","    \n","    ########### YOUR SOLUTION HERE\n","    knn = None # obtain k nearest words according to the analogy\n","    ###########\n","\n","    print_header('{0} - {1} + {2}'.format(pront_pair[0], pront_pair[1], pront_seed))\n","    last = -1\n","    for j in range(k): \n","        word = words[knn[j]]\n","        last = print_row(j, last=last, trg_words=words, knn=knn, sim=sim)\n","    last = print_row(len(knn)-1, last=last, trg_words=words, knn=knn, sim=sim)\n","    print_footer()\n","    print()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1WtVhL0lOVRg"},"source":["prompt_pair = ('good', 'best')\n","prompt_seed = 'bad'\n","analogy(prompt_pair, prompt_seed)\n","\n","prompt_pair = ('man', 'king')\n","prompt_seed = 'woman'\n","analogy(prompt_pair, prompt_seed)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f-IfIhMXOVRp"},"source":["### TODO3\n","-  The embeddings space can be used to do analogies. Please examine the formula in the of slides, and apply it to the three embeddings in the function below `analogy`. If you programmed it correctly, the following analogy should work:\n","    + `man:king; woman:?` (Note that, in the result list, words in the query need to be ignored)\n","    \n","\n","- Check 10 of the analogie below and report in which position is the correct answer (discounting the words in the query, of course).\n","\n","![](https://drive.google.com/uc?id=1Uoa-17hHm2mDKi1ZtIaVrjj60JTBS8rV)\n"]},{"cell_type":"code","metadata":{"id":"2a9cMJs1OVRq"},"source":["analogy(['france', 'paris'], 'italy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NR6dSTdVOVRx"},"source":["## Part 4: Visualization\n","Below we'll use T-SNE to visualize how our high-dimensional word vectors cluster together. T-SNE is used to project these vectors into two dimensions while preserving local stucture. Check out [this post from Christopher Olah](http://colah.github.io/posts/2014-10-Visualizing-MNIST/) to learn more about T-SNE and other ways to visualize high-dimensional data."]},{"cell_type":"code","metadata":{"id":"Son1bq4WOVRy"},"source":["%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","\n","import matplotlib.pyplot as plt\n","from sklearn.manifold import TSNE\n","\n","# reduce size of the matrix to speed up the operations\n","viz_words = 500\n","start_ind = 1000\n","end_ind = start_ind+viz_words\n","small_ind = np.array([word2ind[word] for word in words[start_ind:end_ind]])\n","small_word2ind = {word : i for i, word in enumerate(words[start_ind:end_ind])}\n","small_matrix =  matrix[small_ind]\n","\n","# Project word embeddings to two-dimensions\n","tsne = TSNE()\n","embed_tsne = tsne.fit_transform(small_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eK7V25GQOVSC"},"source":["fig, ax = plt.subplots(figsize=(14, 14))\n","plt.scatter(x=embed_tsne[:,0], y=embed_tsne[:,1], c='steelblue')\n","for i, word in enumerate(words[start_ind:end_ind]):\n","    plt.annotate(word, (embed_tsne[small_word2ind[word], 0], embed_tsne[small_word2ind[word], 1]), alpha=0.7)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yczOM7fROVSH"},"source":["### TODO4\n","- Sort word according to some semantic dimension (you could use the one on sentiment)\n","- Plot some of them using T-SNE \n","- It would help visualization coloring word-points according to their positive/negativ orientation "]},{"cell_type":"markdown","metadata":{"id":"rHYdlMBvOVSJ"},"source":["# Atribution: \n","Adapted by Oier Lopez de Lacalle, Olatz Perez de Viñaspre and Ander Barrena, based on the code by Mikel Artetxe at UPV/EHU"]}]}