{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. LSTM based Language Model.ipynb","provenance":[{"file_id":"1opzCFCVHc7hzsXbOuRoF8ZenFlayg8im","timestamp":1547195062164},{"file_id":"1xen3vjgC50ALslhxIVJVAPfyNPv0nORr","timestamp":1547136286563}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"skxNOBVngUuI"},"source":["# Assignment 3: Language Modelling with LSTM networks"]},{"cell_type":"markdown","metadata":{"id":"od8wz1GsgUuO"},"source":["In this assignment, you will implement an LSTM based language model. We strongly recommend to finish first _lab 4_, which is closely related and is much simpler."]},{"cell_type":"markdown","metadata":{"id":"LG0W24LVgUuR"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"T9YkdodygUuW"},"source":["First, let's load the data as before."]},{"cell_type":"code","metadata":{"id":"dqj5ZEmHqZHg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645208306691,"user_tz":-60,"elapsed":26283,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"f93bb8ed-8092-4061-b47f-fff744595b3c"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/LAP/Subjects/DL/labs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8HqvDAMtL308","executionInfo":{"status":"ok","timestamp":1645208306693,"user_tz":-60,"elapsed":27,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"2061f3ba-abcc-4a67-c6aa-1fdcf47c6d3b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/LAP/Subjects/DL/labs\n"]}]},{"cell_type":"code","metadata":{"id":"lhVrZbAAgUuZ","executionInfo":{"status":"ok","timestamp":1645208309696,"user_tz":-60,"elapsed":3014,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["import re\n","import random\n","\n","# Let's do 2-way positive/negative classification instead of 5-way\n","easy_label_map = {0:0, 1:0, 2:None, 3:1, 4:1}\n","\n","def load_sst_data(path):\n","    data = []\n","    with open(path) as f:\n","        for i, line in enumerate(f): \n","            example = {}\n","            \n","            # Strip out the parse information and the phrase labels---we don't need those here\n","            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n","            example['text'] = text[1:]\n","            data.append(example)\n","\n","    random.seed(1)\n","    random.shuffle(data)\n","    return data\n","\n","sst_home = '../data/trees/'\n","training_set = load_sst_data(sst_home + '/train.txt')\n","dev_set = load_sst_data(sst_home + '/dev.txt')\n","test_set = load_sst_data(sst_home + '/test.txt')\n","\n","# Note: Unlike with k-nearest neighbors, evaluation here should be fast, and we don't need to\n","# trim down the dev and test sets. "],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r7vCn9ElgUui"},"source":["Next, we'll convert the data to index vectors.\n","\n","To simplify your implementation, we'll use a fixed unrolling length of 20. This means that we'll have to expand each sentence into a sequence of 21 word indices. In the conversion process, we'll mark the start of each sentence with a special word symbol `<S>`, mark the end of each sentence (if it occurs within the first 21 words) with a special word symbol `</S>`, mark extra tokens after `</S>` with a special word symbol `<PAD>`, and mark out-of-vocabulary words with `<UNK>`, for unknown. As in the previous assignment, we'll use a very small vocabulary for this assignment, so you'll see `<UNK>` often."]},{"cell_type":"code","metadata":{"id":"HNfCXqF8gUul","executionInfo":{"status":"ok","timestamp":1645208310795,"user_tz":-60,"elapsed":1108,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["import collections\n","import numpy as np\n","\n","def sentence_to_padded_index_sequence(datasets):\n","    '''Annotates datasets with feature vectors.'''\n","    \n","    START = \"<S>\"\n","    END = \"</S>\"\n","    END_PADDING = \"<PAD>\"\n","    UNKNOWN = \"<UNK>\"\n","    SEQ_LEN = 21\n","    \n","    # Extract vocabulary\n","    def tokenize(string):\n","        return string.lower().split()\n","    \n","    word_counter = collections.Counter()\n","    for example in datasets[0]:\n","        word_counter.update(tokenize(example['text']))\n","    \n","    vocabulary = set([word for word in word_counter if word_counter[word] > 25])\n","    vocabulary = list(vocabulary)\n","    vocabulary = [START, END, END_PADDING, UNKNOWN] + vocabulary\n","        \n","    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n","    indices_to_words = {v: k for k, v in word_indices.items()}\n","        \n","    for i, dataset in enumerate(datasets):\n","        for example in dataset:\n","            example['index_sequence'] = np.zeros((SEQ_LEN), dtype=np.int32)\n","            \n","            token_sequence = [START] + tokenize(example['text']) + [END]\n","            \n","            for i in range(SEQ_LEN):\n","                if i < len(token_sequence):\n","                    if token_sequence[i] in word_indices:\n","                        index = word_indices[token_sequence[i]]\n","                    else:\n","                        index = word_indices[UNKNOWN]\n","                else:\n","                    index = word_indices[END_PADDING]\n","                example['index_sequence'][i] = index\n","    return indices_to_words, word_indices\n","    \n","indices_to_words, word_indices = sentence_to_padded_index_sequence([training_set, dev_set, test_set])"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"oCU1I5qSgUus","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645208310797,"user_tz":-60,"elapsed":95,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"651fa28e-809e-47ed-a91b-3d51e558cdf3"},"source":["print(training_set[18])\n","print(len(word_indices))"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["{'text': \"It could have been something special , but two things drag it down to mediocrity -- director Clare Peploe 's misunderstanding of Marivaux 's rhythms , and Mira Sorvino 's limitations as a classical actress .\", 'index_sequence': array([  0, 124, 465, 176, 475,  81, 106, 369,  43, 488, 137,   3, 124,\n","       539, 440,   3, 325, 129,   3,   3,  52], dtype=int32)}\n","603\n"]}]},{"cell_type":"markdown","metadata":{"id":"5ASkTrIogUu3"},"source":["## Assignments: \n","### Part 1: Implementation"]},{"cell_type":"markdown","metadata":{"id":"I__f8rvOgUu5"},"source":["Now, using the starter code and hyperparameter values provided below, implement an LSTM language model with dropout on the non-recurrent connections. Use the standard form of the LSTM reflected in the slides (without peepholes). **You should only have to edit the marked sections of code to build the base LSTM**, though implementing dropout properly may require small changes to the main training loop and to brittle_sampler().\n","\n","**Don't use any TensorFlow code that is specifically built for RNNs**. If a TF function has 'recurrent', 'sequence', 'LSTM', or 'RNN' in its name, you should built it yourself instead of using it. (Your version will likely be much simpler, by the way, since these built in methods are powerful but fairly complex and potentially confusing.)\n","\n","We won't be evaluating our model in the conventional way (perplexity on a held-out test set) for a few reasons: to save time, because we have no baseline to compare against, and because overfitting the training set is a less immediate concern with these models than it was with sentence classifiers. Instead, we'll use the value of the cost function to make sure that the model is converging as expected, and we'll use samples drawn from the model to qualitatively evaluate it.\n","\n","**Tips**: \n","- Check the code for the GRU based sentiment classifier (lab 4), specially the part where the RNN structure is defined.\n","- You'll need to use `tf.nn.embedding_lookup()`, `tf.nn.sparse_softmax_cross_entropy_with_logits()`, and `tf.split()` at least once each. All three should be easy to Google, though the last homework and the last exercise should show examples of the first two.\n","- As before, you'll want to initialize your trained parameters using something like `tf.random_normal(..., stddev=0.1)`\n","\n","**TODOS:**\n","- **TODO1**: Define the parameters of the LSTM (check the given slides in class)\n","- **TODO2**: Build the LSTM LM (follow the instructions in the code-comments)\n"]},{"cell_type":"code","metadata":{"id":"fKishKAwgUu8","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1645208315252,"user_tz":-60,"elapsed":4466,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"76279ba4-9a58-4b03-e1b1-ac3ec7066637"},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","tf.__version__"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.8.0'"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"8o4syEQOgUvG","executionInfo":{"status":"ok","timestamp":1645208315255,"user_tz":-60,"elapsed":35,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["class LanguageModel:\n","    def __init__(self, vocab_size, sequence_length):\n","        # Define the hyperparameters\n","        self.learning_rate = 0.3  # Should be about right\n","        self.training_epochs = 250  # How long to train for - chosen to fit within class time\n","        self.display_epoch_freq = 1  # How often to test and print out statistics\n","        self.dim = 32  # The dimension of the hidden state of the RNN\n","        self.embedding_dim = 16  # The dimension of the learned word embeddings\n","        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n","        self.vocab_size = vocab_size  # Defined by the file reader above\n","        self.sequence_length = sequence_length  # Defined by the file reader above\n","        self.rate = 0.25  # Used in dropout (at training time only, not at sampling time)\n","        \n","        #### Start main editable code block ####\n","        self.trainable_variables = []\n","        # Your model should populate the following four python lists.\n","        # self.logits should contain one [batch_size, vocab_size]-shaped TF tensor of logits \n","        #   for each of the 20 steps of the model.\n","        # self.costs should contain one [batch_size]-shaped TF tensor of cross-entropy loss \n","        #   values for each of the 20 steps of the model.\n","        # self.h and c should each start contain one [batch_size, dim]-shaped TF tensor of LSTM\n","        #   activations for each of the 21 *states* of the model -- one tensor of zeros for the \n","        #   starting state followed by one tensor each for the remaining 20 steps.\n","        # Don't rename any of these variables or change their purpose -- they'll be needed by the\n","        # pre-built sampler.\n","        \n","        self.h_zero = tf.zeros([self.batch_size, self.dim])\n","        self.c_zero = tf.zeros([self.batch_size, self.dim])\n","\n","        # embedding parameters\n","        self.E = tf.Variable(tf.random.normal([self.vocab_size, self.embedding_dim], stddev=0.1))\n","        self.trainable_variables.append(self.E)\n","\n","        # forget parameters\n","        self.W_f = tf.Variable(tf.random.normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n","        self.b_f = tf.Variable(tf.random.normal([self.dim], stddev=0.1))\n","        self.trainable_variables.append(self.W_f)\n","        self.trainable_variables.append(self.b_f)\n","\n","        # input parameters\n","        self.W_i = tf.Variable(tf.random.normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n","        self.b_i = tf.Variable(tf.random.normal([self.dim], stddev=0.1))\n","        self.trainable_variables.append(self.W_i)\n","        self.trainable_variables.append(self.b_i)\n","\n","        # candidate parameters\n","        self.W_c = tf.Variable(tf.random.normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n","        self.b_c = tf.Variable(tf.random.normal([self.dim], stddev=0.1))\n","        self.trainable_variables.append(self.W_c)\n","        self.trainable_variables.append(self.b_c)\n","\n","        # output parameters\n","        self.W_o = tf.Variable(tf.random.normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n","        self.b_o = tf.Variable(tf.random.normal([self.dim], stddev=0.1))\n","        self.trainable_variables.append(self.W_o)\n","        self.trainable_variables.append(self.b_o)\n","\n","        # logit parameters\n","        self.W_l = tf.Variable(tf.random.normal([self.dim, self.vocab_size], stddev=0.1))\n","        self.b_l = tf.Variable(tf.random.normal([self.vocab_size], stddev=0.1))\n","        self.trainable_variables.append(self.W_l)\n","        self.trainable_variables.append(self.b_l)\n","    \n","    def model(self, x, rate, sample=False, h_zero=None, c_zero=None):\n","        def step(x, next_x, h_prev, c_prev):\n","            # TODO\n","            # embedding\n","            emb = tf.nn.embedding_lookup(params=self.E, ids=x)\n","            emb_h_prev = tf.concat([emb, h_prev], 1)\n","            # 1. forget\n","            f = tf.sigmoid(tf.matmul(emb_h_prev, self.W_f) + self.b_f)\n","            # 2. decide and prepare new information\n","            i = tf.sigmoid(tf.matmul(emb_h_prev, self.W_i) + self.b_i)\n","            c_tilde = tf.nn.tanh(tf.matmul(emb_h_prev, self.W_c) + self.b_c)\n","            # 3. update Ct-1 into the new cell state Ct\n","            c_prev = f * c_prev + i * c_tilde\n","            # 4. decide and prepare output\n","            o = tf.sigmoid(tf.matmul(emb_h_prev, self.W_o) + self.b_o)\n","            h_prev = o * tf.nn.tanh(c_prev)\n","            # dropout\n","            h_prev = tf.nn.dropout(h_prev, rate)\n","            # logits\n","            logits = tf.matmul(h_prev, self.W_l) + self.b_l\n","            # costs\n","            costs = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=next_x, logits=logits)\n","            return logits, costs, h_prev, c_prev\n","\n","        self.x_slices = tf.split(x, self.sequence_length, 1)\n","        all_logits = []\n","        all_costs = []\n","        \n","        if h_zero != None and c_zero != None:\n","            self.h = [h_zero]\n","            self.c = [c_zero]\n","        else:\n","            self.h = [self.h_zero]\n","            self.c = [self.c_zero]\n","        \n","        x = tf.reshape(self.x_slices[0], [-1])\n","        \n","        # TODO unroll\n","        h_prev = self.h_zero\n","        c_prev = self.c_zero\n","        for i in range(1, self.sequence_length):\n","            next_x = tf.reshape(self.x_slices[i], [-1])\n","            logits, costs, h_prev, c_prev = step(x, next_x, h_prev, c_prev)\n","            all_logits.append(logits)\n","            all_costs.append(costs)\n","            self.h.append(h_prev)\n","            self.h.append(c_prev)\n","            x = next_x\n","            if sample:\n","                return h_prev, c_prev, logits\n","        \n","        return all_logits, all_costs\n","\n","        #### End main editable code block ####\n","        \n","\n","    def train(self, training_data):\n","        def get_minibatch(dataset, start_index, end_index):\n","            indices = range(start_index, end_index)\n","            vectors = np.vstack([dataset[i]['index_sequence'] for i in indices])\n","            return vectors\n","        \n","       \n","        print('Training.')\n","\n","        # Training cycle\n","        for epoch in range(self.training_epochs):\n","            random.shuffle(training_set)\n","            avg_cost = 0.\n","            total_batch = int(len(training_set) / self.batch_size)\n","            \n","            # Loop over all batches in epoch\n","            for i in range(total_batch):\n","                # Assemble a minibatch of the next B examples\n","                minibatch_vectors = np.int32(get_minibatch(training_set, self.batch_size * i, self.batch_size * (i + 1)))\n","\n","                # Run the optimizer to take a gradient step, and also fetch the value of the \n","                # cost function for logging\n","                with tf.GradientTape() as tape:\n","                    logits, costs = self.model(minibatch_vectors,self.rate)\n","                    # Sum costs for each word in each example, but average cost across examples.\n","                    costs_tensor = tf.concat([tf.expand_dims(cost, 1) for cost in costs], 1)\n","                    cost_per_example = tf.reduce_sum(costs_tensor, 1)\n","                    total_cost = tf.reduce_mean(cost_per_example)\n","                \n","                # This performs the main SGD update equation\n","                gradients = tape.gradient(total_cost, self.trainable_variables)\n","                optimizer = tf.optimizers.SGD(self.learning_rate)\n","                optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n","                                                            \n","                # Compute average loss\n","                avg_cost += total_cost / (total_batch * self.batch_size)\n","                \n","            # Display some statistics about the step\n","            if (epoch+1) % self.display_epoch_freq == 0:\n","                tf.print(\"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \"Sample:\", self.sample())\n","    \n","    def sample(self):\n","        # This samples a sequence of tokens from the model starting with <S>.\n","        # We only ever run the first timestep of the model, and use an effective batch size of one\n","        # but we leave the model unrolled for multiple steps, and use the full batch size to simplify \n","        # the training code. This slows things down.\n","\n","        def brittle_sampler():\n","            # The main sampling code. Can fail randomly due to rounding errors that yield probibilities\n","            # that don't sum to one.\n","            \n","            word_indices = [0] # 0 here is the \"<S>\" symbol\n","            for i in range(self.sequence_length - 1):\n","                dummy_x = np.zeros((self.batch_size, self.sequence_length),dtype=np.int32)\n","                dummy_x[0][0] = word_indices[-1]\n","                model_h = None\n","                model_c = None\n","                if i > 0:\n","                    model_h = h\n","                    model_c = c\n","                h, c, logits = self.model(dummy_x,0.0,sample=True,h_zero=model_h,c_zero=model_c)\n","                logits = logits[0, :] # Discard all but first batch entry\n","                exp_logits = np.exp(logits - np.max(logits))\n","                distribution = exp_logits / exp_logits.sum()\n","                sampled_index = np.flatnonzero(np.random.multinomial(1, distribution))[0]\n","                word_indices.append(sampled_index)\n","            words = [indices_to_words[index] for index in word_indices]\n","            return ' '.join(words)\n","        \n","        while True:\n","            try:\n","              sample = brittle_sampler()\n","              return sample\n","            except ValueError as e:  # Retry if we experience a random failure.\n","              pass"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c7-7pfUSgUvN"},"source":["Now let's train it.\n","\n","Once you're confident your model is doing what you want, let it run for the full 250 epochs. This will take some time—likely between five and thirty minutes. If it much longer on a reasonably modern laptop—more than an hour—that suggests serious problems with your implementation. A properly implemented model with dropout should reach an average cost of less than 0.22 quickly, and then slowly improve from there. We train the model for a fairly long time because these small improvements in cost correspond to fairly large improvements in sample quality.\n","\n","Samples from a trained models should have coherent portions, but they will not resemble interpretable English sentences. Here are three examples from a model with a cost value of 0.202:\n","\n","`<S> the good <UNK> and <UNK> and <UNK> <UNK> with predictable and <UNK> , but also does one of -lrb- <UNK>`\n","\n","`<S> <UNK> has <UNK> actors seems done <UNK> would these <UNK> <UNK> to <UNK> <UNK> <UNK> 're <UNK> to mind .`\n","\n","`<S> an action story that was because the <UNK> <UNK> are when <UNK> as ``` <UNK> '' ' it is any`\n","\n","`-lrb-` and `-rrb` are the way that left and right parentheses are represented in the corpus."]},{"cell_type":"code","metadata":{"id":"zifuF0xQgUvR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645209898743,"user_tz":-60,"elapsed":1583521,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"5094e03d-2867-4548-9464-cee9489e7304"},"source":["model = LanguageModel(len(word_indices), 21)\n","model.train(training_set)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Training.\n","Epoch: 1 Cost: 0.308164 Sample: <S> culture cold , <UNK> by 's it in real <UNK> <UNK> 's <UNK> <UNK> <UNK> before 's <UNK> of instead\n","Epoch: 2 Cost: 0.263807595 Sample: <S> , <UNK> plenty level this ? <UNK> movie version he lacks hard <UNK> each should <UNK> it next else the\n","Epoch: 3 Cost: 0.256947875 Sample: <S> <UNK> <UNK> , us give <UNK> and melodrama you something <UNK> of <UNK> the becomes a only <UNK> <UNK> are\n","Epoch: 4 Cost: 0.252048671 Sample: <S> <UNK> -rrb- they does <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> </S> </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Epoch: 5 Cost: 0.247531205 Sample: <S> <UNK> dark comes back . </S> <PAD> <PAD> <PAD> <PAD> <PAD> </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Epoch: 6 Cost: 0.243655294 Sample: <S> a film keep can <UNK> <UNK> face . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> </S> </S> <PAD>\n","Epoch: 7 Cost: 0.240909591 Sample: <S> there </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Epoch: 8 Cost: 0.237342283 Sample: <S> a <UNK> <UNK> <UNK> more made 's 've . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> </S> <PAD> <PAD> <PAD>\n","Epoch: 9 Cost: 0.235461816 Sample: <S> all performance <UNK> <UNK> <UNK> of <UNK> <UNK> <UNK> , like <UNK> <UNK> <UNK> <UNK> . <PAD> <PAD> </S> <PAD>\n","Epoch: 10 Cost: 0.233389229 Sample: <S> book , this thriller , <UNK> <UNK> <UNK> <UNK> does summer <UNK> <UNK> are <UNK> <UNK> how any find be\n","Epoch: 11 Cost: 0.232153565 Sample: <S> <UNK> going <UNK> also it it children hours <UNK> on had message social worth fact <UNK> <UNK> <UNK> hours like\n","Epoch: 12 Cost: 0.230426073 Sample: <S> it visual although <UNK> <UNK> piece , also its best is just <UNK> may n't one , its say to\n","Epoch: 13 Cost: 0.228568465 Sample: <S> in <UNK> idea </S> <PAD> <PAD> <PAD> </S> <PAD> <PAD> <PAD> <PAD> </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Epoch: 14 Cost: 0.227653205 Sample: <S> <UNK> <UNK> elements <UNK> <UNK> <UNK> and instead <UNK> quite strange <UNK> from the movie will in real from the\n","Epoch: 15 Cost: 0.226780891 Sample: <S> i be plenty film , <UNK> look documentary to <UNK> amusing black ... <UNK> , <UNK> , `` <UNK> has\n","Epoch: 16 Cost: 0.22539033 Sample: <S> <UNK> of <UNK> <UNK> even <UNK> silly gags <UNK> <UNK> and <UNK> <UNK> is <UNK> <UNK> <UNK> line to <UNK>\n","Epoch: 17 Cost: 0.224262461 Sample: <S> a <UNK> so <UNK> <UNK> , her <UNK> , <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Epoch: 18 Cost: 0.223841876 Sample: <S> seems with an <UNK> man as <UNK> are as leave about the <UNK> </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Epoch: 19 Cost: 0.222987249 Sample: <S> <UNK> thriller better of left <UNK> in an film in the ride adventure to young <UNK> <UNK> <UNK> 's <UNK>\n","Epoch: 20 Cost: 0.222330794 Sample: <S> a <UNK> <UNK> <UNK> on <UNK> <UNK> , <UNK> <UNK> feels the last <UNK> has <UNK> the where <UNK> ,\n","Epoch: 21 Cost: 0.221524104 Sample: <S> <UNK> <UNK> <UNK> and <UNK> <UNK> <UNK> movie has one <UNK> <UNK> <UNK> john <UNK> <UNK> and <UNK> <UNK> and\n","Epoch: 22 Cost: 0.220799834 Sample: <S> <UNK> like very a <UNK> , in being as make to make the impossible <UNK> <UNK> its <UNK> <UNK> <UNK>\n","Epoch: 23 Cost: 0.220749274 Sample: <S> when he <UNK> the film 's been a real <UNK> a action <UNK> <UNK> -rrb- script the material that was\n","Epoch: 24 Cost: 0.219996542 Sample: <S> death to good <UNK> in <UNK> of art <UNK> <UNK> and <UNK> that <UNK> for though a lot , <UNK>\n","Epoch: 25 Cost: 0.219937354 Sample: <S> <UNK> <UNK> is enjoy director most obvious <UNK> <UNK> falls <UNK> and worst <UNK> <UNK> movie <UNK> is past a\n","Epoch: 26 Cost: 0.218849242 Sample: <S> <UNK> to be thoroughly <UNK> <UNK> instead of your little a real <UNK> <UNK> <UNK> <UNK> <UNK> head to their\n","Epoch: 27 Cost: 0.219022751 Sample: <S> the right this more , <UNK> <UNK> all <UNK> you series to see -- -lrb- a first <UNK> , put\n","Epoch: 28 Cost: 0.218171194 Sample: <S> this need is <UNK> will <UNK> who quirky from <UNK> <UNK> , <UNK> <UNK> still <UNK> , the <UNK> <UNK>\n","Epoch: 29 Cost: 0.217775628 Sample: <S> one <UNK> , from had <UNK> -- <UNK> a more <UNK> his <UNK> , and film on <UNK> <UNK> this\n","Epoch: 30 Cost: 0.21764645 Sample: <S> if yet a worst <UNK> <UNK> <UNK> had after feature , this rather something works and <UNK> feels and <UNK>\n","Epoch: 31 Cost: 0.216914937 Sample: <S> no <UNK> by given too be <UNK> the <UNK> , <UNK> and the <UNK> <UNK> because all , <UNK> light\n","Epoch: 32 Cost: 0.217061162 Sample: <S> <UNK> not <UNK> <UNK> the piece , should 've quite though may <UNK> , <UNK> <UNK> , it 's <UNK>\n","Epoch: 33 Cost: 0.216423944 Sample: <S> you give great <UNK> , because both <UNK> <UNK> <UNK> , <UNK> amusing that <UNK> to being <UNK> <UNK> as\n","Epoch: 34 Cost: 0.216625139 Sample: <S> it <UNK> of <UNK> the whole , <UNK> and film <UNK> in complex <UNK> fresh much few <UNK> , <UNK>\n","Epoch: 35 Cost: 0.215872675 Sample: <S> there might tries the every end at actors , <UNK> <UNK> <UNK> <UNK> <UNK> 's <UNK> high , hollywood <UNK>\n","Epoch: 36 Cost: 0.21563077 Sample: <S> it was n't a <UNK> know an role <UNK> its <UNK> its <UNK> <UNK> in a close enough of <UNK>\n","Epoch: 37 Cost: 0.215377167 Sample: <S> <UNK> once <UNK> movies <UNK> that an <UNK> formula it 's <UNK> my effects is <UNK> <UNK> <UNK> <UNK> to\n","Epoch: 38 Cost: 0.215241596 Sample: <S> a <UNK> drama , <UNK> as very <UNK> <UNK> <UNK> , <UNK> <UNK> nearly the <UNK> , <UNK> actually <UNK>\n","Epoch: 39 Cost: 0.215153381 Sample: <S> i ca n't <UNK> both , <UNK> and <UNK> <UNK> <UNK> to <UNK> <UNK> <UNK> women 're interesting and fun\n","Epoch: 40 Cost: 0.214342773 Sample: <S> so not <UNK> <UNK> <UNK> , and enjoyable <UNK> , <UNK> -- would <UNK> <UNK> to <UNK> who are the\n","Epoch: 41 Cost: 0.214322969 Sample: <S> <UNK> of <UNK> <UNK> who is nothing of everyone has well is n't <UNK> the wonderful <UNK> comes a work\n","Epoch: 42 Cost: 0.214182138 Sample: <S> a year <UNK> , and sad - <UNK> seem as a <UNK> <UNK> of <UNK> with touching , <UNK> <UNK>\n","Epoch: 43 Cost: 0.214020833 Sample: <S> there makes just the <UNK> for your <UNK> , <UNK> to the <UNK> , us <UNK> as strong ' <UNK>\n","Epoch: 44 Cost: 0.213755846 Sample: <S> each big original actors along , <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> in `` <UNK> satisfying <UNK> is\n","Epoch: 45 Cost: 0.213868305 Sample: <S> the <UNK> a <UNK> <UNK> coming-of-age <UNK> <UNK> ideas in such that makes a audience , so two <UNK> <UNK>\n","Epoch: 46 Cost: 0.213576 Sample: <S> <UNK> <UNK> <UNK> film that it <UNK> <UNK> <UNK> that 's camera <UNK> <UNK> into the seems 'm <UNK> character\n","Epoch: 47 Cost: 0.213143498 Sample: <S> a <UNK> <UNK> one ... <UNK> comedy about one know it 's <UNK> <UNK> <UNK> with <UNK> <UNK> by does\n","Epoch: 48 Cost: 0.213085115 Sample: <S> everything the actors such -lrb- it <UNK> <UNK> and <UNK> a <UNK> , <UNK> <UNK> into the <UNK> some visual\n","Epoch: 49 Cost: 0.212739602 Sample: <S> <UNK> <UNK> worth the <UNK> , <UNK> for <UNK> , the <UNK> the film to <UNK> dialogue from the <UNK>\n","Epoch: 50 Cost: 0.212593257 Sample: <S> <UNK> , because it 's <UNK> easily <UNK> about one as <UNK> -- a <UNK> <UNK> a film that the\n","Epoch: 51 Cost: 0.212251619 Sample: <S> an little of a <UNK> <UNK> it <UNK> to own story is about a lot on the <UNK> shows ,\n","Epoch: 52 Cost: 0.212419093 Sample: <S> an touching in the head , a <UNK> when the <UNK> yet completely <UNK> viewers <UNK> characters as <UNK> and\n","Epoch: 53 Cost: 0.211919755 Sample: <S> the <UNK> <UNK> mr. <UNK> ' <UNK> manages with <UNK> its already just , <UNK> , but <UNK> , has\n","Epoch: 54 Cost: 0.212300494 Sample: <S> <UNK> such <UNK> before events , but <UNK> <UNK> <UNK> tries out at a <UNK> , watching a <UNK> <UNK>\n","Epoch: 55 Cost: 0.211724564 Sample: <S> - , <UNK> can <UNK> in <UNK> cinema life , <UNK> visually the plot is a war film in the\n","Epoch: 56 Cost: 0.211462095 Sample: <S> the <UNK> <UNK> for place the man should seen this less over the <UNK> <UNK> , while viewer to make\n","Epoch: 57 Cost: 0.21173121 Sample: <S> an performance should find <UNK> <UNK> , <UNK> does an <UNK> a good movie as the <UNK> why it seems\n","Epoch: 58 Cost: 0.211313963 Sample: <S> ` work of being a <UNK> an <UNK> of this <UNK> <UNK> for all many special , one so <UNK>\n","Epoch: 59 Cost: 0.211294368 Sample: <S> everything from <UNK> experience with the little so <UNK> <UNK> than more effort in <UNK> a series of the direction\n","Epoch: 60 Cost: 0.210849449 Sample: <S> for <UNK> <UNK> <UNK> , and <UNK> , we 'll find occasionally <UNK> and overall , i found acting .\n","Epoch: 61 Cost: 0.210639983 Sample: <S> in the <UNK> <UNK> for <UNK> offers so short , <UNK> , you 'd <UNK> from <UNK> just else .\n","Epoch: 62 Cost: 0.210776135 Sample: <S> lack and <UNK> attempt by a entertaining <UNK> it had an <UNK> another <UNK> , watching the world with its\n","Epoch: 63 Cost: 0.210630357 Sample: <S> <UNK> , <UNK> and <UNK> both <UNK> that <UNK> is n't is n't leave it 's half film ca n't\n","Epoch: 64 Cost: 0.210334376 Sample: <S> <UNK> if as a <UNK> <UNK> <UNK> 's do n't <UNK> <UNK> in the <UNK> <UNK> <UNK> elements you is\n","Epoch: 65 Cost: 0.210141018 Sample: <S> the <UNK> <UNK> style of the <UNK> to watch flick with <UNK> into with a only <UNK> <UNK> <UNK> an\n","Epoch: 66 Cost: 0.210559189 Sample: <S> it <UNK> of the <UNK> enough can <UNK> , <UNK> clever and the `` place together you fails in <UNK>\n","Epoch: 67 Cost: 0.209985584 Sample: <S> <UNK> <UNK> where its <UNK> to the level and <UNK> strong <UNK> make a off <UNK> all <UNK> for <UNK>\n","Epoch: 68 Cost: 0.20954597 Sample: <S> though its <UNK> 's <UNK> <UNK> and a dramatic <UNK> , <UNK> of <UNK> -- the <UNK> <UNK> of a\n","Epoch: 69 Cost: 0.20954527 Sample: <S> <UNK> <UNK> of portrait of into this <UNK> to only <UNK> <UNK> <UNK> brilliant experience ; the <UNK> intelligent ,\n","Epoch: 70 Cost: 0.209350377 Sample: <S> <UNK> ... rather <UNK> -- in so engaging <UNK> ending in the <UNK> 's <UNK> and about <UNK> people has\n","Epoch: 71 Cost: 0.209579289 Sample: <S> like the <UNK> and the `` <UNK> <UNK> <UNK> up , a <UNK> come to see -lrb- day , a\n","Epoch: 72 Cost: 0.209585577 Sample: <S> it <UNK> turns after one thriller , <UNK> is <UNK> is n't when it was an <UNK> with the <UNK>\n","Epoch: 73 Cost: 0.209113508 Sample: <S> we 've me the laughs can music to be <UNK> would find mystery 'll be <UNK> <UNK> , <UNK> <UNK>\n","Epoch: 74 Cost: 0.209419802 Sample: <S> too completely <UNK> a <UNK> its big movie when you 'd <UNK> that they could still <UNK> at the material\n","Epoch: 75 Cost: 0.209053382 Sample: <S> the <UNK> <UNK> in the <UNK> <UNK> of <UNK> 's one of <UNK> ' <UNK> is a <UNK> <UNK> <UNK>\n","Epoch: 76 Cost: 0.208796382 Sample: <S> what is <UNK> with <UNK> , not look might be <UNK> over a <UNK> <UNK> away about ... <UNK> of\n","Epoch: 77 Cost: 0.208821893 Sample: <S> too <UNK> <UNK> better to <UNK> <UNK> to <UNK> he might not enjoy <UNK> in an strong for <UNK> and\n","Epoch: 78 Cost: 0.208598793 Sample: <S> at the two story in the <UNK> and his <UNK> , <UNK> <UNK> you 're a <UNK> of the <UNK>\n","Epoch: 79 Cost: 0.208527043 Sample: <S> a <UNK> <UNK> <UNK> ... the <UNK> political <UNK> <UNK> about the time <UNK> <UNK> in horror plot as <UNK>\n","Epoch: 80 Cost: 0.208425596 Sample: <S> human film a good 's <UNK> <UNK> of <UNK> acting girl , for <UNK> to after <UNK> , <UNK> <UNK>\n","Epoch: 81 Cost: 0.208454594 Sample: <S> hollywood to thought shows <UNK> <UNK> <UNK> has an <UNK> in their <UNK> and one sort and <UNK> films that\n","Epoch: 82 Cost: 0.208084151 Sample: <S> -lrb- great <UNK> off some <UNK> <UNK> <UNK> minutes can <UNK> as <UNK> <UNK> <UNK> a rather than <UNK> <UNK>\n","Epoch: 83 Cost: 0.208068594 Sample: <S> the <UNK> <UNK> its <UNK> to <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Epoch: 84 Cost: 0.20824632 Sample: <S> -lrb- <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Epoch: 85 Cost: 0.207983494 Sample: <S> the <UNK> , the problem , looks <UNK> <UNK> of <UNK> <UNK> is enough to <UNK> out of <UNK> of\n","Epoch: 86 Cost: 0.207918972 Sample: <S> have actually so better <UNK> going to this film -- fact about <UNK> <UNK> of <UNK> them but people .\n","Epoch: 87 Cost: 0.207659364 Sample: <S> <UNK> from dark and you is <UNK> <UNK> <UNK> <UNK> above man 's <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> and\n","Epoch: 88 Cost: 0.207821295 Sample: <S> what <UNK> <UNK> ... but it do something title <UNK> <UNK> 's <UNK> <UNK> the film that <UNK> 's <UNK>\n","Epoch: 89 Cost: 0.207569227 Sample: <S> a <UNK> <UNK> are the <UNK> bad and <UNK> of the worst , <UNK> manages to watch the more ,\n","Epoch: 90 Cost: 0.207554519 Sample: <S> although what never <UNK> that <UNK> <UNK> the <UNK> of the <UNK> <UNK> to watch genre whose true romantic the\n","Epoch: 91 Cost: 0.207598075 Sample: <S> <UNK> , there is too <UNK> documentary that 's <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Epoch: 92 Cost: 0.207525179 Sample: <S> one , when now the movie , <UNK> leaves its <UNK> <UNK> series of death into an complex stuff that\n","Epoch: 93 Cost: 0.207294106 Sample: <S> <UNK> <UNK> <UNK> the film us to <UNK> <UNK> and last <UNK> <UNK> and <UNK> after <UNK> of a <UNK>\n","Epoch: 94 Cost: 0.207144707 Sample: <S> the <UNK> viewer one of <UNK> film perfectly bad hollywood <UNK> world <UNK> <UNK> <UNK> to its <UNK> . </S>\n","Epoch: 95 Cost: 0.20716165 Sample: <S> <UNK> <UNK> feeling <UNK> , <UNK> with their <UNK> , <UNK> , good minutes that the people ' almost <UNK>\n","Epoch: 96 Cost: 0.207062393 Sample: <S> not surprisingly , they watch <UNK> of <UNK> <UNK> <UNK> <UNK> <UNK> that n't <UNK> that is movie . </S>\n","Epoch: 97 Cost: 0.207210109 Sample: <S> it 's <UNK> to <UNK> as the last <UNK> and who 's <UNK> <UNK> <UNK> his <UNK> , <UNK> had\n","Epoch: 98 Cost: 0.207011551 Sample: <S> <UNK> <UNK> in the energy art , memorable -- as to look far of director <UNK> 's <UNK> <UNK> ...\n","Epoch: 99 Cost: 0.206606582 Sample: <S> an <UNK> done <UNK> <UNK> are more <UNK> is not too <UNK> leaves some attempt to <UNK> to be <UNK>\n","Epoch: 100 Cost: 0.206817016 Sample: <S> <UNK> keep its film that <UNK> actually <UNK> head , the story 's <UNK> are <UNK> , though <UNK> thoroughly\n","Epoch: 101 Cost: 0.206654057 Sample: <S> <UNK> up <UNK> movies something <UNK> <UNK> and a <UNK> , <UNK> <UNK> and <UNK> <UNK> <UNK> 's <UNK> and\n","Epoch: 102 Cost: 0.20677413 Sample: <S> a <UNK> with <UNK> ultimately above a n't have young <UNK> <UNK> <UNK> to be to <UNK> is <UNK> <UNK>\n","Epoch: 103 Cost: 0.206435978 Sample: <S> it is going , <UNK> a <UNK> <UNK> <UNK> to look , and <UNK> of <UNK> pleasure in a <UNK>\n","Epoch: 104 Cost: 0.206346959 Sample: <S> what makes the time , the whole , <UNK> <UNK> <UNK> , <UNK> <UNK> ... and <UNK> elements <UNK> ,\n","Epoch: 105 Cost: 0.206289813 Sample: <S> the <UNK> <UNK> his <UNK> is one of the movie on <UNK> film 's <UNK> <UNK> has romantic <UNK> sweet\n","Epoch: 106 Cost: 0.20619145 Sample: <S> one of just lacks <UNK> him of the <UNK> ' first take over the <UNK> is being <UNK> <UNK> <UNK>\n","Epoch: 107 Cost: 0.206182972 Sample: <S> ... but the movie here is n't <UNK> is so beautifully <UNK> , <UNK> <UNK> <UNK> <UNK> and fans of\n","Epoch: 108 Cost: 0.206319734 Sample: <S> <UNK> , <UNK> by <UNK> and <UNK> , as the <UNK> as it never feel is a lot of <UNK>\n","Epoch: 109 Cost: 0.206371695 Sample: <S> <UNK> direction , <UNK> people <UNK> and the <UNK> old <UNK> , <UNK> turns and <UNK> and <UNK> a story\n","Epoch: 110 Cost: 0.206098974 Sample: <S> enough to believe by <UNK> <UNK> <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Epoch: 111 Cost: 0.206115589 Sample: <S> <UNK> in movies to the <UNK> <UNK> <UNK> <UNK> of <UNK> entertainment at a <UNK> , and <UNK> than a\n","Epoch: 112 Cost: 0.206197828 Sample: <S> there <UNK> into <UNK> and <UNK> ? </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Epoch: 113 Cost: 0.205843955 Sample: <S> a <UNK> has the <UNK> again <UNK> , it 's <UNK> <UNK> <UNK> some romantic comedy as like two movies\n","Epoch: 114 Cost: 0.205782369 Sample: <S> <UNK> , <UNK> more <UNK> acting , <UNK> <UNK> do n't some <UNK> <UNK> video , the <UNK> of <UNK>\n","Epoch: 115 Cost: 0.205934376 Sample: <S> <UNK> 's <UNK> to <UNK> , <UNK> enough can only for <UNK> , <UNK> was <UNK> down <UNK> a movie\n","Epoch: 116 Cost: 0.205613911 Sample: <S> watching the kind film that <UNK> <UNK> suspense , <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Epoch: 117 Cost: 0.205711335 Sample: <S> like a <UNK> on the well , the movie . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Epoch: 118 Cost: 0.20555903 Sample: <S> a <UNK> you know <UNK> <UNK> <UNK> on <UNK> that <UNK> <UNK> <UNK> performances to <UNK> 's <UNK> written with\n","Epoch: 119 Cost: 0.205707058 Sample: <S> <UNK> <UNK> , silly , but the <UNK> of his should believe <UNK> <UNK> ... well out of <UNK> <UNK>\n","Epoch: 120 Cost: 0.205212593 Sample: <S> it has to take on <UNK> ; just directed and <UNK> french <UNK> by a <UNK> the <UNK> star ultimately\n","Epoch: 121 Cost: 0.205287963 Sample: <S> <UNK> did n't been a time are much film -- as his <UNK> <UNK> thoroughly even <UNK> coming-of-age humor </S>\n","Epoch: 122 Cost: 0.205302149 Sample: <S> i passion <UNK> , <UNK> <UNK> <UNK> , <UNK> , <UNK> gets <UNK> , , <UNK> seems well better <UNK>\n","Epoch: 123 Cost: 0.205560416 Sample: <S> an <UNK> , i was care , cliches . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Epoch: 124 Cost: 0.205214232 Sample: <S> the <UNK> <UNK> , <UNK> us about its <UNK> <UNK> , is not be a sequel as <UNK> , <UNK>\n","Epoch: 125 Cost: 0.205210656 Sample: <S> the <UNK> and complex on and <UNK> hollywood <UNK> <UNK> <UNK> of the very amusing , <UNK> , <UNK> <UNK>\n","Epoch: 126 Cost: 0.205427289 Sample: <S> part at his <UNK> with <UNK> on <UNK> the <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Epoch: 127 Cost: 0.205282211 Sample: <S> get the <UNK> at ... <UNK> to <UNK> still <UNK> <UNK> who come and <UNK> 's <UNK> ideas but <UNK>\n","Epoch: 128 Cost: 0.205299258 Sample: <S> the silly or <UNK> -lrb- <UNK> <UNK> that the <UNK> none of characters and a <UNK> <UNK> <UNK> new imagination\n","Epoch: 129 Cost: 0.205199331 Sample: <S> it 's <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Epoch: 130 Cost: 0.204936206 Sample: <S> it may <UNK> <UNK> <UNK> and a little <UNK> <UNK> <UNK> the way about its <UNK> 's <UNK> <UNK> .\n","Epoch: 131 Cost: 0.204753101 Sample: <S> an <UNK> , <UNK> if you 're a <UNK> for the movie <UNK> <UNK> <UNK> <UNK> 's year . </S>\n","Epoch: 132 Cost: 0.204884559 Sample: <S> <UNK> <UNK> , <UNK> <UNK> <UNK> or easily <UNK> <UNK> only thing in <UNK> <UNK> <UNK> silly , <UNK> are\n","Epoch: 133 Cost: 0.204925448 Sample: <S> <UNK> and <UNK> <UNK> <UNK> from high <UNK> <UNK> <UNK> of the <UNK> <UNK> directed , <UNK> of <UNK> <UNK>\n","Epoch: 134 Cost: 0.204613656 Sample: <S> not it 's entertainment out of the <UNK> the film how to <UNK> and half -lrb- times as a serious\n","Epoch: 135 Cost: 0.204898119 Sample: <S> i ca n't have the filmmakers say me <UNK> is left out of star <UNK> and <UNK> and <UNK> <UNK>\n","Epoch: 136 Cost: 0.20489572 Sample: <S> it <UNK> scenes , along the film to <UNK> and <UNK> their screenplay <UNK> , formula <UNK> <UNK> an <UNK>\n","Epoch: 137 Cost: 0.204730362 Sample: <S> <UNK> -rrb- and the <UNK> <UNK> <UNK> <UNK> `` is a a <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Epoch: 138 Cost: 0.204482153 Sample: <S> <UNK> is <UNK> <UNK> , but <UNK> , while <UNK> <UNK> <UNK> '' was <UNK> <UNK> <UNK> <UNK> -- <UNK>\n","Epoch: 139 Cost: 0.204428807 Sample: <S> a <UNK> <UNK> sure to <UNK> is for the <UNK> version of the <UNK> and <UNK> <UNK> <UNK> you to\n","Epoch: 140 Cost: 0.204306275 Sample: <S> <UNK> <UNK> in those <UNK> <UNK> , in what is not <UNK> <UNK> <UNK> and each <UNK> and <UNK> one\n","Epoch: 141 Cost: 0.204512507 Sample: <S> both <UNK> <UNK> off a human movies that yet it that more head <UNK> film by <UNK> <UNK> less than\n","Epoch: 142 Cost: 0.204454392 Sample: <S> the little <UNK> <UNK> to <UNK> by <UNK> <UNK> director <UNK> that as it and his <UNK> . </S> <PAD>\n","Epoch: 143 Cost: 0.204332501 Sample: <S> true in an <UNK> , we feel like the <UNK> of <UNK> <UNK> 's <UNK> to <UNK> <UNK> <UNK> <UNK>\n","Epoch: 144 Cost: 0.204273224 Sample: <S> being about lives is a <UNK> , in a <UNK> the only interest like <UNK> `` <UNK> to the best\n","Epoch: 145 Cost: 0.204396173 Sample: <S> you 's <UNK> <UNK> scenes is that 's worst <UNK> and more <UNK> <UNK> that <UNK> at kids to <UNK>\n","Epoch: 146 Cost: 0.204201788 Sample: <S> <UNK> and even it characters with better moments of <UNK> for me else goes to <UNK> will <UNK> <UNK> look\n","Epoch: 147 Cost: 0.204178527 Sample: <S> it has the movie the <UNK> the <UNK> violence is <UNK> for <UNK> picture of the <UNK> of <UNK> <UNK>\n","Epoch: 148 Cost: 0.204041898 Sample: <S> <UNK> , next cinema <UNK> history is the message less <UNK> between <UNK> and <UNK> 's <UNK> a hollywood <UNK>\n","Epoch: 149 Cost: 0.204153821 Sample: <S> <UNK> to being funny it 's <UNK> off , wo n't <UNK> <UNK> <UNK> 's <UNK> fresh action . </S>\n","Epoch: 150 Cost: 0.204099432 Sample: <S> ... an <UNK> <UNK> the most <UNK> , rich <UNK> to the <UNK> years that she <UNK> , amusing ,\n","Epoch: 151 Cost: 0.204286337 Sample: <S> they matter , <UNK> two time , despite <UNK> he <UNK> 's <UNK> , and a <UNK> social , <UNK>\n","Epoch: 152 Cost: 0.203861341 Sample: <S> a <UNK> for <UNK> <UNK> have been <UNK> ' <UNK> right too <UNK> to <UNK> <UNK> , <UNK> by the\n","Epoch: 153 Cost: 0.203958392 Sample: <S> ... far hour <UNK> make the film in the character , <UNK> comes up the <UNK> romantic movie that starts\n","Epoch: 154 Cost: 0.203856647 Sample: <S> the film <UNK> <UNK> is a fascinating ... the film sense but it 's <UNK> <UNK> the events , but\n","Epoch: 155 Cost: 0.203520417 Sample: <S> the <UNK> <UNK> that 's <UNK> <UNK> looks . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Epoch: 156 Cost: 0.203757241 Sample: <S> ` <UNK> <UNK> , and <UNK> <UNK> to <UNK> <UNK> <UNK> <UNK> , ' <UNK> the <UNK> in the <UNK>\n","Epoch: 157 Cost: 0.203825071 Sample: <S> it <UNK> <UNK> without <UNK> and <UNK> , <UNK> performances with any elements of its world over this works on\n","Epoch: 158 Cost: 0.20381622 Sample: <S> <UNK> the <UNK> a <UNK> and two <UNK> <UNK> 's <UNK> or <UNK> on without an <UNK> woman <UNK> <UNK>\n","Epoch: 159 Cost: 0.203713685 Sample: <S> a <UNK> her sense of something <UNK> <UNK> the right <UNK> , romantic comedy <UNK> like a side . </S>\n","Epoch: 160 Cost: 0.203671679 Sample: <S> a <UNK> the <UNK> , <UNK> for the <UNK> <UNK> to watching us that what that its <UNK> about the\n","Epoch: 161 Cost: 0.203855395 Sample: <S> <UNK> a good , <UNK> is <UNK> 's <UNK> <UNK> that it 's half , <UNK> <UNK> and <UNK> <UNK>\n","Epoch: 162 Cost: 0.203544185 Sample: <S> what 's <UNK> 's right as a <UNK> , the film why offers a <UNK> <UNK> ii -lrb- a <UNK>\n","Epoch: 163 Cost: 0.203597307 Sample: <S> its the <UNK> over his subject matter <UNK> can be a <UNK> , a end beyond <UNK> the humor and\n","Epoch: 164 Cost: 0.203516483 Sample: <S> <UNK> the <UNK> about an <UNK> <UNK> finally like the <UNK> , <UNK> and a <UNK> <UNK> for a movie\n","Epoch: 165 Cost: 0.203397527 Sample: <S> i could have no <UNK> <UNK> time , <UNK> could be <UNK> in its <UNK> <UNK> , some and a\n","Epoch: 166 Cost: 0.203484476 Sample: <S> <UNK> has the way is so <UNK> back without <UNK> of <UNK> his <UNK> of the <UNK> to <UNK> adventure\n","Epoch: 167 Cost: 0.203564152 Sample: <S> <UNK> with <UNK> of its <UNK> <UNK> every <UNK> <UNK> was n't some <UNK> , the story is very <UNK>\n","Epoch: 168 Cost: 0.203231022 Sample: <S> <UNK> <UNK> running at <UNK> , the story , and next , funny and <UNK> out the subject , solid\n","Epoch: 169 Cost: 0.203335971 Sample: <S> <UNK> for the <UNK> <UNK> as <UNK> comedy , <UNK> at cliches <UNK> <UNK> , viewers come off the <UNK>\n","Epoch: 170 Cost: 0.203226864 Sample: <S> it she 's <UNK> ' <UNK> <UNK> its <UNK> -lrb- a <UNK> the performances in all to <UNK> <UNK> or\n","Epoch: 171 Cost: 0.202982605 Sample: <S> the title <UNK> <UNK> of <UNK> <UNK> into <UNK> is that 's <UNK> on the the <UNK> <UNK> , contrived\n","Epoch: 172 Cost: 0.203070551 Sample: <S> way for both a <UNK> rare of its <UNK> to <UNK> <UNK> <UNK> subject is almost what is <UNK> a\n","Epoch: 173 Cost: 0.202921614 Sample: <S> in the <UNK> and <UNK> action cliches so <UNK> performances , goes to true in <UNK> <UNK> has lost the\n","Epoch: 174 Cost: 0.20321463 Sample: <S> a <UNK> , dumb style <UNK> to <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Epoch: 175 Cost: 0.203186557 Sample: <S> as i still <UNK> never <UNK> by <UNK> <UNK> , <UNK> as -lrb- <UNK> and that is such <UNK> the\n","Epoch: 176 Cost: 0.20288533 Sample: <S> ... <UNK> with the most <UNK> seem a film <UNK> by all two <UNK> in the story as the screen\n","Epoch: 177 Cost: 0.202854291 Sample: <S> a <UNK> his <UNK> 's <UNK> and the <UNK> <UNK> her tv humor by a lot of this mess that\n","Epoch: 178 Cost: 0.20320636 Sample: <S> director , <UNK> and <UNK> is shot , it is a <UNK> <UNK> proves with <UNK> <UNK> , has its\n","Epoch: 179 Cost: 0.202818945 Sample: <S> proves of <UNK> and <UNK> , <UNK> <UNK> <UNK> a <UNK> that it may <UNK> <UNK> his summer already funny\n","Epoch: 180 Cost: 0.203071192 Sample: <S> as a <UNK> to <UNK> and you can <UNK> , intelligent and a <UNK> sometimes you might be he looks\n","Epoch: 181 Cost: 0.202883571 Sample: <S> <UNK> , the <UNK> <UNK> with hard to <UNK> up <UNK> <UNK> as <UNK> will <UNK> offers <UNK> of art\n","Epoch: 182 Cost: 0.203078762 Sample: <S> i <UNK> <UNK> cast is a <UNK> , rare <UNK> <UNK> in the <UNK> and it <UNK> <UNK> <UNK> <UNK>\n","Epoch: 183 Cost: 0.202885166 Sample: <S> or ideas by <UNK> more so <UNK> <UNK> about <UNK> and then each three <UNK> is so <UNK> <UNK> <UNK>\n","Epoch: 184 Cost: 0.20282 Sample: <S> a <UNK> <UNK> <UNK> melodrama , the <UNK> life , <UNK> , <UNK> at <UNK> , pretty woman 's characters\n","Epoch: 185 Cost: 0.20266144 Sample: <S> the film like a movie as a <UNK> , <UNK> <UNK> than it be very <UNK> <UNK> -- so <UNK>\n","Epoch: 186 Cost: 0.202700749 Sample: <S> with <UNK> death . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Epoch: 187 Cost: 0.202437267 Sample: <S> as <UNK> of the <UNK> <UNK> <UNK> , which also <UNK> may <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Epoch: 188 Cost: 0.202836558 Sample: <S> <UNK> , beautifully <UNK> and in this <UNK> to <UNK> <UNK> power of her <UNK> <UNK> , <UNK> <UNK> are\n","Epoch: 189 Cost: 0.202860847 Sample: <S> in its <UNK> her material comes out it the <UNK> 's good <UNK> <UNK> out of dark style with one\n","Epoch: 190 Cost: 0.202560008 Sample: <S> that the <UNK> <UNK> is not a feature wo n't really made <UNK> of at <UNK> it is <UNK> of\n","Epoch: 191 Cost: 0.202694833 Sample: <S> <UNK> films -- <UNK> but <UNK> , <UNK> it about the <UNK> down and special , <UNK> and <UNK> can\n","Epoch: 192 Cost: 0.202657402 Sample: <S> it <UNK> in <UNK> in some <UNK> <UNK> and <UNK> that so <UNK> <UNK> <UNK> <UNK> to <UNK> a perfectly\n","Epoch: 193 Cost: 0.202554658 Sample: <S> a need when you <UNK> watching my big <UNK> the <UNK> soap opera never <UNK> about <UNK> an <UNK> <UNK>\n","Epoch: 194 Cost: 0.20260559 Sample: <S> <UNK> off , to work of the feature is <UNK> home movie <UNK> actually as <UNK> and <UNK> and <UNK>\n","Epoch: 195 Cost: 0.202303723 Sample: <S> when you 'll have the film is the <UNK> -- the first high `` <UNK> become in a <UNK> <UNK>\n","Epoch: 196 Cost: 0.202516392 Sample: <S> she is <UNK> become how in the <UNK> <UNK> , <UNK> <UNK> study , filmmaker as its <UNK> for <UNK>\n","Epoch: 197 Cost: 0.202388406 Sample: <S> <UNK> in hollywood <UNK> above the script itself down entertaining <UNK> <UNK> 's emotional filmmaking , if `` <UNK> as\n","Epoch: 198 Cost: 0.202474803 Sample: <S> more mess in the level is <UNK> <UNK> <UNK> '' at that only <UNK> that me take for the <UNK>\n","Epoch: 199 Cost: 0.202471569 Sample: <S> this movie it 's only mystery that has <UNK> <UNK> , <UNK> and face about <UNK> <UNK> hours , it\n","Epoch: 200 Cost: 0.202160448 Sample: <S> he <UNK> is the film for this barely <UNK> , intriguing , <UNK> <UNK> -- <UNK> <UNK> to big <UNK>\n","Epoch: 201 Cost: 0.202471346 Sample: <S> <UNK> in the kind of <UNK> the <UNK> , good more <UNK> could come back before what 's <UNK> <UNK>\n","Epoch: 202 Cost: 0.20247221 Sample: <S> although <UNK> dramatic <UNK> <UNK> to its <UNK> <UNK> , <UNK> movies in <UNK> <UNK> film 's <UNK> . </S>\n","Epoch: 203 Cost: 0.202289119 Sample: <S> <UNK> life is about <UNK> that it 's <UNK> me that <UNK> <UNK> ... be shot your <UNK> 's <UNK>\n","Epoch: 204 Cost: 0.202119738 Sample: <S> instead of the <UNK> <UNK> you to <UNK> <UNK> and <UNK> <UNK> 's <UNK> <UNK> about <UNK> , its plot\n","Epoch: 205 Cost: 0.202150658 Sample: <S> <UNK> enough of <UNK> of need to <UNK> <UNK> -lrb- <UNK> a <UNK> <UNK> <UNK> <UNK> <UNK> by the film\n","Epoch: 206 Cost: 0.202254385 Sample: <S> <UNK> <UNK> <UNK> <UNK> <UNK> comedy , two <UNK> into a <UNK> <UNK> <UNK> when a <UNK> , <UNK> <UNK>\n","Epoch: 207 Cost: 0.202354193 Sample: <S> i 've <UNK> everyone be the <UNK> are <UNK> <UNK> jokes of <UNK> ... and a <UNK> . </S> <PAD>\n","Epoch: 208 Cost: 0.202131346 Sample: <S> a <UNK> rather than a <UNK> animation <UNK> <UNK> that an exercise to <UNK> amusing into <UNK> it 's <UNK>\n","Epoch: 209 Cost: 0.202125177 Sample: <S> its <UNK> , <UNK> like a <UNK> around the dialogue and good film is of the personal good , and\n","Epoch: 210 Cost: 0.202275842 Sample: <S> all his plot <UNK> of men than last <UNK> cliches and ` <UNK> may <UNK> , earnest experience with <UNK>\n","Epoch: 211 Cost: 0.202232599 Sample: <S> this <UNK> looks far <UNK> a thoughtful ... <UNK> of the first take through both a terrific <UNK> : <UNK>\n","Epoch: 212 Cost: 0.202140406 Sample: <S> -lrb- <UNK> 's performance is <UNK> in culture before you 're sure of <UNK> and <UNK> , and <UNK> is\n","Epoch: 213 Cost: 0.202082217 Sample: <S> rather <UNK> family <UNK> <UNK> , <UNK> in <UNK> , and <UNK> special drama about time and <UNK> , <UNK>\n","Epoch: 214 Cost: 0.201927036 Sample: <S> the cinematic the best in <UNK> , <UNK> <UNK> , just like a <UNK> <UNK> <UNK> , one that <UNK>\n","Epoch: 215 Cost: 0.202056482 Sample: <S> the film ... a <UNK> an <UNK> who love and it 's <UNK> , <UNK> <UNK> there 's <UNK> on\n","Epoch: 216 Cost: 0.202038646 Sample: <S> is <UNK> to lack of <UNK> in their <UNK> 's art of ideas by fun of a <UNK> characters ,\n","Epoch: 217 Cost: 0.201899379 Sample: <S> a <UNK> <UNK> <UNK> , <UNK> <UNK> the case more <UNK> plot men with <UNK> to actors are <UNK> not\n","Epoch: 218 Cost: 0.201890916 Sample: <S> <UNK> of <UNK> so from a <UNK> 's <UNK> <UNK> in its <UNK> , ' performance -- and <UNK> ,\n","Epoch: 219 Cost: 0.201881319 Sample: <S> <UNK> slow to life in , <UNK> ; <UNK> , an <UNK> <UNK> <UNK> of watch <UNK> is <UNK> ,\n","Epoch: 220 Cost: 0.201950461 Sample: <S> he 's <UNK> , just <UNK> and a big look a <UNK> and even almost <UNK> <UNK> what you quite\n","Epoch: 221 Cost: 0.201961681 Sample: <S> <UNK> , if he is a <UNK> <UNK> of <UNK> <UNK> <UNK> at interesting and <UNK> 'd <UNK> to be\n","Epoch: 222 Cost: 0.201851517 Sample: <S> there 's <UNK> <UNK> , <UNK> but no <UNK> <UNK> of the music performances for the movie that <UNK> .\n","Epoch: 223 Cost: 0.201686159 Sample: <S> despite their comic with <UNK> of the past <UNK> <UNK> <UNK> to <UNK> , shot on <UNK> and the <UNK>\n","Epoch: 224 Cost: 0.201631442 Sample: <S> the beautiful . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Epoch: 225 Cost: 0.201757118 Sample: <S> <UNK> scenes of video , <UNK> is the is certainly <UNK> feature to <UNK> <UNK> on the film plays like\n","Epoch: 226 Cost: 0.201499313 Sample: <S> even <UNK> <UNK> , , both <UNK> <UNK> , <UNK> <UNK> <UNK> , <UNK> as <UNK> that <UNK> <UNK> ,\n","Epoch: 227 Cost: 0.202112272 Sample: <S> in <UNK> most <UNK> film gets the video take as on this is <UNK> and almost a <UNK> . </S>\n","Epoch: 228 Cost: 0.20180513 Sample: <S> <UNK> , <UNK> , <UNK> , about each charm , to not only that one of <UNK> performance , while\n","Epoch: 229 Cost: 0.201520532 Sample: <S> there has been <UNK> of <UNK> because it manages between <UNK> <UNK> , as there , <UNK> , but <UNK>\n","Epoch: 230 Cost: 0.20163779 Sample: <S> the way that a <UNK> the <UNK> animation , and <UNK> kids 're <UNK> <UNK> with of the ` <UNK>\n","Epoch: 231 Cost: 0.201699868 Sample: <S> the <UNK> , <UNK> <UNK> <UNK> , <UNK> for the <UNK> in a <UNK> everything is <UNK> <UNK> <UNK> and\n","Epoch: 232 Cost: 0.201540604 Sample: <S> <UNK> with a <UNK> is a <UNK> if not <UNK> music <UNK> of during the <UNK> to be quirky <UNK>\n","Epoch: 233 Cost: 0.201699093 Sample: <S> a <UNK> to <UNK> <UNK> <UNK> but not by <UNK> to play <UNK> ... you 're an <UNK> who can\n","Epoch: 234 Cost: 0.201621309 Sample: <S> a visual <UNK> <UNK> <UNK> <UNK> <UNK> by <UNK> its audience to see it may make a <UNK> a <UNK>\n","Epoch: 235 Cost: 0.201647937 Sample: <S> still is a <UNK> enough <UNK> <UNK> also get the <UNK> <UNK> to the <UNK> <UNK> and find movies ,\n","Epoch: 236 Cost: 0.20165652 Sample: <S> a <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Epoch: 237 Cost: 0.201445937 Sample: <S> a <UNK> <UNK> with <UNK> <UNK> 's men by <UNK> and a <UNK> , <UNK> in a sometimes <UNK> ,\n","Epoch: 238 Cost: 0.201392069 Sample: <S> <UNK> , the past the <UNK> <UNK> 's <UNK> <UNK> has his material the movie <UNK> characters at <UNK> the\n","Epoch: 239 Cost: 0.201715082 Sample: <S> those of <UNK> <UNK> <UNK> , a <UNK> the <UNK> <UNK> and an good minutes of <UNK> <UNK> of the\n","Epoch: 240 Cost: 0.201427057 Sample: <S> <UNK> of <UNK> , <UNK> from <UNK> <UNK> emotional <UNK> does n't <UNK> <UNK> as <UNK> and <UNK> , <UNK>\n","Epoch: 241 Cost: 0.201589808 Sample: <S> <UNK> , , or the <UNK> to <UNK> and as <UNK> , this movie before them should so time ,\n","Epoch: 242 Cost: 0.201286167 Sample: <S> some <UNK> and flat direction to keep on the <UNK> of a <UNK> -- a few <UNK> from the <UNK>\n","Epoch: 243 Cost: 0.201726526 Sample: <S> what 's kind of <UNK> <UNK> on his <UNK> it goes interest by <UNK> <UNK> and <UNK> which <UNK> 's\n","Epoch: 244 Cost: 0.201498881 Sample: <S> <UNK> <UNK> in the <UNK> <UNK> <UNK> is take a <UNK> <UNK> , <UNK> 's <UNK> <UNK> of a moving\n","Epoch: 245 Cost: 0.201478466 Sample: <S> <UNK> - and <UNK> <UNK> . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Epoch: 246 Cost: 0.201335788 Sample: <S> not less than <UNK> into the <UNK> of <UNK> but be <UNK> -- of <UNK> <UNK> a effort complex ,\n","Epoch: 247 Cost: 0.201447889 Sample: <S> it 's `` <UNK> <UNK> gives quite got <UNK> ii ... but <UNK> <UNK> and it 's most a fine\n","Epoch: 248 Cost: 0.201473564 Sample: <S> it 's <UNK> <UNK> ... ' this ... <UNK> its own story of a <UNK> -lrb- the movie , <UNK>\n","Epoch: 249 Cost: 0.201522201 Sample: <S> one of <UNK> 's a <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> and problem through as <UNK> <UNK> , <UNK> by\n","Epoch: 250 Cost: 0.201393366 Sample: <S> it <UNK> , <UNK> , <UNK> 's <UNK> for his <UNK> a big <UNK> <UNK> to a <UNK> <UNK> ,\n"]}]},{"cell_type":"markdown","metadata":{"id":"2kPUX6OrgUva"},"source":["Now we can draw as many samples as we like."]},{"cell_type":"code","source":["for _ in range(10):\n","    print(model.sample())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"APCy1BmKKmCq","executionInfo":{"status":"ok","timestamp":1645210252281,"user_tz":-60,"elapsed":3103,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"7bdf1656-b469-40b1-ca94-4b336bddb8c7"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["<S> like this case of the movie <UNK> <UNK> one <UNK> <UNK> <UNK> <UNK> , it does n't another movie that\n","<S> though its <UNK> up but his <UNK> , truly <UNK> up <UNK> , the camera feels so <UNK> and or\n","<S> finally <UNK> and <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> <UNK> movies <UNK> <UNK> to heart and beautifully <UNK> <UNK>\n","<S> i 've <UNK> <UNK> of <UNK> film 's <UNK> and <UNK> dramatic <UNK> minutes more so screenplay are <UNK> <UNK>\n","<S> the movie together performances to a years <UNK> the <UNK> in the <UNK> takes <UNK> 's most <UNK> filmmaking .\n","<S> <UNK> <UNK> , directed but a <UNK> dumb writing which would be <UNK> <UNK> ; the <UNK> that the <UNK>\n","<S> the <UNK> with <UNK> book ... <UNK> of the art with <UNK> <UNK> and <UNK> <UNK> work that <UNK> but\n","<S> each years -- 's <UNK> during a , emotionally <UNK> on its <UNK> 's <UNK> out , has a <UNK>\n","<S> <UNK> music , in , makes a time is an <UNK> seems impossible is <UNK> <UNK> must n't not much\n","<S> with this cliches and <UNK> as <UNK> 's <UNK> <UNK> to <UNK> <UNK> <UNK> filmmakers , <UNK> and <UNK> no\n"]}]},{"cell_type":"markdown","metadata":{"id":"5HsJ9ONQgUvk"},"source":["### Part 2: Questions"]},{"cell_type":"markdown","metadata":{"id":"pF05DtcWgUvm"},"source":["**Question 1:** Looking at the samples that your model produced towards the end of training, point out three properties of (written) English that it seems to have learned."]},{"cell_type":"markdown","source":["The phrases are generally well formed. The part of speech categories of the words are generally correct (determiners and adjectives before nouns).\n","\n","If we look at small groups of words most of them make sense, most phrases make sense. \n","\n","Agrement is ok many times but there are also some mistakes.\n","\n","\n","\n"],"metadata":{"id":"lFZ9qHUoh4fx"}},{"cell_type":"markdown","metadata":{"id":"-MeNJ9k-gUvp"},"source":["**Question 2:** If we could make the model as big as we wanted, train as long as we wanted, and adjust or remove dropout at will, could we ever get the model to reach a cost value of 0.0? In a single sentence, say why."]},{"cell_type":"markdown","source":["A very big model without dropout would be able to memorise the whole training data and reach a cost value of 0.0. However, that would mean that the model is overfitting to the training data."],"metadata":{"id":"vyl-v510QHoz"}},{"cell_type":"markdown","metadata":{"id":"VXKts8jJgUvs"},"source":["**Question 3:** Give an example of a situation where the LSTM language model's ability to propagate information across many steps (when trained for long enough, at least) would cause it to reach a better cost value than a model like a simple RNN without that ability. (Answer in one sentence or so.)"]},{"cell_type":"markdown","source":["RNN gradients can vanish or explode with long sentences. LSTM models are better for long sentences where there are long distance dependencies. These sentences are common in language."],"metadata":{"id":"iRx9JTkoRpc4"}},{"cell_type":"markdown","metadata":{"id":"75iq_DjIgUvv"},"source":["**Question 4:** Would the model be any worse if we were to just delete unknown words instead of using an `<UNK>` token? (Answer in one sentence or so.)"]},{"cell_type":"markdown","source":["I think that the unkown token generally helps the model. However, it would be better if the sampler took the next most probable word instead. Also, having a bigger vocabulary would reduce the number of unkown tokens."],"metadata":{"id":"FSwa5jR7SaBX"}},{"cell_type":"markdown","metadata":{"id":"gb-PCtUagUvy"},"source":["# Atribution:\n","Adapted by Oier Lopez de Lacalle, Olatz Perez de Viñaspre and Ander Barrena, based on a notebook by Sam Bowman at NYU"]}]}