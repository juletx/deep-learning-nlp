{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. LSTM based Language Model.ipynb","provenance":[{"file_id":"1opzCFCVHc7hzsXbOuRoF8ZenFlayg8im","timestamp":1547195062164},{"file_id":"1xen3vjgC50ALslhxIVJVAPfyNPv0nORr","timestamp":1547136286563}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"skxNOBVngUuI"},"source":["# Assignment 3: Language Modelling with LSTM networks"]},{"cell_type":"markdown","metadata":{"id":"od8wz1GsgUuO"},"source":["In this assignment, you will implement an LSTM based language model. We strongly recommend to finish first _lab 4_, which is closely related and is much simpler."]},{"cell_type":"markdown","metadata":{"id":"LG0W24LVgUuR"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"T9YkdodygUuW"},"source":["First, let's load the data as before."]},{"cell_type":"code","metadata":{"id":"dqj5ZEmHqZHg"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lhVrZbAAgUuZ"},"source":["import re\n","import random\n","\n","# Let's do 2-way positive/negative classification instead of 5-way\n","easy_label_map = {0:0, 1:0, 2:None, 3:1, 4:1}\n","\n","def load_sst_data(path):\n","    data = []\n","    with open(path) as f:\n","        for i, line in enumerate(f): \n","            example = {}\n","            \n","            # Strip out the parse information and the phrase labels---we don't need those here\n","            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n","            example['text'] = text[1:]\n","            data.append(example)\n","\n","    random.seed(1)\n","    random.shuffle(data)\n","    return data\n","\n","sst_home = 'drive/My Drive/Colab Notebooks/dl4nlp_labs/data/trees/'\n","training_set = load_sst_data(sst_home + '/train.txt')\n","dev_set = load_sst_data(sst_home + '/dev.txt')\n","test_set = load_sst_data(sst_home + '/test.txt')\n","\n","# Note: Unlike with k-nearest neighbors, evaluation here should be fast, and we don't need to\n","# trim down the dev and test sets. "],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"TAVmSzNmmf-u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r7vCn9ElgUui"},"source":["Next, we'll convert the data to index vectors.\n","\n","To simplify your implementation, we'll use a fixed unrolling length of 20. This means that we'll have to expand each sentence into a sequence of 21 word indices. In the conversion process, we'll mark the start of each sentence with a special word symbol `<S>`, mark the end of each sentence (if it occurs within the first 21 words) with a special word symbol `</S>`, mark extra tokens after `</S>` with a special word symbol `<PAD>`, and mark out-of-vocabulary words with `<UNK>`, for unknown. As in the previous assignment, we'll use a very small vocabulary for this assignment, so you'll see `<UNK>` often."]},{"cell_type":"code","metadata":{"id":"HNfCXqF8gUul"},"source":["import collections\n","import numpy as np\n","\n","def sentence_to_padded_index_sequence(datasets):\n","    '''Annotates datasets with feature vectors.'''\n","    \n","    START = \"<S>\"\n","    END = \"</S>\"\n","    END_PADDING = \"<PAD>\"\n","    UNKNOWN = \"<UNK>\"\n","    SEQ_LEN = 21\n","    \n","    # Extract vocabulary\n","    def tokenize(string):\n","        return string.lower().split()\n","    \n","    word_counter = collections.Counter()\n","    for example in datasets[0]:\n","        word_counter.update(tokenize(example['text']))\n","    \n","    vocabulary = set([word for word in word_counter if word_counter[word] > 25])\n","    vocabulary = list(vocabulary)\n","    vocabulary = [START, END, END_PADDING, UNKNOWN] + vocabulary\n","        \n","    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n","    indices_to_words = {v: k for k, v in word_indices.items()}\n","        \n","    for i, dataset in enumerate(datasets):\n","        for example in dataset:\n","            example['index_sequence'] = np.zeros((SEQ_LEN), dtype=np.int32)\n","            \n","            token_sequence = [START] + tokenize(example['text']) + [END]\n","            \n","            for i in range(SEQ_LEN):\n","                if i < len(token_sequence):\n","                    if token_sequence[i] in word_indices:\n","                        index = word_indices[token_sequence[i]]\n","                    else:\n","                        index = word_indices[UNKNOWN]\n","                else:\n","                    index = word_indices[END_PADDING]\n","                example['index_sequence'][i] = index\n","    return indices_to_words, word_indices\n","    \n","indices_to_words, word_indices = sentence_to_padded_index_sequence([training_set, dev_set, test_set])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oCU1I5qSgUus"},"source":["print(training_set[18])\n","print(len(word_indices))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ASkTrIogUu3"},"source":["## Assignments: \n","### Part 1: Implementation"]},{"cell_type":"markdown","metadata":{"id":"I__f8rvOgUu5"},"source":["Now, using the starter code and hyperparameter values provided below, implement an LSTM language model with dropout on the non-recurrent connections. Use the standard form of the LSTM reflected in the slides (without peepholes). **You should only have to edit the marked sections of code to build the base LSTM**, though implementing dropout properly may require small changes to the main training loop and to brittle_sampler().\n","\n","**Don't use any TensorFlow code that is specifically built for RNNs**. If a TF function has 'recurrent', 'sequence', 'LSTM', or 'RNN' in its name, you should built it yourself instead of using it. (Your version will likely be much simpler, by the way, since these built in methods are powerful but fairly complex and potentially confusing.)\n","\n","We won't be evaluating our model in the conventional way (perplexity on a held-out test set) for a few reasons: to save time, because we have no baseline to compare against, and because overfitting the training set is a less immediate concern with these models than it was with sentence classifiers. Instead, we'll use the value of the cost function to make sure that the model is converging as expected, and we'll use samples drawn from the model to qualitatively evaluate it.\n","\n","**Tips**: \n","- Check the code for the GRU based sentiment classifier (lab 4), specially the part where the RNN structure is defined.\n","- You'll need to use `tf.nn.embedding_lookup()`, `tf.nn.sparse_softmax_cross_entropy_with_logits()`, and `tf.split()` at least once each. All three should be easy to Google, though the last homework and the last exercise should show examples of the first two.\n","- As before, you'll want to initialize your trained parameters using something like `tf.random_normal(..., stddev=0.1)`\n","\n","**TODOS:**\n","- **TODO1**: Define the parameters of the LSTM (check the given slides in class)\n","- **TODO2**: Build the LSTM LM (follow the instructions in the code-comments)\n"]},{"cell_type":"code","metadata":{"id":"fKishKAwgUu8"},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","tf.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8o4syEQOgUvG"},"source":["class LanguageModel:\n","    def __init__(self, vocab_size, sequence_length):\n","        # Define the hyperparameters\n","        self.learning_rate = 0.3  # Should be about right\n","        self.training_epochs = 250  # How long to train for - chosen to fit within class time\n","        self.display_epoch_freq = 1  # How often to test and print out statistics\n","        self.dim = 32  # The dimension of the hidden state of the RNN\n","        self.embedding_dim = 16  # The dimension of the learned word embeddings\n","        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n","        self.vocab_size = vocab_size  # Defined by the file reader above\n","        self.sequence_length = sequence_length  # Defined by the file reader above\n","        self.rate = 0.25  # Used in dropout (at training time only, not at sampling time)\n","        \n","        #### Start main editable code block ####\n","        self.trainable_variables = []\n","        # Your model should populate the following four python lists.\n","        # self.logits should contain one [batch_size, vocab_size]-shaped TF tensor of logits \n","        #   for each of the 20 steps of the model.\n","        # self.costs should contain one [batch_size]-shaped TF tensor of cross-entropy loss \n","        #   values for each of the 20 steps of the model.\n","        # self.h and c should each start contain one [batch_size, dim]-shaped TF tensor of LSTM\n","        #   activations for each of the 21 *states* of the model -- one tensor of zeros for the \n","        #   starting state followed by one tensor each for the remaining 20 steps.\n","        # Don't rename any of these variables or change their purpose -- they'll be needed by the\n","        # pre-built sampler.\n","        \n","        self.h_zero = tf.zeros([self.batch_size, self.dim])\n","        self.c_zero = tf.zeros([self.batch_size, self.dim])\n","        \n","    \n","    def model(self,x,rate,sample=False,h_zero=None,c_zero=None):\n","        def step(x, next_x, h_prev, c_prev):\n","            # TODO\n","            return logits, costs, h_prev, c_prev\n","\n","        self.x_slices = tf.split(x, self.sequence_length, 1)\n","        all_logits = []\n","        all_costs = []\n","        \n","        if h_zero != None and c_zero != None:\n","          self.h = [h_zero]\n","          self.c = [c_zero]\n","        else:\n","          self.h = [self.h_zero]\n","          self.c = [self.c_zero]\n","        \n","        x = tf.reshape(self.x_slices[0], [-1])\n","        \n","        #TODO unroll\n","        \n","        return all_logits,all_costs\n","\n","        #### End main editable code block ####\n","        \n","\n","    def train(self, training_data):\n","        def get_minibatch(dataset, start_index, end_index):\n","            indices = range(start_index, end_index)\n","            vectors = np.vstack([dataset[i]['index_sequence'] for i in indices])\n","            return vectors\n","        \n","       \n","        print('Training.')\n","\n","        # Training cycle\n","        for epoch in range(self.training_epochs):\n","            random.shuffle(training_set)\n","            avg_cost = 0.\n","            total_batch = int(len(training_set) / self.batch_size)\n","            \n","            # Loop over all batches in epoch\n","            for i in range(total_batch):\n","                # Assemble a minibatch of the next B examples\n","                minibatch_vectors = np.int32(get_minibatch(training_set, self.batch_size * i, self.batch_size * (i + 1)))\n","\n","                # Run the optimizer to take a gradient step, and also fetch the value of the \n","                # cost function for logging\n","                with tf.GradientTape() as tape:\n","                  logits,costs = self.model(minibatch_vectors,self.rate)\n","                  # Sum costs for each word in each example, but average cost across examples.\n","                  costs_tensor = tf.concat([tf.expand_dims(cost, 1) for cost in costs], 1)\n","                  cost_per_example = tf.reduce_sum(costs_tensor, 1)\n","                  total_cost = tf.reduce_mean(cost_per_example)\n","            \n","                # This performs the main SGD update equation\n","                gradients = tape.gradient(total_cost, self.trainable_variables)\n","                optimizer = tf.optimizers.SGD(self.learning_rate)\n","                optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n","                                                            \n","                # Compute average loss\n","                avg_cost += total_cost / (total_batch * self.batch_size)\n","                \n","            # Display some statistics about the step\n","            if (epoch+1) % self.display_epoch_freq == 0:\n","                tf.print(\"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \"Sample:\", self.sample())\n","    \n","    def sample(self):\n","        # This samples a sequence of tokens from the model starting with <S>.\n","        # We only ever run the first timestep of the model, and use an effective batch size of one\n","        # but we leave the model unrolled for multiple steps, and use the full batch size to simplify \n","        # the training code. This slows things down.\n","\n","        def brittle_sampler():\n","            # The main sampling code. Can fail randomly due to rounding errors that yield probibilities\n","            # that don't sum to one.\n","            \n","            word_indices = [0] # 0 here is the \"<S>\" symbol\n","            for i in range(self.sequence_length - 1):\n","                dummy_x = np.zeros((self.batch_size, self.sequence_length),dtype=np.int32)\n","                dummy_x[0][0] = word_indices[-1]\n","                model_h = None\n","                model_c = None\n","                if i > 0:\n","                    model_h = h\n","                    model_c = c\n","                h, c, logits = self.model(dummy_x,0.0,sample=True,h_zero=model_h,c_zero=model_c)\n","                logits = logits[0, :] # Discard all but first batch entry\n","                exp_logits = np.exp(logits - np.max(logits))\n","                distribution = exp_logits / exp_logits.sum()\n","                sampled_index = np.flatnonzero(np.random.multinomial(1, distribution))[0]\n","                word_indices.append(sampled_index)\n","            words = [indices_to_words[index] for index in word_indices]\n","            return ' '.join(words)\n","        \n","        while True:\n","            try:\n","              sample = brittle_sampler()\n","              return sample\n","            except ValueError as e:  # Retry if we experience a random failure.\n","              pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c7-7pfUSgUvN"},"source":["Now let's train it.\n","\n","Once you're confident your model is doing what you want, let it run for the full 250 epochs. This will take some time—likely between five and thirty minutes. If it much longer on a reasonably modern laptop—more than an hour—that suggests serious problems with your implementation. A properly implemented model with dropout should reach an average cost of less than 0.22 quickly, and then slowly improve from there. We train the model for a fairly long time because these small improvements in cost correspond to fairly large improvements in sample quality.\n","\n","Samples from a trained models should have coherent portions, but they will not resemble interpretable English sentences. Here are three examples from a model with a cost value of 0.202:\n","\n","`<S> the good <UNK> and <UNK> and <UNK> <UNK> with predictable and <UNK> , but also does one of -lrb- <UNK>`\n","\n","`<S> <UNK> has <UNK> actors seems done <UNK> would these <UNK> <UNK> to <UNK> <UNK> <UNK> 're <UNK> to mind .`\n","\n","`<S> an action story that was because the <UNK> <UNK> are when <UNK> as ``` <UNK> '' ' it is any`\n","\n","`-lrb-` and `-rrb` are the way that left and right parentheses are represented in the corpus."]},{"cell_type":"code","metadata":{"id":"zifuF0xQgUvR"},"source":["model = LanguageModel(len(word_indices), 21)\n","model.train(training_set)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2kPUX6OrgUva"},"source":["Now we can draw as many samples as we like."]},{"cell_type":"code","metadata":{"id":"94XdnKR5gUvf"},"source":["model.sample()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5HsJ9ONQgUvk"},"source":["### Part 2: Questions"]},{"cell_type":"markdown","metadata":{"id":"pF05DtcWgUvm"},"source":["**Question 1:** Looking at the samples that your model produced towards the end of training, point out three properties of (written) English that it seems to have learned."]},{"cell_type":"markdown","metadata":{"id":"-MeNJ9k-gUvp"},"source":["**Question 2:** If we could make the model as big as we wanted, train as long as we wanted, and adjust or remove dropout at will, could we ever get the model to reach a cost value of 0.0? In a single sentence, say why."]},{"cell_type":"markdown","metadata":{"id":"VXKts8jJgUvs"},"source":["**Question 3:** Give an example of a situation where the LSTM language model's ability to propagate information across many steps (when trained for long enough, at least) would cause it to reach a better cost value than a model like a simple RNN without that ability. (Answer in one sentence or so.)"]},{"cell_type":"markdown","metadata":{"id":"75iq_DjIgUvv"},"source":["**Question 4:** Would the model be any worse if we were to just delete unknown words instead of using an `<UNK>` token? (Answer in one sentence or so.)"]},{"cell_type":"markdown","metadata":{"id":"gb-PCtUagUvy"},"source":["# Atribution:\n","Adapted by Oier Lopez de Lacalle, Olatz Perez de Viñaspre and Ander Barrena, based on a notebook by Sam Bowman at NYU"]}]}