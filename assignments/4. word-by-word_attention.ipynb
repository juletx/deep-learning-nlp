{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4. word-by-word_attention.ipynb","provenance":[{"file_id":"11Kdnj0MMqylj-zwJHcKdYy4Hb3KVNMDN","timestamp":1548166173153}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1NLDPKU36L5x"},"source":["# Assignment 4: A GRU-pair model for SNLI with attention"]},{"cell_type":"markdown","metadata":{"id":"pRtm3ddE6L5z"},"source":["In this assignment we'll build and train an attention-based model for SNLI based on the word-by-word attention with matching layer designed from [Wang and Jiang](https://arxiv.org/pdf/1512.08849v2.pdf). \n","\n","\n","As in Lab5  we'll focus on  __Natural Language Inference__ (NLI) task. Recall that NLI consists in determining whether a natural language _hypothesis_ can justifiably be inferred from a natural language _premise_. Thus given a pair of  premise  and  hypothesis texts, the task is to classify them into three categories: _entailment_, _contradiction_, and _neutral_. The table below shows a few examples of premise/hypothesis along with the annotated labels (examples taken from [here](https://nlp.stanford.edu/pubs/snli_paper.pdf)).\n","\n","The dataset we will use in this lab can be downloaded from [here](https://nlp.stanford.edu/projects/snli/) and it is a corpus comprised by The Stanford Natural Language Processing Group. Actually we are using a subset of the whole dataset. This is the same subset used in Lab5 so you can compare the results. \n","\n","\n","| Premise                                                               | Category      | Hypothesis                                                         |\n","|:--------------------------------------------------------------------|:---------------|:--------------------------------------------------------------------|\n","| A man inspects the uniform of a figure in some East Asian country. | contradiction | The man is sleeping                                                |\n","| An older and younger man smiling.                                  | neutral       | Two men are smiling and laughing at the cats playing on the floor. |\n","| A black race car starts up in front of a crowd of people.          | contradiction | A man is driving down a lonely road.                               |\n","| A soccer game with multiple males playing.                         | entailment    | Some men are playing a sport.                                     |\n","| A smiling costumed woman is holding an umbrella.                   | neutral       | A happy woman in a fairy costume holds an umbrella.                |\n","\n","----\n","\n","## Attention Model\n","As stated above, the model will incorporate the basic __word-by-word attention__ + __matching layer__ design from [Wang and Jiang](https://arxiv.org/pdf/1512.08849v2.pdf), but it will use __GRUs__ instead of LSTMs, and it will use [Luong et al.](http://www.aclweb.org/anthology/D15-1166)'s 'general' bilinear attention scoring function. See below for the implementation details.\n","\n",">>> ![](https://drive.google.com/uc?id=11Y9qwCUaTKV-CRxtbXindwxKNnVnn15a)\n","\n","\n","The paper proposes to use three LSTMs (GRU in our case) to 1) encode the steps in the premise ($H^{s}$),  2) encode the steps in hypothesis (e.g $h_{k}^{t}$) and 3) encode the matching steps ($h_{k}^{m}$). Please go to  [Wang and Jiang](https://arxiv.org/pdf/1512.08849v2.pdf) to understand the details of the architecture and the attention model. \n","\n","\n","\n","__Note__: Do not start this assignment at the last minute, since the model you build may take several hours to train."]},{"cell_type":"markdown","metadata":{"id":"XvJlxeYI6L51"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"OwZy8uz6Cgsc"},"source":["You will need first to mount your Google Drive  folder and give access to the Notebook. Please run the following cell and follow the instructions."]},{"cell_type":"code","metadata":{"id":"lj5TC8wxC9Fa"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WM6cl5KX6L51"},"source":["In a similar way as in previous labs, we'll first load the data and inspect it. You should have you data in your drive under ```dl4nlp_labs/data/snli``` (the data is the same the one that you can downloaded from [here](https://nlp.stanford.edu/projects/snli/) , but we get rid off stuff we do not need this lab).\n","\n","You we'll need to mount your Drive folder first to access the data. This will require one-step authentication. Please when you run the cell below follow the instructions. Once you mount everything, make sure ```snli_home = 'drive/My Drive/Colab Notebooks/dl4nlp_labs/data/snli/``` is correct path for your data.\n","\n","The cells below upload the training, development and test files:\n","\n","-  ```snli_1.0_train.jsonl.bz2```\n","- ```snli_1.0_dev.jsonl.bz2```\n","-  ```snli_1.0_test.jsonl.bz2```\n"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"mOLov_oh6L53"},"source":["snli_home = 'drive/My Drive/Colab Notebooks/dl4nlp_labs/data/snli'\n","\n","import bz2\n","import re\n","import random\n","import json\n","\n","LABEL_MAP = {\n","    \"entailment\": 0,\n","    \"neutral\": 1,\n","    \"contradiction\": 2\n","}\n","\n","def load_snli_data(path):\n","    data = []\n","    with bz2.open(path) as f:\n","        for line in f:\n","            line = line.decode('utf-8')\n","            loaded_example = json.loads(line)\n","            if loaded_example[\"gold_label\"] not in LABEL_MAP:\n","                continue\n","            loaded_example[\"label\"] = LABEL_MAP[loaded_example[\"gold_label\"]]\n","            data.append(loaded_example)\n","        random.seed(1)\n","        random.shuffle(data)\n","    return data\n","     \n","training_set = load_snli_data(snli_home + '/snli_1.0_train.jsonl.bz2')\n","dev_set = load_snli_data(snli_home + '/snli_1.0_dev.jsonl.bz2')\n","test_set = load_snli_data(snli_home + '/snli_1.0_test.jsonl.bz2')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RGl7_6TY6L57"},"source":["Next, we'll convert the data to index vectors in the same way that we've done for in-class exercises with RNN-based sentiment models. A few notes:\n","\n","- We use a sequence length of only 10, which is short enough that we're truncating a large fraction of sentences.\n","- Tokenization is easy here because we're relying on the output of a parser (which does tokenization as part of parsing), just as with the SST corpus that we've been using until now. Note that we use the 'sentence1_binary_parse' field of each example rather than the human-readable 'sentence1'.\n","- We're using a moderately large vocabulary of about 36k words."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"OY8SV3iD6L59"},"source":["SEQ_LEN = 10\n","\n","import collections\n","import numpy as np\n","\n","def sentences_to_padded_index_sequences(datasets):\n","    '''Annotates datasets with feature vectors.'''\n","    \n","    PADDING = \"<PAD>\"\n","    UNKNOWN = \"<UNK>\"\n","    \n","    # Extract vocabulary\n","    def tokenize(string):\n","        string = re.sub(r'\\(|\\)', '', string)\n","        return string.lower().split()\n","    \n","    word_counter = collections.Counter()\n","    for example in datasets[0]:\n","        word_counter.update(tokenize(example['sentence1_binary_parse']))\n","        word_counter.update(tokenize(example['sentence2_binary_parse']))\n","        \n","    vocabulary = set([word for word in word_counter])\n","    vocabulary = list(vocabulary)\n","    vocabulary = [PADDING, UNKNOWN] + vocabulary\n","        \n","    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n","    indices_to_words = {v: k for k, v in word_indices.items()}\n","        \n","    for i, dataset in enumerate(datasets):\n","        for example in dataset:\n","            for sentence in ['sentence1_binary_parse', 'sentence2_binary_parse']:\n","                example[sentence + '_index_sequence'] = np.zeros((SEQ_LEN), dtype=np.int32)\n","\n","                token_sequence = tokenize(example[sentence])\n","                padding = SEQ_LEN - len(token_sequence)\n","\n","                for i in range(SEQ_LEN):\n","                    if i >= padding:\n","                        if token_sequence[i - padding] in word_indices:\n","                            index = word_indices[token_sequence[i - padding]]\n","                        else:\n","                            index = word_indices[UNKNOWN]\n","                    else:\n","                        index = word_indices[PADDING]\n","                    example[sentence + '_index_sequence'][i] = index\n","    return indices_to_words, word_indices\n","    \n","indices_to_words, word_indices = sentences_to_padded_index_sequences([training_set, dev_set, test_set])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"JOKpxHer6L6B"},"source":["print (training_set[6])\n","print (len(word_indices))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9kjYsALi6L6F"},"source":["Now we load GloVe. You'll need the same file that you used for the in-class exercise on word embeddings. Make sure ```glove_home = 'drive/My Drive/Colab Notebooks/dl4nlp_labs/data/embeddings/``` is correct path for your word embeddings file."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"qlfar7Ao6L6F"},"source":["import bz2\n","\n","glove_home = 'drive/My Drive/Colab Notebooks/dl4nlp_labs/data/embeddings'\n","words_to_load = 50000\n","\n","with bz2.open(glove_home + '/glove.6B.50d.txt.bz2') as f:\n","    loaded_embeddings = np.zeros((len(word_indices), 50), dtype='float32')\n","    for i, line in enumerate(f):\n","        if i >= words_to_load: \n","            break\n","        line = line.decode('utf-8')\n","        s = line.split()\n","        if s[0] in word_indices:\n","            loaded_embeddings[word_indices[s[0]], :] = np.asarray(s[1:])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9fFumQCg6L6J"},"source":["Now we set up an evaluation function as before."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"7i9-T6hS6L6K"},"source":["def evaluate_classifier(classifier, eval_set):\n","    correct = 0\n","    hypotheses = classifier(eval_set)\n","    for i, example in enumerate(eval_set):\n","        hypothesis = hypotheses[i]\n","        if hypothesis == example['label']:\n","            correct += 1        \n","    return correct / float(len(eval_set))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"37xu6TEE6L6O"},"source":["## Assignment Part"]},{"cell_type":"markdown","metadata":{"id":"qPaTx40R6L6Q"},"source":["Expand the below starter code to build a working attention-based NLI model. The model should feature the following:\n","\n","- 50D word embeddings initialized with GloVe and trained. (Using self.E should provide this.)\n","- Three GRUs (with two different sets of parameters): a premise encoder, a hypothesis encoder (sharing the same parameters), and matching GRU, taking the place of [Wang and Jiang](https://arxiv.org/pdf/1512.08849v2.pdf)'s mLSTM, with its own parameters. There shouldn't be a connection between the premise and hypothesis GRUs: the starting hidden state for each of the three GRUs should be zeros.\n","- Word-by-word attention using [Luong et al.](http://www.aclweb.org/anthology/D15-1166)'s 'general' (bilinear) scoring function.\n","- A three-way softmax classifier whose input is the final hidden state of the matching RNN.\n","\n","You are welcome to use code from solutions to past exercises. As ever, do not use any other __pre-written code__ or __TF functions__ that are specific to RNNs or attention. You can reuse the code of previous labs (specially, take advange of the code provided in the solution of Lab5).\n"]},{"cell_type":"markdown","metadata":{"id":"XPem9tI56L6Q"},"source":["As in class, the below line may give you a warning. It should be safe to ignore."]},{"cell_type":"markdown","metadata":{"id":"KeRuh14dFXLt"},"source":["## Guide for Completing the Assigment\n","\n","As the assignement is quite demanding and  due to stundets do no have too m , We provide some instructions to complete correctly the assignment that can be useful. This set of hints/explanaitions is a guide for one type of solution, but it is not obligatory to follow these instruction. So feel free to come up with another way of implementing the model. \n","\n","\n","1.   **Read the papers**. Thoroughly read the paper that describes the model with word-by-word+matching layer: [Wang and Jiang](https://arxiv.org/pdf/1512.08849v2.pdf). In addition, it should be very helpful to understand [Rocktäschel's](https://arxiv.org/pdf/1509.06664v4.pdf) model, as the model to be implemented realies on this one. Optionally,  you read [Luong et al.](http://www.aclweb.org/anthology/D15-1166) to know more about the general 'bilinear'  attention score function.\n","\n","2.   **Attention scoring function**. In this implementation, we are going to change the attention scoring function shown in equation 6 ([Wang and Jiang](https://arxiv.org/pdf/1512.08849v2.pdf)). Instead we'll use  [Luong et al.](http://www.aclweb.org/anthology/D15-1166) general 'bilinear' attention scoring.\n","\n",">Following same notation of [Wang and Jiang](https://arxiv.org/pdf/1512.08849v2.pdf) equation 6 is defined as follow:\n","\n",">> $ e_{kj} = \\mathbf{w^{e}} \\cdot tanh (\\mathbf{W^{s}h^{s}_{j}} +  \\mathbf{W^{t}h^{t}_{k}} +  \\mathbf{W^{m}h^{m}_{k-1}} )$\n","\n","> You should substitute by this:\n","\n",">> $e_{kj} = [ \\mathbf{h^{t}_{k}}; \\mathbf{h^{m}_{k-1}} ] \\ \\mathbf{W} \\ \\mathbf{h^{s}_{j}}$\n","\n","> Note that in Lab5 we were learning attention parameter using wider set of matrices (e.g. $\\mathbf{W^{y}}, \\mathbf{W^{h}}, \\mathbf{w}, \\ldots$). In this case we considerably reduce the complexity of the attention scoring function as we are using only one parameter matrix (note that in this case $\\mathbf{W}$ is not symmetric).\n","\n","3. **Premise encoder** (```premise_step```). Take a look at the solution of Lab5, it should be very simillar, if not the same. \n","\n","4. **Hypothesis encoder** (```hypothesis_step```). This is the most complicated part, as you need to implement the attention mechanism as well. Apart fron the use of the two GRUs (the encoder for the premise and hypothesis, and the matching GRU), you need to implement the equations 2, 3, and the new scoring function defined above.\n","\n","> Think of what you need to compute the hypothesis step and what should return the functions. For this you will need to revise the equations that defined the attention model. \n","\n","5. **Unroll the premise**. As you are reading the word sequence of the premise and storing the state of each time step (```premise_step```), it is good idea to calculate a part of the function of the bilinear attention scoring of each time step: $\\mathbf{W \\ h_{j}^{s}}$ (```projected_premise_step```).\n","\n","6. **Unroll the hypothesis**. As similarly done in Lab5, you need to run over the hypothesis sequence (do not condition with last output of the premise!) updating the state of the GRU and keep tha last one. \n","You should get attention weights of each time step to plot them.\n","\n","\n"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"ORfIbXAE6L6S"},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"7jOD906B6L6V"},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","tf.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"9ggJEL7K6L6Z"},"source":["class RNNEntailmentClassifier:\n","    def __init__(self, vocab_size, sequence_length):\n","        # Define the hyperparameters\n","        self.training_epochs = 30  # How long to train for - chosen to fit within class time\n","        self.display_epoch_freq = 1  # How often to print cost (in epochs)\n","        self.display_step_freq = 250  # How often to test (in steps)\n","        self.dim = 24  # The dimension of the hidden state of each RNN\n","        self.embedding_dim = 50  # The dimension of the learned word embeddings\n","        self.batch_size = 64  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n","        self.vocab_size = vocab_size  # Defined by the file reader above\n","        self.sequence_length = sequence_length  # Defined by the file reader above\n","\n","        self.step = 1\n","        self.epoch = 0\n","        \n","        # Define the parameters\n","        self.trainable_variables = []\n","\n","        self.E = tf.Variable(loaded_embeddings, trainable=False)\n","        self.trainable_variables.append(self.E)\n","        \n","        # TODO: Define the parameters of the GRUs\n","        #  - Note that we need to learn two GRUs: \n","        #       1) for encoding premise and hypothesis\n","        #       2) for maching GRU. Hint: What is the dimensionality of input?\n","        #\n","        #  - You can re-use part of you code in lab5, but you need to set different\n","        #    dimensionality to each GRU.\n","        #  - Use Figure above to understand how you can organize you GRU models.\n","        \n","        # TODO: Define the attention parameters.\n","        #  - Attention parameters: You need to define just one variable that learns matching \n","        #    premise and hypothesis sequences.\n","        #  - Attention score is defined in eq. 8 in Luong et al. (general score)\n","        #  - This simplify eq.6 in Wang and Jiang for the attention score.\n","        #  - Hint: What are the dimensionality of vectors of both side in the equation? \n","        \n","        # TODO: Define the paremeters for the classification layer (as in Lab5)\n","          \n","    # Define the model: Complete the functions \n","    def model(self,premise_x,hypothesis_x):\n","        # TODO: define the GRU function (Hint: check lab 5)\n","        def gru(emb, h_prev, name):\n","          pass\n","\n","        # TODO: Define one step of the premise encoder GRU.\n","        def premise_step(x, h_prev):\n","          pass\n","        \n","        # TODO: Define one step of the hypothesis encoder GRU.\n","        def hypothesis_step(x, h_prev, h_prev_attn, premise_steps, projected_premise_steps):\n","          # - Note that attention mechanism is inside hypothesis step.\n","          pass         \n","          \n","        # Split up the inputs and into individual tensors, unroll sentences with attention.\n","       \n","        # TODO: Split up the inputs into individual tensors\n","       \n","        \n","        # TODO: Unroll the first RNN (Premise). (Hint: check lab 5)\n","        # 1.calculate the state of every premise step.\n","        # 2.calculate a part of the function of the bilinear attention scoring of each time step: projected_premise_step\n","        # keep all the premise steps and projected premise steps\n","        \n","                \n","        # TODO: Unroll the second RNN (Hypothesis). (Hint: check lab 5)\n","        # 1. encode the hypothesis steps with the attention.\n","        # 2. keep the attention weights of each step.\n","                \n","        # TODO: Compute the logits (Hint: check lab 5)\n","        \n","        \n","    def train(self, training_data, dev_data):\n","        def get_minibatch(dataset, start_index, end_index):\n","            indices = range(start_index, end_index)\n","            premise_vectors = np.vstack([dataset[i]['sentence1_binary_parse_index_sequence'] for i in indices])\n","            hypothesis_vectors = np.vstack([dataset[i]['sentence2_binary_parse_index_sequence'] for i in indices])\n","            labels = [dataset[i]['label'] for i in indices]\n","            return premise_vectors, hypothesis_vectors, labels\n","\n","        print('Training.')\n","\n","        # Training cycle\n","        for _ in range(self.training_epochs):\n","            random.shuffle(training_data)\n","            avg_cost = 0.\n","            total_batch = int(len(training_data) / self.batch_size)\n","            \n","            # Loop over all batches in epoch\n","            for i in range(total_batch):\n","                # Assemble a minibatch of the next B examples\n","                minibatch_premise_vectors, minibatch_hypothesis_vectors, minibatch_labels = get_minibatch(\n","                    training_data, self.batch_size * i, self.batch_size * (i + 1))\n","\n","                # Run the optimizer to take a gradient step, and also fetch the value of the \n","                # cost function for logging\n","                with tf.GradientTape() as tape:\n","                  logits,_ = self.model(minibatch_premise_vectors,minibatch_hypothesis_vectors)\n","                \n","                  # Define the cost function (here, the softmax exp and sum are built in)\n","                  total_cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=minibatch_labels))\n","\n","                # This  performs the main SGD update equation with gradient clipping\n","                optimizer_obj = tf.optimizers.Adam()\n","                gradients = tape.gradient(total_cost, self.trainable_variables)\n","                gvs = zip(gradients, self.trainable_variables)\n","                capped_gvs = [(tf.clip_by_norm(grad, 5.0), var) for grad, var in gvs if grad is not None]\n","                optimizer = optimizer_obj.apply_gradients(capped_gvs)\n","\n","                if self.step % self.display_step_freq == 0:\n","                    print(\"Step:\", self.step, \"Dev acc:\", evaluate_classifier(self.classify, dev_data[0:1000]), \\\n","                        \"Train acc:\", evaluate_classifier(self.classify, training_data[0:1000])) \n","                                  \n","                self.step += 1\n","                avg_cost += total_cost / (total_batch * self.batch_size)\n","                                \n","            # Display some statistics about the step\n","            # Evaluating only one batch worth of data -- simplifies implementation slightly\n","            if self.epoch % self.display_epoch_freq == 0:\n","                tf.print(\"Epoch:\", (self.epoch + 1), \"Cost:\", avg_cost) \n","            self.epoch += 1\n","    \n","    def classify(self, examples):\n","        # This classifies a list of examples\n","        premise_vectors = np.vstack([example['sentence1_binary_parse_index_sequence'] for example in examples])\n","        hypothesis_vectors = np.vstack([example['sentence2_binary_parse_index_sequence'] for example in examples])\n","        logits,_ = self.model(premise_vectors,hypothesis_vectors)\n","\n","        return np.argmax(logits, axis=1)\n","\n","    def get_attn(self, examples):\n","        premise_vectors = np.vstack([example['sentence1_binary_parse_index_sequence'] for example in examples])\n","        hypothesis_vectors = np.vstack([example['sentence2_binary_parse_index_sequence'] for example in examples])\n","        _,attn_weights = self.model(premise_vectors,hypothesis_vectors)\n","        return np.reshape(attn_weights, [len(examples), 10, 10])\n","        \n","    def plot_attn(self, examples):\n","        attn_weights = self.get_attn(examples)\n","\n","        for i in range(len(examples)):    \n","            fig = plt.figure()\n","            ax = fig.add_subplot(111)\n","            ax.matshow(attn_weights[i,:,:], vmin=0., vmax=1., cmap=plt.cm.inferno)\n","\n","            premise_tokens = [indices_to_words[index] for index in examples[i]['sentence1_binary_parse_index_sequence']]\n","            hypothesis_tokens = [indices_to_words[index] for index in examples[i]['sentence2_binary_parse_index_sequence']]\n","\n","            ax.set_yticklabels(premise_tokens)\n","            ax.set_xticklabels(hypothesis_tokens, rotation=45)\n","\n","            plt.xticks(np.arange(0, 10, 1.0))\n","            plt.yticks(np.arange(0, 10, 1.0))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0SHAj-je6L6c"},"source":["This creates a classifier and initializes it. Note that starting and stopping training will not re-initialize the model."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"vcGN9CQY6L6c"},"source":["classifier = RNNEntailmentClassifier(len(word_indices), SEQ_LEN)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0NqYXpgs6L6f"},"source":["Running this line tests that you've successfully avoided a common error in implementing attention: mixing up dimensions in a way that allows information about one example in a batch to impact the results for other examples. It should do nothing if the model behaves correctly, and will throw an error otherwise."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"mRPvIDZb6L6h"},"source":["assert (classifier.get_attn(training_set[0:2])[0, :, :] == \\\n","        classifier.get_attn(training_set[0:3])[0, :, :]).all(), \\\n","       'Warning: There is cross-example information flow.'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"UUv5GuyX6L6j"},"source":["Run the model below. Your goal is dev set performance above 72-74% at at least a few steps within the first 30 epochs. This may require running your model overnight, though it won't necessarily require running for all 30 epochs. Since a single epoch can take a long time with this model, you'll see accuracy results every 250 steps rather than every epoch."]},{"cell_type":"code","metadata":{"scrolled":false,"id":"29hQNjEP6L6k"},"source":["classifier.train(training_set, dev_set)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p_zJG3jQ6L6n"},"source":["This will print some visualizations for the first ten training examples. It should be useful in the questions below."]},{"cell_type":"code","metadata":{"scrolled":false,"id":"0GORtIXF6L6p"},"source":["classifier.plot_attn(training_set[0:10])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pN1q0UoH6L6t"},"source":["# Atribution:\n","Adapted by Oier Lopez de Lacalle, Olatz Perez de Viñaspre and Ander Barrena, based on a notebook by Sam Bowman at NYU"]}]}