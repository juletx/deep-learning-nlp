{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5. Attention Model.ipynb","provenance":[{"file_id":"1ZoBileMEj1oEUrpMUCJLpk4wAz71jc5-","timestamp":1546445147768},{"file_id":"18g7qxTfvWUjDcu1OgRFuUReaqO53kKL5","timestamp":1546445076779}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"UC9CMGxITYgZ"},"source":["# Lab 5: NLI with Attention"]},{"cell_type":"markdown","metadata":{"id":"bsh61LXxTYgg"},"source":["In this assignment we train an attention model for NLI based on [Rocktäschel's](https://arxiv.org/pdf/1509.06664v4.pdf) initial (non word-by-word) model.  Although there are more complicated attention models (see for example the word-by-word attention model), it is interesting enough to understand how attentions work and how it is possible to implement with TensorFlow.\n","\n","For this we'll focus on a different NLP task: __Natural Language Inference__ (NLI). NLI consists in determining whether a natural language _hypothesis_ can justifiably be inferred from a natural language _premise_. Thus given a pair of  premise  and  hypothesis texts, the task is to classify them into three categories: _entailment_, _contradiction_, and _neutral_. The table below shows a few examples of premise/hypothesis along with the annotated labels (examples taken from [here](https://nlp.stanford.edu/pubs/snli_paper.pdf)).\n","\n","The dataset we will use in this lab can be downloaded from [here](https://nlp.stanford.edu/projects/snli/) and it is a corpus comprised by The Stanford Natural Language Processing Group. Actually we are using a subset of the whole dataset. We'll more details about the dataset in the next section. \n","\n","\n","| Premise                                                               | Category      | Hypothesis                                                         | \n","|:--------------------------------------------------------------------|:---------------|:--------------------------------------------------------------------|\n","| A man inspects the uniform of a figure in some East Asian country. | contradiction | The man is sleeping                                                |\n","| An older and younger man smiling.                                  | neutral       | Two men are smiling and laughing at the cats playing on the floor. |\n","| A black race car starts up in front of a crowd of people.          | contradiction | A man is driving down a lonely road.                               |\n","| A soccer game with multiple males playing.                         | entailment    | Some men are playing a sport.                                     |\n","| A smiling costumed woman is holding an umbrella.                   | neutral       | A happy woman in a fairy costume holds an umbrella.                |\n","\n","\n","\n","\n","## Attention Model\n","We will make some simplification to the model presented in [Rocktäschel's](https://arxiv.org/pdf/1509.06664v4.pdf) model. In our case, the conditional encoding of the sentences is obtained usin two different ```GRU``` layers. The premise is read by one ```GRU``` and the hypothesis sentence is read by the other GRU that learns different parameters. Two ```GRU``` layers are connected in a way that the second ```GRU``` is initialized with the last cell state of the premise GRU like shown in the example (the figure is taken from [Rocktäschel's](https://arxiv.org/pdf/1509.06664v4.pdf) paper).  \n","\n","![image](http://ixa2.si.ehu.es/~jibloleo/uc3m_dl4nlp/img/main_arc.jpeg)\n","\n","The paper proposes two attention models, we'll focus on the non word-by-word attention (Figure B), which is described in Section 2.3 of the paper. The model learns to capture important pieces of information of the premise based on the last output of the hypotesis (which contains the semantics of the hypothesis sentences). Please go through the paper to know about the mathematical details."]},{"cell_type":"markdown","metadata":{"id":"RYrGvuhR_ke6"},"source":["## Load data\n","In a similar way as in previous labs, we'll first load the data and inspect it. You should have you data in your drive under ```drive/My Drive/Colab Notebooks/dl4nlp_labs/data/snli/``` (the data is the same the one that you can downloaded from [here](https://nlp.stanford.edu/projects/snli/) , but we get rid off stuff we do not need this lab).\n","\n","You we'll need to mount your Drive folder first to access the data. This will require one-step authentication. Please when you run the cell below follow the instructions. Once you mount everything, make sure ```snli_home = 'drive/My Drive/Colab Notebooks/dl4nlp_labs/data/snli/``` is correct path for your data.\n","\n","The cells below upload the training, development and test files:\n","\n","-  ```snli_1.0_train.jsonl.bz2```\n","- ```snli_1.0_dev.jsonl.bz2```\n","-  ```snli_1.0_test.jsonl.bz2```\n","\n","\n","-----"]},{"cell_type":"markdown","metadata":{"id":"b1Xen3k1BN2A"},"source":["It is good idea before we upload the data to make sure that GPUs are visible for the notebook. "]},{"cell_type":"code","metadata":{"id":"An7FRUQ4BRAp"},"source":["## check that your notebook sees a GPU, otherwise it would take for ever.\n","%tensorflow_version 2.x\n","import tensorflow as tf\n","tf.__version__\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oS1uoRMxBVOH"},"source":["-----\n","Now we are ready to mount our Drive files:"]},{"cell_type":"code","metadata":{"id":"pHmXBsWZBdol"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mvkKgFPQTYgp"},"source":["This will take a couple of minutes to run if you load whole data, so you may want to start editing the code below at the same time. As it is now, the code is ready for debugging, it only loads 1000 examples.\n","\n","You'll use Glove embeddings as well. If you want to run local, follow this instructions: If you dowloaded for the assignment 3 then edit `glove_home` to point the correct embedding file. Otherwise, if need to download them, You can do it at the following [url](http://nlp.stanford.edu/data/glove.6B.zip) (1GB). The zip file includes embeddings of different dimensionality (50d, 100d, 200d, 300d) for a vocabulary of 400000 words. Decompress them and place somewhere, for example in `./embeddings/` folder."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Rkv7UWB6TYgr"},"source":["import re\n","import bz2\n","import random\n","import json\n","\n","LABEL_MAP = {\n","    \"entailment\": 0,\n","    \"neutral\": 1,\n","    \"contradiction\": 2\n","}\n","\n","def load_snli_data(path):\n","    data = []\n","    with bz2.open(path) as f:\n","        for i, line in enumerate(f):\n","            line = line.decode('utf-8')\n","            if i >= 1000000:  # Edit to use less data for debugging. set to 1000000 for testing.\n","                break\n","            loaded_example = json.loads(line)\n","            if loaded_example[\"gold_label\"] not in LABEL_MAP:\n","                continue\n","            loaded_example[\"label\"] = LABEL_MAP[loaded_example[\"gold_label\"]]\n","            data.append(loaded_example)\n","        random.seed(1)\n","        random.shuffle(data)\n","    return data\n","\n","snli_home = 'drive/My Drive/Colab Notebooks/dl4nlp_labs/data/snli/'  \n","training_set = load_snli_data(snli_home + '/snli_1.0_train.jsonl.bz2')\n","dev_set = load_snli_data(snli_home + '/snli_1.0_dev.jsonl.bz2')\n","test_set = load_snli_data(snli_home + '/snli_1.0_test.jsonl.bz2')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EFqLbrCzTYgz"},"source":["Next, we'll convert the data to index vectors in the same way that we've done for in-class exercises with RNN-based sentiment models. A few notes:\n","\n","- We use a sequence length of only 10, which is short enough that we're truncating a large fraction of sentences.\n","- Tokenization is easy here because we're relying on the output of a parser (which does tokenization as part of parsing), just as with the SST corpus that we've been using until now. Note that we use the 'sentence1_binary_parse' field of each example rather than the human-readable 'sentence1'.\n","- We're using a moderately large vocabulary (for a class exercise) of about 36k words."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"QrMdWe3nTYg3"},"source":["SEQ_LEN = 10\n","\n","import collections\n","import numpy as np\n","\n","def sentences_to_padded_index_sequences(datasets):\n","    '''Annotates datasets with feature vectors.'''\n","    \n","    PADDING = \"<PAD>\"\n","    UNKNOWN = \"<UNK>\"\n","    \n","    # Extract vocabulary\n","    def tokenize(string):\n","        string = re.sub(r'\\(|\\)', '', string)\n","        return string.lower().split()\n","    \n","    word_counter = collections.Counter()\n","    for example in datasets[0]:\n","        word_counter.update(tokenize(example['sentence1_binary_parse']))\n","        word_counter.update(tokenize(example['sentence2_binary_parse']))\n","        \n","    vocabulary = set([word for word in word_counter])\n","    vocabulary = list(vocabulary)\n","    vocabulary = [PADDING, UNKNOWN] + vocabulary\n","        \n","    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n","    indices_to_words = {v: k for k, v in word_indices.items()}\n","        \n","    for i, dataset in enumerate(datasets):\n","        for example in dataset:\n","            for sentence in ['sentence1_binary_parse', 'sentence2_binary_parse']:\n","                example[sentence + '_index_sequence'] = np.zeros((SEQ_LEN), dtype=np.int32)\n","\n","                token_sequence = tokenize(example[sentence])\n","                padding = SEQ_LEN - len(token_sequence)\n","\n","                for i in range(SEQ_LEN):\n","                    if i >= padding:\n","                        if token_sequence[i - padding] in word_indices:\n","                            index = word_indices[token_sequence[i - padding]]\n","                        else:\n","                            index = word_indices[UNKNOWN]\n","                    else:\n","                        index = word_indices[PADDING]\n","                    example[sentence + '_index_sequence'][i] = index\n","    return indices_to_words, word_indices\n","    \n","indices_to_words, word_indices = sentences_to_padded_index_sequences([training_set, dev_set, test_set])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"GAnBhPtqTYg_"},"source":["print(training_set[6])\n","print(len(word_indices))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eVr8PaEcTYhI"},"source":["Now we load GloVe. You'll need to decompress the file in 'data' folder.  You'll need the same file that you used for the in-class exercise on word embeddings."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"2lq3CyJmTYhL"},"source":["import bz2\n","\n","glove_home = 'drive/My Drive/Colab Notebooks/dl4nlp_labs/data/embeddings/'\n","words_to_load = 45000\n","\n","with bz2.open(glove_home + 'glove.6B.50d.txt.bz2') as f:\n","    loaded_embeddings = np.zeros((len(word_indices), 50), dtype='float32')\n","    for i, line in enumerate(f):\n","        if i >= words_to_load: \n","            break\n","        line = line.decode('utf-8')\n","        s = line.split()\n","        if s[0] in word_indices:\n","            loaded_embeddings[word_indices[s[0]], :] = np.asarray(s[1:])\n","\n","word = 'dog'\n","word_ind = word_indices[word]\n","print('Loaded embedding for {}:\\n{}'.format(word,loaded_embeddings[word_ind]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xxwkGqRITYhW"},"source":["Now we set up an evaluation function as before."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"W0ukvNA8TYha"},"source":["def evaluate_classifier(classifier, eval_set):\n","    correct = 0\n","    hypotheses = classifier(eval_set)\n","    for i, example in enumerate(eval_set):\n","        hypothesis = hypotheses[i]\n","        if hypothesis == example['label']:\n","            correct += 1        \n","    return correct / float(len(eval_set))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ip2ebGefTYhh"},"source":["## Assignments: Build GRU pair with attention "]},{"cell_type":"markdown","metadata":{"id":"kFStDPPmh6Ju"},"source":["The intuition behind the next attention mechanism is that once we read the whole hypothesis sentence and store its semantics in the last state ($\\mathbf{h}_{N}$), the model looks back in the premise text to find some useful piece of information that updates the current semantics in state $\\mathbf{h}_{N}$.\n","\n","![image](http://ixa2.si.ehu.es/~jibloleo/uc3m_dl4nlp/img/snli_with_attention.png)\n","\n","To learn attentions the paper proposes the following non-linear combinations:\n","\n"," __Intermediate attention representation__: \n","\n","> $\\mathbf{M} = \\tanh(\\mathbf{W}^{y}\\mathbf{Y} + \\mathbf{W}^{h}\\mathbf{h}_{N}\\otimes \\mathbf{e}_{L})$\n","\n","> in which $\\mathbf{W}^y, \\mathbf{W}^ĥ \\in \\mathbb{R}^{k\\times k}$ are trained projections matrices and $\\mathbf{M} \\in \\mathbb{R}^{k \\times L}$.   $\\mathbf{e}_L \\in \\mathbb{R^L}$ is a vector of 1s, which its functions is to repeat $L$ times the vector obtained from  $\\mathbf{W}^{h}\\mathbf{h}_{N}$. $L$ is the length of the sequence, and $k$ the number of hidden units of the GRU layers.\n","\n","\n"," __Attention weights__: \n","\n","> $\\alpha = \\text{softmax}(\\mathbf{w}^T\\mathbf{M}$)\n","\n","> where $\\mathbf{w} \\in \\mathbb{R}^k$ is a trained vector that parametrizes the attention given to each word in the premise, and produce a vector $\\alpha \\in \\mathbb{R}^L$ of attention weights.\n","\n"," __Attention results__: \n","\n","> $\\mathbf{r} = \\mathbf{Y}\\alpha^T$\n","\n","> where $\\mathbf{r} \\in \\mathbb{R}^k$ is the weighted representation of the premise text.\n","\n","\n","\n","The final __sentence-pair representation__ is obtained from a non-linear combination of the attention weighted\n","representation $\\mathbf{r}$ of the premise and the last output vector $\\mathbf{h}_N$ using\n","\n","> $\\mathbf{h}^{*} = \\tanh(\\mathbf{W}^{p}\\mathbf{r} + \\mathbf{W}^{x}\\mathbf{h}_N)$\n","\n","> where  $\\mathbf{h}* \\in \\mathbb{R}^k$ , and $\\mathbf{W}^p$, $\\mathbf{W}^x$ are trained projection matrices.\n","\n","-----\n"]},{"cell_type":"markdown","metadata":{"id":"98FyvXXgTYhm"},"source":["Run the first block of code right away to make sure you have the proper dependencies. If you are working on your machine, you may need to install [matplotlib](http://matplotlib.org/users/installing.html), which should be fairly straightforward.\n","\n","### TODO:\n","- Fill in the missing componant below to complete Rocktäschel-style attention. Details of the attention mechanism are described in Section 2.3 of the [paper](https://arxiv.org/pdf/1509.06664v4.pdf).\n","\n","\n","- More specifically you need to implement the following two equations:\n","\n","> $\\mathbf{M} = \\tanh(\\mathbf{W}^{y}\\mathbf{Y} + \\mathbf{W}^{h}\\mathbf{h}_{N}\\otimes \\mathbf{e}_{L})$ \n","\n","> $\\alpha = \\text{softmax}(\\mathbf{w}^T\\mathbf{M})$\n"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"_nDGdlwYTYhn"},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"lX51uxHRTYhz"},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","tf.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"buWTbwThTYh4"},"source":["class RNNEntailmentClassifier:\n","    def __init__(self, vocab_size, sequence_length):\n","        # Define the hyperparameters\n","        self.training_epochs = 5  # How long to train for - chosen to fit within class time\n","        self.display_epoch_freq = 1  # How often to print out cost statistics (in epochs)\n","        self.display_step_freq = 250  # How often to test (in steps)\n","        self.dim = 24  # The dimension of the hidden state of the RNN\n","        self.embedding_dim = 50  # The dimension of the learned word embeddings\n","        self.batch_size = 64  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n","        self.vocab_size = vocab_size  # Defined by the file reader above\n","        self.sequence_length = sequence_length  # Defined by the file reader above\n","        self.step = 1\n","        self.epoch = 1\n","\n","        # Define the parameters\n","        self.trainable_variables = []\n","        self.E = tf.Variable(loaded_embeddings, trainable=False)\n","        self.trainable_variables.append(self.E)\n","        \n","        self.W_rnn = {}\n","        self.W_r = {}\n","        self.W_z = {}\n","        self.b_rnn = {}\n","        self.b_r = {}\n","        self.b_z = {}\n","        \n","        for self.name in ['p', 'h']:\n","            in_dim = self.embedding_dim\n","            \n","            self.W_rnn[self.name] = tf.Variable(tf.random.normal([in_dim + self.dim, self.dim], stddev=0.1))\n","            self.b_rnn[self.name] = tf.Variable(tf.random.normal([self.dim], stddev=0.1))\n","            self.trainable_variables.append(self.W_rnn[self.name])\n","            self.trainable_variables.append(self.b_rnn[self.name])\n","\n","            self.W_r[self.name] = tf.Variable(tf.random.normal([in_dim + self.dim, self.dim], stddev=0.1))\n","            self.b_r[self.name] = tf.Variable(tf.random.normal([self.dim], stddev=0.1))\n","            self.trainable_variables.append(self.W_r[self.name])\n","            self.trainable_variables.append(self.b_r[self.name])\n","\n","            self.W_z[self.name] = tf.Variable(tf.random.normal([in_dim + self.dim, self.dim], stddev=0.1))\n","            self.b_z[self.name] = tf.Variable(tf.random.normal([self.dim], stddev=0.1))\n","            self.trainable_variables.append(self.W_z[self.name])\n","            self.trainable_variables.append(self.b_z[self.name])         \n","\n","        # TODO: You'll need to use these three parameters.\n","        self.W_h_attn = tf.Variable(tf.random.normal([self.dim, self.dim], stddev=0.1))\n","        self.W_y_attn = tf.Variable(tf.random.normal([self.dim, self.dim], stddev=0.1))\n","        self.w_attn = tf.Variable(tf.random.normal([self.dim, 1], stddev=0.1))\n","        self.trainable_variables.append(self.W_h_attn)\n","        self.trainable_variables.append(self.W_y_attn)\n","        self.trainable_variables.append(self.w_attn)\n","\n","        self.W_combination = tf.Variable(tf.random.normal([2 * self.dim, self.dim], stddev=0.1))\n","        self.b_combination = tf.Variable(tf.random.normal([self.dim], stddev=0.1))\n","        self.trainable_variables.append(self.W_combination)\n","        self.trainable_variables.append(self.b_combination)\n","        \n","        self.W_cl = tf.Variable(tf.random.normal([self.dim, 3], stddev=0.1))\n","        self.b_cl = tf.Variable(tf.random.normal([3], stddev=0.1))\n","        self.trainable_variables.append(self.W_cl)\n","        self.trainable_variables.append(self.b_cl)\n","\n","    def model(self,premise_x,hypothesis_x):\n","\n","        # Define the GRU function\n","        def gru(emb, h_prev, name):\n","            emb_h_prev = tf.concat([emb, h_prev], 1, name=name + '_emb_h_prev')\n","            z = tf.nn.sigmoid(tf.matmul(emb_h_prev, self.W_z[name])  + self.b_z[name], name=name + '_z')\n","            r = tf.nn.sigmoid(tf.matmul(emb_h_prev, self.W_r[name])  + self.b_r[name], name=name + '_r')\n","            emb_r_h_prev = tf.concat([emb, r * h_prev], 1, name=name + '_emb_r_h_prev')\n","            h_tilde = tf.nn.tanh(tf.matmul(emb_r_h_prev, self.W_rnn[name])  + self.b_rnn[name], name=name + '_h_tilde')\n","            h = (1. - z) * h_prev + z * h_tilde\n","            return h\n","        \n","        # Define one step of the premise encoder RNN\n","        def premise_step(x, h_prev):\n","            emb = tf.nn.embedding_lookup(self.E, x)\n","            return gru(emb, h_prev, 'p')\n","        \n","        # Define one step of the hypothesis encoder RNN\n","        def hypothesis_step(x, h_prev):\n","            emb = tf.nn.embedding_lookup(self.E, x)\n","            return gru(emb, h_prev, 'h')\n","\n","        # Split up the inputs into individual tensors\n","        self.x_premise_slices = tf.split(premise_x, self.sequence_length, 1)\n","        self.x_hypothesis_slices = tf.split(hypothesis_x, self.sequence_length, 1)\n","        \n","        self.h_zero = tf.zeros(tf.stack([tf.shape(premise_x)[0], self.dim]))\n","        \n","        # Unroll the first RNN\n","        premise_h_prev = self.h_zero\n","        premise_steps_list = []\n","\n","        for t in range(self.sequence_length):\n","            x_t = tf.reshape(self.x_premise_slices[t], [-1])\n","            premise_h_prev = premise_step(x_t, premise_h_prev)\n","            premise_steps_list.append(premise_h_prev)\n","            \n","        premise_steps = tf.stack(premise_steps_list, axis=1, name='premise_steps')\n"," \n","        # Unroll the second RNN\n","        h_prev_hypothesis = premise_h_prev  # Continue running the same RNN\n","        \n","        for t in range(self.sequence_length):\n","            x_t = tf.reshape(self.x_hypothesis_slices[t], [-1])\n","            h_prev_hypothesis = hypothesis_step(x_t, h_prev_hypothesis)\n","        \n","        # Do attention\n","        wm_list = []\n","        \n","        # TODO: Fill wm_list with one scalar (a.k.a., vector with one scalar for each batch entry)\n","        # for each word in the premise.\n","        # This'll likely be easiest if you use a loop to iterate over timesteps.\n","        # You should use the three `attn` parameters defined above, as well as `premise_steps_list`.\n","        \n","\n","        # End above TODO\n","        \n","        wm = tf.stack(wm_list, axis=1)        \n","        attn_weights = tf.nn.softmax(wm, axis=1)\n","        attn_result = tf.reduce_sum(tf.multiply(attn_weights, premise_steps, name='attn_result_unsummed'),\n","                                    1, name='attn_result')\n","        \n","        # Combine the results of attention with the final GRU state\n","        concat_features = tf.concat([attn_result, h_prev_hypothesis], 1, name=self.name + '_emb_h_prev')\n","        pair_features = tf.nn.tanh(tf.matmul(concat_features, self.W_combination) + self.b_combination)\n","        \n","        # Compute the logits\n","        logits = tf.matmul(pair_features, self.W_cl) + self.b_cl\n","        return logits,attn_weights   \n","        \n","        \n","        \n","    def train(self, training_data, dev_data):\n","        def get_minibatch(dataset, start_index, end_index):\n","            indices = range(start_index, end_index)\n","            premise_vectors = np.vstack([dataset[i]['sentence1_binary_parse_index_sequence'] for i in indices])\n","            hypothesis_vectors = np.vstack([dataset[i]['sentence2_binary_parse_index_sequence'] for i in indices])\n","            labels = [dataset[i]['label'] for i in indices]\n","            return premise_vectors, hypothesis_vectors, labels\n","\n","        print('Training.')\n","\n","        # Training cycle\n","        for _ in range(self.training_epochs):\n","            random.shuffle(training_data)\n","            avg_cost = 0.\n","            total_batch = int(len(training_data) / self.batch_size)\n","            \n","            # Loop over all batches in epoch\n","            for i in range(total_batch):\n","                # Assemble a minibatch of the next B examples\n","                minibatch_premise_vectors, minibatch_hypothesis_vectors, minibatch_labels = get_minibatch(\n","                    training_data, self.batch_size * i, self.batch_size * (i + 1))\n","\n","                # Run the optimizer to take a gradient step, and also fetch the value of the \n","                # cost function for logging\n","                with tf.GradientTape() as tape:\n","                  logits,_ = self.model(minibatch_premise_vectors,minibatch_hypothesis_vectors)\n","                  # Define the cost function (here, the softmax exp and sum are built in)\n","                  total_cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=minibatch_labels))\n","        \n","                # This  performs the main SGD update equation with gradient clipping\n","                optimizer = tf.optimizers.Adam()\n","                gradients = tape.gradient(total_cost, self.trainable_variables)\n","                gvs = zip(gradients, self.trainable_variables)\n","                capped_gvs = [(tf.clip_by_norm(grad, 5.0), var) for grad, var in gvs if grad is not None]\n","                optimizer.apply_gradients(capped_gvs)\n","        \n","\n","                if self.step % self.display_step_freq == 0:\n","                    tf.print(\"Step:\", self.step, \"Dev acc:\", evaluate_classifier(self.classify, dev_data[0:1000]), \\\n","                        \"Train acc:\", evaluate_classifier(self.classify, training_data[0:1000]))\n","                                  \n","                self.step += 1\n","                avg_cost += total_cost / (total_batch * self.batch_size)\n","                                \n","            # Display some statistics about the step\n","            # Evaluating only one batch worth of data -- simplifies implementation slightly\n","            if self.epoch % self.display_epoch_freq == 0:\n","                tf.print (\"Epoch:\", self.epoch, \"Cost:\", avg_cost) \n","            self.epoch += 1\n","    \n","    def classify(self, examples):\n","        # This classifies a list of examples\n","        premise_vectors = np.vstack([example['sentence1_binary_parse_index_sequence'] for example in examples])\n","        hypothesis_vectors = np.vstack([example['sentence2_binary_parse_index_sequence'] for example in examples])\n","        logits,_ = self.model(premise_vectors,hypothesis_vectors)\n","\n","        return np.argmax(logits, axis=1)\n","\n","    def get_attn(self, examples):\n","        premise_vectors = np.vstack([example['sentence1_binary_parse_index_sequence'] for example in examples])\n","        hypothesis_vectors = np.vstack([example['sentence2_binary_parse_index_sequence'] for example in examples])\n","        _,attn_weights = self.model(premise_vectors,hypothesis_vectors)\n","        return tf.reshape(attn_weights, [len(examples), 10, 1])\n","        \n","    def plot_attn(self, examples):\n","        attn_weights = self.get_attn(examples)\n","\n","        for i in range(len(examples)):    \n","            fig = plt.figure()\n","            ax = fig.add_subplot(111)\n","            ax.matshow(np.transpose(attn_weights[i,:,:]), vmin=0., vmax=1., cmap=plt.cm.inferno)\n","            premise_tokens = [indices_to_words[index] for index in examples[i]['sentence1_binary_parse_index_sequence']]\n","            hypothesis_tokens = [indices_to_words[index] for index in examples[i]['sentence2_binary_parse_index_sequence']]\n","            plt.text(0, 1, 'H: ' + ' '.join(hypothesis_tokens))\n","            ax.set_xticklabels(premise_tokens, rotation=45)\n","            plt.xticks(np.arange(0, 10, 1.0))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QBRKnEsITYiA"},"source":["Next, create an instance of the model. Unlike in previous exercises, initialization happens here, rather than at the start of training. You can now initialize a model once and start and stop training as needed."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"3hfFv6XGTYiB"},"source":["classifier = RNNEntailmentClassifier(len(word_indices), SEQ_LEN)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n8giLvXgTYiJ"},"source":["In implementing attention, it's easy to accidentally mix information between )different examples in a batch. This assertion will fail if you've done so. Run it whenever you edit core model code."]},{"cell_type":"code","metadata":{"scrolled":false,"id":"s6CLKGfdTYiN"},"source":["assert (classifier.get_attn(training_set[0:2])[0, :, :] == \\\n","        classifier.get_attn(training_set[0:3])[0, :, :]).numpy().all(), \\\n","       'Warning: There is cross-example information flow.'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"zwdn6ONvTYiU"},"source":["Running the training long enough __on the whole training set__, you may get around 62% dev accuracy. This should take around five or ten minutes (or maybe more). This isn't great performance, but it's good enough that we should start to see attention play a role. If you have extra time, run longer."]},{"cell_type":"code","metadata":{"scrolled":false,"id":"IvaM0NbCTYiW"},"source":["classifier.train(training_set, dev_set)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gwgbvwEzTYic"},"source":["## Visualization"]},{"cell_type":"markdown","metadata":{"id":"MpE8izzkTYid"},"source":["This will print some (NYU-colored) visualizations for the first ten dev examples. Explore these examples and more, and see if you can identify any patterns in what the model has learned."]},{"cell_type":"code","metadata":{"scrolled":false,"id":"XwUN1OJvTYij"},"source":["classifier.plot_attn(dev_set[0:50])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"m9v5raLQTYiq"},"source":["# Atribution:\n","Adapted by Oier Lopez de Lacalle, Olatz Perez de Viñaspre and Ander Barrena, based on a notebook by Sam Bowman at NYU"]}]}