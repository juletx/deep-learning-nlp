{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4. GRU based classifier.ipynb","provenance":[{"file_id":"1Vl3m9t--nz4Hc9qX_KU7RejgzghT-N7u","timestamp":1546426740075},{"file_id":"1a57gIHj4bJfMFtw9UTI3P4rVOXWoLX_Y","timestamp":1546426686601}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"mPFakP2qNnpx"},"source":["# Lab4: Sentiment with GRUs"]},{"cell_type":"markdown","metadata":{"id":"DnmzS4xcNnp2"},"source":["In this assignment, you'll convert the RNN sentiment classifier from last time into a **GRU** RNN sentiment classifier. While the small dataset and tiny vocabulary that we're using here (for speed) will limit the performance of the model, it should still do substantially better than the plain RNN.\n","\n","![](http://vignette1.wikia.nocookie.net/despicableme/images/b/ba/Gru.jpg/revision/latest/scale-to-width-down/250?cb=20130711023954)"]},{"cell_type":"markdown","metadata":{"id":"QIO-J9F4Nnp5"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"DfdccRPyNnp-"},"source":["First, let's load the data as before."]},{"cell_type":"code","metadata":{"id":"yKXUkp6lOTxp"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/LAP/Subjects/DL/labs"],"metadata":{"id":"aLI_DnZ8MwJP"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZFYDv62UNnqA"},"source":["import re\n","import random\n","\n","# Let's do 2-way positive/negative classification instead of 5-way\n","def load_sst_data(path,\n","                  easy_label_map={0:0, 1:0, 2:None, 3:1, 4:1}):\n","    data = []\n","    with open(path) as f:\n","        for i, line in enumerate(f): \n","            example = {}\n","            example['label'] = easy_label_map[int(line[1])]\n","            if example['label'] is None:\n","                continue\n","            \n","            # Strip out the parse information and the phrase labels--\n","            # ---we don't need those here\n","            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n","            example['text'] = text[1:]\n","            data.append(example)\n","    \n","    random.seed(1)\n","    random.shuffle(data)\n","    return data\n","   \n","sst_home = '../data/trees/'\n","training_set = load_sst_data(sst_home + '/train.txt')\n","dev_set = load_sst_data(sst_home + '/dev.txt')\n","test_set = load_sst_data(sst_home + '/test.txt')\n","\n","# Note: Unlike with k-nearest neighbors, evaluation here should be fast, and we don't need to\n","# trim down the dev and test sets. "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PeiVYJONNnqK"},"source":["Next, we'll convert the data to index vectors.\n","\n","To simplify your implementation, we'll use a fixed unrolling length of 20. In the conversion process, we'll cut off excess words (towards the left/start end of the sentence), pad short sentences (to the left) with a special word symbol `<PAD>`, and mark out-of-vocabulary words with `<UNK>`, for unknown. As in the previous assignment, we'll use a very small vocabulary for this assignment, so you'll see `<UNK>` often."]},{"cell_type":"code","metadata":{"id":"zHhrsUHGNnqM"},"source":["import collections\n","import numpy as np\n","\n","def sentence_to_padded_index_sequence(datasets):\n","    '''Annotates datasets with feature vectors.'''\n","    \n","\n","    PADDING = \"<PAD>\"\n","    UNKNOWN = \"<UNK>\"\n","    SEQ_LEN = 20\n","    \n","    # Extract vocabulary\n","    def tokenize(string):\n","        return string.lower().split()\n","    \n","    word_counter = collections.Counter()\n","    for example in datasets[0]:\n","        word_counter.update(tokenize(example['text']))\n","    \n","    vocabulary = set([word for word in word_counter if word_counter[word] > 10])\n","    vocabulary = list(vocabulary)\n","    vocabulary = [PADDING, UNKNOWN] + vocabulary\n","        \n","    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n","    indices_to_words = {v: k for k, v in word_indices.items()}\n","        \n","    for i, dataset in enumerate(datasets):\n","        for example in dataset:\n","            example['index_sequence'] = np.zeros((SEQ_LEN), dtype=np.int32)\n","            \n","            token_sequence = tokenize(example['text'])\n","            padding = SEQ_LEN - len(token_sequence)\n","            \n","            for i in range(SEQ_LEN):\n","                if i >= padding:\n","                    if token_sequence[i - padding] in word_indices:\n","                        index = word_indices[token_sequence[i - padding]]\n","                    else:\n","                        index = word_indices[UNKNOWN]\n","                else:\n","                    index = word_indices[PADDING]\n","                example['index_sequence'][i] = index\n","    return indices_to_words, word_indices\n","    \n","indices_to_words, word_indices = sentence_to_padded_index_sequence([training_set, dev_set, test_set])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3FrcDlK8NnqU"},"source":["print(training_set[18])\n","print(len(word_indices))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DEKDVRYwNnqd"},"source":["def evaluate_classifier(classifier, eval_set):\n","    correct = 0\n","    hypotheses = classifier(eval_set)\n","    for i, example in enumerate(eval_set):\n","        hypothesis = hypotheses[i]\n","        if hypothesis == example['label']:\n","            correct += 1        \n","    return correct / float(len(eval_set))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zBLwIVNhNnqj"},"source":["## Assignments: Building the RNN"]},{"cell_type":"markdown","metadata":{"id":"N_q4prT9Nnqn"},"source":["The class below is a solved version of last week's RNN exercise. The only change I've made is to increase the learning rate, since GRUs are less likely to do crazy things during optimization. Your job is to convert it into a GRU model. You should have to:\n","\n","- **TODO1**: Add additional trained parameters.\n","- **TODO2**: Modify the `step()` function.\n","- **TODO3**: Modify L2 regularization to incorporate the new parameters.\n","\n","You shouldn't have to edit anything outside of `__init__()`."]},{"cell_type":"code","metadata":{"id":"0GXbwUBINnqr"},"source":["%tensorflow_version 2.x\n","!pip install tensorflow==2.4.0\n","import tensorflow as tf\n","tf.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nCzr4whzNnqy"},"source":["class RNNSentimentClassifier:\n","    def __init__(self, vocab_size, sequence_length):\n","        # Define the hyperparameters\n","        self.learning_rate = 1.0  # Should be about right\n","        self.training_epochs = 500  # How long to train for - chosen to fit within class time\n","        self.display_epoch_freq = 5  # How often to test and print out statistics\n","        self.dim = 12  # The dimension of the hidden state of the RNN\n","        self.embedding_dim = 8  # The dimension of the learned word embeddings\n","        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n","        self.vocab_size = vocab_size  # Defined by the file reader above\n","        self.sequence_length = sequence_length  # Defined by the file reader above\n","        self.l2_lambda = 0.001\n","        \n","        self.trainable_variables = []\n","\n","        # Define the parameters\n","        self.E = tf.Variable(tf.random.normal([self.vocab_size, self.embedding_dim], stddev=0.1))\n","        self.trainable_variables.append(self.E)\n","\n","        self.W_rnn = tf.Variable(tf.random.normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n","        self.b_rnn = tf.Variable(tf.random.normal([self.dim], stddev=0.1))\n","        self.trainable_variables.append(self.W_rnn)\n","        self.trainable_variables.append(self.b_rnn)\n","        \n","        self.W_cl = tf.Variable(tf.random.normal([self.dim, 2], stddev=0.1))\n","        self.b_cl = tf.Variable(tf.random.normal([2], stddev=0.1))\n","        self.trainable_variables.append(self.W_cl)\n","        self.trainable_variables.append(self.b_cl)\n","        \n","        # TODO1: Add additional GRU parameters\n","                \n","    def model(self,x):\n","        # Split up the inputs into individual tensors\n","        self.x_slices = tf.split(x, self.sequence_length, 1)\n","        self.h_zero = tf.zeros([self.batch_size, self.dim])\n","\n","        # Define one step of the RNN\n","        # TODO2: Modify the step() function to compute GRU step\n","        def step(x, h_prev):\n","            emb = tf.nn.embedding_lookup(params=self.E,ids=x)\n","            emb_h_prev = tf.concat([emb, h_prev], 1)\n","            h = tf.nn.tanh(tf.matmul(emb_h_prev, self.W_rnn)  + self.b_rnn)\n","            return h\n","                \n","        h_prev = self.h_zero\n","        \n","        # Unroll the RNN\n","        for t in range(self.sequence_length):\n","            x_t = tf.reshape(self.x_slices[t], [-1])\n","            h_prev = step(x_t, h_prev)\n","        \n","        # Compute the logits using one last linear layer\n","        logits = tf.matmul(h_prev, self.W_cl) + self.b_cl\n","        return logits\n","        \n","    def train(self, training_data, dev_set):\n","        def get_minibatch(dataset, start_index, end_index):\n","            indices = range(start_index, end_index)\n","            vectors = np.vstack([dataset[i]['index_sequence'] for i in indices])\n","            labels = [dataset[i]['label'] for i in indices]\n","            return vectors, labels\n","    \n","        print ('Training.')\n","\n","        # Training cycle\n","        train_acc = []\n","        dev_acc = []\n","        epochs = []\n","        for epoch in range(self.training_epochs):\n","            random.shuffle(training_set)\n","            avg_cost = 0.\n","            total_batch = int(len(training_set) / self.batch_size)\n","            \n","            # Loop over all batches in epoch\n","            for i in range(total_batch):\n","                # Assemble a minibatch of the next B examples\n","                minibatch_vectors, minibatch_labels = get_minibatch(training_set, \n","                                                                    self.batch_size * i, \n","                                                                    self.batch_size * (i + 1))\n","\n","                # Run the optimizer to take a gradient step, and also fetch the value of the \n","                # cost function for logging\n","                with tf.GradientTape() as tape:\n","                  logits = self.model(minibatch_vectors)\n","                  # Define the L2 cost\n","                  # TODO3: Modify L2 regularization to incorporate the new parameters.\n","                  l2_cost = self.l2_lambda * (tf.reduce_sum(tf.square(self.W_rnn)) +\n","                                                   tf.reduce_sum(tf.square(self.W_cl)))\n","        \n","                  # Define the cost function (here, the softmax exp and sum are built in)\n","                  total_cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=minibatch_labels, logits=logits) + l2_cost)\n","        \n","                # This  performs the main SGD update equation with gradient clipping\n","                optimizer = tf.optimizers.SGD(self.learning_rate)\n","                gradients = tape.gradient(total_cost, self.trainable_variables)\n","                gvs = zip(gradients, self.trainable_variables)\n","                capped_gvs = [(tf.clip_by_norm(grad, 5.0), var) for grad, var in gvs if grad is not None]\n","                optimizer.apply_gradients(capped_gvs)\n","                                  \n","                # Compute average loss\n","                avg_cost += total_cost / total_batch\n","                \n","            # Display some statistics about the step\n","            # Evaluating only one batch worth of data -- simplifies implementation slightly\n","            if (epoch+1) % self.display_epoch_freq == 0:\n","                dev_acc.append(evaluate_classifier(self.classify, dev_set[0:256]))\n","                train_acc.append(evaluate_classifier(self.classify, training_set[0:256]))\n","                epochs.append(epoch+1)\n","                tf.print(\"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \\\n","                    \"Dev acc:\", dev_acc[-1], \\\n","                    \"Train acc:\", train_acc[-1])  \n","        return train_acc, dev_acc, epochs\n","    \n","    def classify(self, examples):\n","        # This classifies a list of examples\n","        vectors = np.vstack([example['index_sequence'] for example in examples])\n","        logits = self.model(vectors)\n","        return np.argmax(logits, axis=1)\n","\n","np.random.seed(1)\n","tf.random.set_seed(1)\n","\n","classifier = RNNSentimentClassifier(len(word_indices), 20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l0ZpROqlNnq6"},"source":["Now let's train it. If the GRU is doing what it should, you should reach 80% accuracy within your first 200 epochs—a substantial improvement over the 70% figure we saw last week."]},{"cell_type":"code","metadata":{"id":"PlBwvZT9Nnq9"},"source":["train_acc, dev_acc, epochs = classifier.train(training_set, dev_set)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X6JEOOTmNnrI"},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline \n","\n","def plot_learning_curve(par_values, train_scores, dev_scores, title=\"Learning Curve\", xlab=\"\", ylab=\"Accuracy\", ylim=None):\n","    \"\"\"\n","    Generate a simple plot of the test and training learning curve.\n","\n","    Parameters\n","    ----------\n","    par_values : list of checked values of the current parameter.\n","    \n","    train_scores : list of scores obtained in training set (same length as par_values).\n","    \n","    test_scores : list of scores obtained in dev set (same length as par_values)\n","\n","    title : string\n","        Title for the chart.\n","\n","    ylim : tuple, shape (ymin, ymax), optional\n","        Defines minimum and maximum yvalues plotted.\n","    \"\"\"\n","    plt.figure()\n","    plt.title(title)\n","    if ylim is not None:\n","        plt.ylim(*ylim)\n","    plt.xlabel(xlab)\n","    plt.ylabel(ylab)\n","    \n","    plt.grid()\n","    plt.plot(par_values, train_scores, color=\"r\",label=\"Training score\")\n","    plt.plot(par_values, dev_scores, color=\"g\", label=\"Dev score\")\n","\n","    plt.legend(loc=\"best\")\n","    return plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PaPOrYk02uMb"},"source":["plt = plot_learning_curve(epochs, train_acc, dev_acc, xlab=\"Epoch\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VuyZiJ_YSpxG"},"source":["#  Atribution:\n","Adapted by Oier Lopez de Lacalle, Olatz Perez de Viñaspre and Ander Barrena, based on a notebook by Sam Bowman at NYU"]}]}