{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. MLP and Dropout.ipynb","provenance":[{"file_id":"1mkhICIQba8Oz0Iu3FKw1EGZTIzdFHsXv","timestamp":1541614347511}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"HzzMSlqjSGBg"},"source":["# Lab2: MLPs and Dropout"]},{"cell_type":"markdown","metadata":{"id":"oJof5oljSGBk"},"source":["First, let's load the data as before."]},{"cell_type":"code","metadata":{"id":"e8fdLvE4Ygyq"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OoRAgDVzSGBn"},"source":["# Load the data\n","import re\n","import random\n","\n","# Let's do 2-way positive/negative classification instead of 5-way    \n","def load_sst_data(path,\n","                  easy_label_map={0:0, 1:0, 2:None, 3:1, 4:1}):\n","    data = []\n","    with open(path) as f:\n","        for i, line in enumerate(f): \n","            example = {}\n","            example['label'] = easy_label_map[int(line[1])]\n","            if example['label'] is None:\n","                continue\n","            \n","            # Strip out the parse information and the phrase labels--\n","            # ---we don't need those here\n","            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n","            example['text'] = text[1:]\n","            data.append(example)\n","    random.seed(1)\n","    random.shuffle(data)\n","    return data\n","\n","sst_home = 'drive/My Drive/Colab Notebooks/dl4nlp_labs/data/trees/'\n","training_set = load_sst_data(sst_home + 'train.txt')\n","dev_set = load_sst_data(sst_home + 'dev.txt')\n","test_set = load_sst_data(sst_home + 'test.txt')\n","\n","print('Training size: {}'.format(len(training_set)))\n","print('Dev size: {}'.format(len(dev_set)))\n","print('Test size: {}'.format(len(test_set)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RvlDOvE9SGBy"},"source":["And extract bag-of-words feature vectors. For speed, we'll only use words that appear at least 10 times in the training set, leaving us with $|V|=1254$."]},{"cell_type":"code","metadata":{"id":"ZzJYCAobSGB0"},"source":["import collections\n","import numpy as np\n","\n","def feature_function(datasets):\n","    '''Annotates datasets with feature vectors.'''\n","    \n","    # Extract vocabulary\n","    def tokenize(string):\n","        return string.split()\n","    \n","    word_counter = collections.Counter()\n","    for example in datasets[0]:\n","        word_counter.update(tokenize(example['text']))\n","    \n","    vocabulary = set([word for word in word_counter if word_counter[word] > 10])\n","                                \n","    feature_names = set()\n","    for i, dataset in enumerate(datasets):\n","        for example in dataset:\n","            example['features'] = collections.defaultdict(float)\n","            \n","            # Extract features (by name) for one example\n","            word_counter = collections.Counter(tokenize(example['text']))\n","            for x in word_counter.items():\n","                if x[0] in vocabulary:\n","                    example[\"features\"][\"word_count_for_\" + x[0]] = x[1]\n","            \n","            feature_names.update(example['features'].keys())\n","                            \n","    # By now, we know what all the features will be, so we can\n","    # assign indices to them.\n","    feature_indices = dict(zip(feature_names, range(len(feature_names))))\n","    indices_to_features = {v: k for k, v in feature_indices.items()}\n","    dim = len(feature_indices)\n","                \n","    # Now we create actual vectors from those indices.\n","    for dataset in datasets:\n","        for example in dataset:\n","            example['vector'] = np.zeros((dim))\n","            for feature in example['features']:\n","                example['vector'][feature_indices[feature]] = example['features'][feature]\n","    return indices_to_features, dim\n","    \n","indices_to_features, dim = feature_function([training_set, dev_set, test_set])\n","\n","print('Vocabulary size: {}'.format(dim))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gUlD7zgkSGB7"},"source":["And define a batch evalution function."]},{"cell_type":"code","metadata":{"id":"A533GAkvSGB9"},"source":["def evaluate_classifier(classifier, eval_set):\n","    correct = 0\n","    hypotheses = classifier(eval_set)\n","    for i, example in enumerate(eval_set):\n","        hypothesis = hypotheses[i]\n","        if hypothesis == example['label']:\n","            correct += 1        \n","    return correct / float(len(eval_set))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H0Qn58-YSGCD"},"source":["## Assignments\n","\n","Now for the fun part! The below should be a working implementation of logistic regression in TensorFlow.\n","\n","### Part One:\n","\n","Modify it to turn it into an MLP with two ReLU hidden layers of 50 dimensions.\n","\n","Keep in mind that initializing weight matrices with zeros causes problems in deep neural networks trained by SGD. (Why?) You should use tf.random.normal instead, with stddev=0.1.\n","\n","If your model works, it should be able to overfit, reaching about 90% accuracy *on the training set* in the first 100 epochs.\n","\n","### Part Two:\n","\n","After each hidden layer, add dropout with a 80% keep rate (20% of drop rate). You're welcome to use `tf.nn.dropout`.\n","\n","Remember that dropout behaves differently at training time and at test time. This is not automatic. You can implement in various ways, but an easy way can be this:\n","\n","- Hint: Treat the dropout rate as an input to the model, just like `x`. At training time, feed it a value of `0.2`, at test time, feed it a value of `0.0`. You can explore different dropout values.\n","\n","If dropout works, your model should overfit less, but should still perform about as well (or, hopefully, better) on the dev set."]},{"cell_type":"code","metadata":{"id":"CMJeulPiSGCF"},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","tf.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EyS-IkAYSGCL"},"source":["class logistic_regression_classifier:\n","    def __init__(self, dim):\n","        # Define the hyperparameters\n","        self.learning_rate = 0.3  # Should be about right\n","        self.training_epochs = 100  # How long to train for - chosen to fit within class time\n","        self.display_epoch_freq = 1  # How often to test and print out statistics\n","        self.dim = dim  # The number of features\n","        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n","        \n","        # TODO: Use these.\n","        self.hidden_layer_sizes = [50, 50]\n","        self.rate = 0.2\n","\n","        # TODO: Overwrite this section\n","        ### Start of model definition ###\n","        self.trainable_variables = []\n","         # Define (most of) the model\n","        '''Variables'''\n","        #Output layer\n","        self.W = tf.Variable(tf.zeros([self.dim, 2]), dtype='float32')\n","        self.b = tf.Variable(tf.zeros([2]), dtype='float32')\n","        self.trainable_variables.append(self.W)\n","        self.trainable_variables.append(self.b)\n","\n","        \n","        # TODO: Overwrite this section\n","\n","    def model(self,x):\n","        '''Training Computation'''\n","        # TODO: Overwrite this section\n","        #Output layer activation\n","        logits = tf.matmul(x, self.W) + self.b\n","        # TODO: Overwrite this section\n","        ### End of model definition ###\n","        return logits\n","     \n","\n","    def train(self, training_data, dev_set):\n","        def get_minibatch(dataset, start_index, end_index):\n","            indices = range(start_index, end_index)\n","            vectors = np.float32(np.vstack([dataset[i]['vector'] for i in indices]))\n","            labels = [dataset[i]['label'] for i in indices]\n","            return vectors, labels\n","      \n","        print ('Training.')\n","\n","        # Training cycle\n","        for epoch in range(self.training_epochs):\n","            random.shuffle(training_set)\n","            avg_cost = 0.\n","            total_batch = int(len(training_set) / self.batch_size)\n","            # Loop over all batches in epoch\n","            for i in range(total_batch):\n","                # Assemble a minibatch of the next B examples\n","                minibatch_vectors, minibatch_labels = get_minibatch(training_set, \n","                                                                    self.batch_size * i, \n","                                                                    self.batch_size * (i + 1))\n","                # Run the optimizer to take a gradient step, and also fetch the value of the \n","                # cost function for logging\n","                with tf.GradientTape() as tape:\n","                  logits = self.model(minibatch_vectors)\n","                  # Define the cost function (here, the exp and sum are built in)\n","                  cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=minibatch_labels))\n","                gradients = tape.gradient(cost, self.trainable_variables)\n","                optimizer = tf.optimizers.SGD(self.learning_rate)\n","                optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n","                # Compute average loss\n","                avg_cost += cost / total_batch\n","                \n","                # Display some statistics about the step\n","            if (epoch+1) % self.display_epoch_freq == 0:\n","                tf.print (\"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \\\n","                    \"Dev acc:\", evaluate_classifier(self.classify, dev_set[0:500]), \\\n","                    \"Train acc:\", evaluate_classifier(self.classify, training_set[0:500]))\n","    \n","    def classify(self, examples):\n","        # This classifies a list of examples\n","        vectors = np.float32(np.vstack([example['vector'] for example in examples]))\n","        logits = self.model(vectors)\n","        return np.argmax(logits, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sU7c7S6rSGCT"},"source":["Now let's train it."]},{"cell_type":"code","metadata":{"id":"kymCD3LkSGCW"},"source":["classifier = logistic_regression_classifier(dim)\n","classifier.train(training_set, dev_set)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F_6kMcOhSGCe"},"source":["And evaluate it."]},{"cell_type":"code","metadata":{"id":"ZkTmNJpCSGCf"},"source":["evaluate_classifier(classifier.classify, test_set)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kjBUT-D2spyT"},"source":["# Atribution:\n","Adapted by Oier Lopez de Lacalle, Olatz Perez de Vi√±aspre and Ander Barrena, based on a notebook by Sam Bowman at NYU"]}]}