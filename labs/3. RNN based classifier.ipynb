{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. RNN based classifier.ipynb","provenance":[{"file_id":"1N3d6R4lnmdNEIh3VWdAAOeP6bQc0kSbQ","timestamp":1545915164296},{"file_id":"1K4mLe1n4WyLsDu43z3dAQlYxTec9TENC","timestamp":1545914730104}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"YxcogNx1spwp"},"source":["# Lab3: Sentiment, but slower!"]},{"cell_type":"markdown","metadata":{"id":"w7huCqsRspww"},"source":["In this assignment, you'll implement an **RNN-based sentence classifier**. Plain ol' RNNs aren't very good at sentiment classification, and they're very picky about things like learning rates. However, they're the foundation for things like LSTMs, which we'll learn about next week, and which *are* quite useful."]},{"cell_type":"markdown","metadata":{"id":"tcEY8ElAspw4"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"v2LXIRspspw_"},"source":["First, let's load the data as before."]},{"cell_type":"code","metadata":{"id":"MvjTs7K1t17g"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WWRmPqvTspxB"},"source":["import re\n","import random\n","\n","# Let's do 2-way positive/negative classification instead of 5-way\n","def load_sst_data(path,\n","                  easy_label_map={0:0, 1:0, 2:None, 3:1, 4:1}):\n","    data = []\n","    with open(path) as f:\n","        for i, line in enumerate(f): \n","            example = {}\n","            example['label'] = easy_label_map[int(line[1])]\n","            if example['label'] is None:\n","                continue\n","            \n","            # Strip out the parse information and the phrase labels--\n","            # ---we don't need those here\n","            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n","            example['text'] = text[1:]\n","            data.append(example)\n","    \n","    random.seed(1)\n","    random.shuffle(data)\n","    return data\n","   \n","sst_home = 'drive/My Drive/Colab Notebooks/dl4nlp_labs/data/trees/'\n","training_set = load_sst_data(sst_home + '/train.txt')\n","dev_set = load_sst_data(sst_home + '/dev.txt')\n","test_set = load_sst_data(sst_home + '/test.txt')\n","\n","print('Training size: {}'.format(len(training_set)))\n","print('Dev size: {}'.format(len(dev_set)))\n","print('Test size: {}'.format(len(test_set)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vaoqE0D7spxL"},"source":["Next, we'll convert the data to __index vectors__.\n","\n","To simplify your implementation, we'll use a __fixed unrolling length of 20__. In the conversion process, we'll cut off excess words (towards the left/start end of the sentence), pad short sentences (to the left) with a special word symbol `<PAD>`, and mark out-of-vocabulary words with `<UNK>`, for unknown. As in the previous assignment, we'll use a very small vocabulary for this assignment, so you'll see `<UNK>` often."]},{"cell_type":"code","metadata":{"id":"L7d0AMI6spxQ"},"source":["import collections\n","import numpy as np\n","\n","def sentence_to_padded_index_sequence(datasets):\n","    '''Annotates datasets with feature vectors.'''\n","    \n","    PADDING = \"<PAD>\"\n","    UNKNOWN = \"<UNK>\"\n","    SEQ_LEN = 20\n","    \n","    # Extract vocabulary\n","    def tokenize(string):\n","        return string.lower().split()\n","    \n","    word_counter = collections.Counter()\n","    for example in datasets[0]:\n","        word_counter.update(tokenize(example['text']))\n","    \n","    vocabulary = set([word for word in word_counter if word_counter[word] > 10])\n","    vocabulary = list(vocabulary)\n","    vocabulary = [PADDING, UNKNOWN] + vocabulary\n","        \n","    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n","    indices_to_words = {v: k for k, v in word_indices.items()}\n","        \n","    for i, dataset in enumerate(datasets):\n","        for example in dataset:\n","            example['index_sequence'] = np.zeros((SEQ_LEN), dtype=np.int32)\n","            \n","            token_sequence = tokenize(example['text'])\n","            padding = SEQ_LEN - len(token_sequence)\n","            \n","            for i in range(SEQ_LEN):\n","                if i >= padding:\n","                    if token_sequence[i - padding] in word_indices:\n","                        index = word_indices[token_sequence[i - padding]]\n","                    else:\n","                        index = word_indices[UNKNOWN]\n","                else:\n","                    index = word_indices[PADDING]\n","                example['index_sequence'][i] = index\n","    return indices_to_words, word_indices\n","    \n","indices_to_words, word_indices = sentence_to_padded_index_sequence([training_set, dev_set, test_set])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Apdx7iJospxW"},"source":["print(training_set[18])\n","print(len(word_indices))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0_qTijrGspxf"},"source":["def evaluate_classifier(classifier, eval_set):\n","    correct = 0\n","    hypotheses = classifier(eval_set)\n","    for i, example in enumerate(eval_set):\n","        hypothesis = hypotheses[i]\n","        if hypothesis == example['label']:\n","            correct += 1        \n","    return correct / float(len(eval_set))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cLcqDcjvspxm"},"source":["## Assignments: Building the RNN"]},{"cell_type":"markdown","metadata":{"id":"_uC9N01Mspxq"},"source":["Replace the TODOs in the code below to make RNN work. If it's set up properly, it should reach dev set accuracy of about 0.70 within 500 epochs with the given hyperparameters.\n","\n","You will find 3 TODOs in the code.\n","\n","### TODO 1:\n","\n","- You have to define the RNN parameters (attribute *self.dim* sets dimmension of hidden state). \n","\n","- (Hint) The paremters take input's embedding (*self.embedding_dim*) and the previous hidden state (*self.dim*) and provides the current hidden state (*self.dim*).\n","\n","### TODO 2:\n","\n","- Write a (very short) Python function that defines one step of an RNN. (Hint) In each step current input and previous hidden states are involved. \n","\n","- Recall from slides: $f(h_{t-1}, p_t) = tanh(W[h_{t-1};p_t])$. Note that input $x$ at time step $t$ is *translated* to its embedding representation. \n","\n","\n","![](https://drive.google.com/uc?id=1VNI--El3renuefGD0R7AOlcxI4ycLj4V)\n","\n","\n","### TODO 3:\n","\n","- Unroll the RNN using a *for* loop, and obtain the sentence representation with the final hidden state.\n","\n","- (Hint) Note that we are vectorizing the whole minibatch. That is, in each step we are processing all the examples in the batch together in one go. Try to understand the following two code lines:\n","\n","   $\\rightarrow$ ``self.x_slices = tf.split(self.x, self.sequence_length, 1)``\n","   \n","   $\\rightarrow$ ``self.h_zero = tf.zeros([self.batch_size, self.dim])``\n","   \n","- (Hint) It might be a good idea to reshape (tf.reshape) the tensor at step t in a single tensor. "]},{"cell_type":"code","metadata":{"id":"tG8H7bYgspxw"},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","tf.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"piE_Cr6zspx6"},"source":["class RNNSentimentClassifier:\n","    def __init__(self, vocab_size, sequence_length):\n","        # Define the hyperparameters\n","        self.learning_rate = 0.2  # Should be about right\n","        self.training_epochs = 500  # How long to train for - chosen to fit within class time\n","        self.display_epoch_freq = 5  # How often to test and print out statistics\n","        self.dim = 24  # The dimension of the hidden state of the RNN\n","        self.embedding_dim = 8  # The dimension of the learned word embeddings\n","        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n","        self.vocab_size = vocab_size  # Defined by the file reader above\n","        self.sequence_length = sequence_length  # Defined by the file reader above\n","        self.l2_lambda = 0.001\n","        \n","        self.trainable_variables = []\n","\n","        # Define the parameters\n","        self.E = tf.Variable(tf.random.normal([self.vocab_size, self.embedding_dim], stddev=0.1))\n","        self.trainable_variables.append(self.E)\n","        \n","        self.W_cl = tf.Variableself.E = tf.Variable(tf.random.normal([self.vocab_size, self.embedding_dim], stddev=0(tf.random.normal([self.dim, 2], stddev=0.1))\n","        self.b_cl = tf.Variable(tf.random.normal([2], stddev=0.1))\n","        self.trainable_variables.append(self.W_cl)\n","        self.trainable_variables.append(self.b_cl)\n","        \n","        # TODO 1: Define the RNN parameters\n","        \n","        \n","    def model(self,x):\n","        # Split up the inputs into individual tensors\n","        self.x_slices = tf.split(x, self.sequence_length, 1)\n","    \n","        # Define the start state of the RNN\n","        self.h_zero = tf.zeros([self.batch_size, self.dim])  \n","        \n","        # TODO 2: Write a (very short) Python function that defines one step of an RNN\n","        def step(x, h_prev):\n","            #add your code here\n","            return h\n","        \n","        # TODO 3: Unroll the RNN using a for loop, and obtain the sentence representation with the final hidden state\n","        sentence_representation = None\n","\n","        # Compute the logits using one last linear layer\n","        logits = tf.matmul(sentence_representation, self.W_cl) + self.b_cl\n","        return logits\n","\n","    def train(self, training_data, dev_set):\n","        def get_minibatch(dataset, start_index, end_index):\n","            indices = range(start_index, end_index)\n","            vectors = np.vstack([dataset[i]['index_sequence'] for i in indices])\n","            labels = [dataset[i]['label'] for i in indices]\n","            return vectors, labels\n","      \n","        print('Training.')\n","\n","        # Training cycle\n","        for epoch in range(self.training_epochs):\n","            random.shuffle(training_set)\n","            avg_cost = 0.\n","            total_batch = int(len(training_set) / self.batch_size)\n","            \n","            # Loop over all batches in epoch\n","            for i in range(total_batch):\n","                # Assemble a minibatch of the next B examples\n","                miniself.E = tf.Variable(tf.random.normal([self.vocab_size, self.embedding_dim], stddev=0batch_vectors, minibatch_labels = get_minibatch(training_set, \n","                                                                    self.batch_size * i, \n","                                                                    self.batch_size * (i + 1))\n","\n","                # Run the optimizer to take a gradient step, and also fetch the value of the \n","                # cost function for logging\n","                with tf.GradientTape() as tape:\n","                  logits = self.model(minibatch_vectors)\n","                \n","                  # Define the L2 cost\n","                  self.l2_cost = self.l2_lambda * (tf.reduce_sum(tf.square(self.W_rnn)) +\n","                                                  tf.reduce_sum(tf.square(self.W_cl)))\n","\n","                  # Define the cost function (here, the softmax exp and sum are built in)\n","                  total_cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=minibatch_labels, logits=logits) + self.l2_cost)\n","        \n","                # This  performs the main SGD update equation with gradient clipping\n","                optimizer = tf.optimizers.SGD(self.learning_rate)\n","                gradients = tape.gradient(total_cost, self.trainable_variables)\n","                gvs = zip(gradients, self.trainable_variables)\n","                capped_gvs = [(tf.clip_by_norm(grad, 5.0), var) for grad, var in gvs if grad is not None]\n","                optimizer.apply_gradients(capped_gvs)\n","                                                                            \n","                # Compute average loss\n","                avg_cost += total_cost / total_batch\n","                \n","            # Display some statistics about the step\n","            # Evaluating only one batch worth of data -- simplifies implementation slightly\n","            if (epoch+1) % self.display_epoch_freq == 0:\n","                tf.print(\"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \\\n","                    \"Dev acc:\", evaluate_classifier(self.classify, dev_set[0:256]), \\\n","                    \"Train acc:\", evaluate_classifier(self.classify, training_set[0:256]))  \n","    \n","    def classify(self, examples):\n","        # This classifies a list of examples\n","        vectors = np.vstack([example['index_sequence'] for example in examples])\n","        logits = self.model(vectors)\n","        return np.argmax(logits, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"26SBs4iespyE"},"source":["classifier = RNNSentimentClassifier(len(word_indices), 20)\n","classifier.train(training_set, dev_set)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kjBUT-D2spyT"},"source":["# Atribution:\n","Adapted by Oier Lopez de Lacalle, Olatz Perez de Vi√±aspre and Ander Barrena, based on a notebook by Sam Bowman at NYU"]}]}