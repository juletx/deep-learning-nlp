{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. RNN based classifier.ipynb","provenance":[{"file_id":"1N3d6R4lnmdNEIh3VWdAAOeP6bQc0kSbQ","timestamp":1545915164296},{"file_id":"1K4mLe1n4WyLsDu43z3dAQlYxTec9TENC","timestamp":1545914730104}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"YxcogNx1spwp"},"source":["# Lab3: Sentiment, but slower!"]},{"cell_type":"markdown","metadata":{"id":"w7huCqsRspww"},"source":["In this assignment, you'll implement an **RNN-based sentence classifier**. Plain ol' RNNs aren't very good at sentiment classification, and they're very picky about things like learning rates. However, they're the foundation for things like LSTMs, which we'll learn about next week, and which *are* quite useful."]},{"cell_type":"markdown","metadata":{"id":"tcEY8ElAspw4"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"v2LXIRspspw_"},"source":["First, let's load the data as before."]},{"cell_type":"code","metadata":{"id":"MvjTs7K1t17g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642437410208,"user_tz":-60,"elapsed":66327,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"f7026d3e-ef85-4e41-ac65-254484c56034"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/LAP/Subjects/DL/labs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"smUZ-bsa9kQv","executionInfo":{"status":"ok","timestamp":1642438807514,"user_tz":-60,"elapsed":728,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"9879afb2-2140-4c56-8752-c03e82038067"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/LAP/Subjects/DL/labs\n"]}]},{"cell_type":"code","metadata":{"id":"WWRmPqvTspxB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642438840652,"user_tz":-60,"elapsed":1619,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"b88fc405-348b-4db7-9c2c-c08e9f28da9d"},"source":["import re\n","import random\n","\n","# Let's do 2-way positive/negative classification instead of 5-way\n","def load_sst_data(path,\n","                  easy_label_map={0:0, 1:0, 2:None, 3:1, 4:1}):\n","    data = []\n","    with open(path) as f:\n","        for i, line in enumerate(f): \n","            example = {}\n","            example['label'] = easy_label_map[int(line[1])]\n","            if example['label'] is None:\n","                continue\n","            \n","            # Strip out the parse information and the phrase labels--\n","            # ---we don't need those here\n","            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n","            example['text'] = text[1:]\n","            data.append(example)\n","    \n","    random.seed(1)\n","    random.shuffle(data)\n","    return data\n","   \n","sst_home = '../data/trees/'\n","training_set = load_sst_data(sst_home + '/train.txt')\n","dev_set = load_sst_data(sst_home + '/dev.txt')\n","test_set = load_sst_data(sst_home + '/test.txt')\n","\n","print('Training size: {}'.format(len(training_set)))\n","print('Dev size: {}'.format(len(dev_set)))\n","print('Test size: {}'.format(len(test_set)))"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Training size: 6920\n","Dev size: 872\n","Test size: 1821\n"]}]},{"cell_type":"markdown","metadata":{"id":"vaoqE0D7spxL"},"source":["Next, we'll convert the data to __index vectors__.\n","\n","To simplify your implementation, we'll use a __fixed unrolling length of 20__. In the conversion process, we'll cut off excess words (towards the left/start end of the sentence), pad short sentences (to the left) with a special word symbol `<PAD>`, and mark out-of-vocabulary words with `<UNK>`, for unknown. As in the previous assignment, we'll use a very small vocabulary for this assignment, so you'll see `<UNK>` often."]},{"cell_type":"code","metadata":{"id":"L7d0AMI6spxQ","executionInfo":{"status":"ok","timestamp":1642438887922,"user_tz":-60,"elapsed":314,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["import collections\n","import numpy as np\n","\n","def sentence_to_padded_index_sequence(datasets):\n","    '''Annotates datasets with feature vectors.'''\n","    \n","    PADDING = \"<PAD>\"\n","    UNKNOWN = \"<UNK>\"\n","    SEQ_LEN = 20\n","    \n","    # Extract vocabulary\n","    def tokenize(string):\n","        return string.lower().split()\n","    \n","    word_counter = collections.Counter()\n","    for example in datasets[0]:\n","        word_counter.update(tokenize(example['text']))\n","    \n","    vocabulary = set([word for word in word_counter if word_counter[word] > 10])\n","    vocabulary = list(vocabulary)\n","    vocabulary = [PADDING, UNKNOWN] + vocabulary\n","        \n","    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n","    indices_to_words = {v: k for k, v in word_indices.items()}\n","        \n","    for i, dataset in enumerate(datasets):\n","        for example in dataset:\n","            example['index_sequence'] = np.zeros((SEQ_LEN), dtype=np.int32)\n","            \n","            token_sequence = tokenize(example['text'])\n","            padding = SEQ_LEN - len(token_sequence)\n","            \n","            for i in range(SEQ_LEN):\n","                if i >= padding:\n","                    if token_sequence[i - padding] in word_indices:\n","                        index = word_indices[token_sequence[i - padding]]\n","                    else:\n","                        index = word_indices[UNKNOWN]\n","                else:\n","                    index = word_indices[PADDING]\n","                example['index_sequence'][i] = index\n","    return indices_to_words, word_indices\n","    \n","indices_to_words, word_indices = sentence_to_padded_index_sequence([training_set, dev_set, test_set])"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Apdx7iJospxW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642438944280,"user_tz":-60,"elapsed":271,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"6230619e-a7d8-442b-f02c-283474d8895f"},"source":["print(training_set[18])\n","print(len(word_indices))"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["{'label': 1, 'text': 'As the dominant Christine , Sylvie Testud is icily brilliant .', 'index_sequence': array([  0,   0,   0,   0,   0,   0,   0,   0,   0,  37, 985,   1,   1,\n","       138,   1,   1, 793,   1, 150, 845], dtype=int32)}\n","1250\n"]}]},{"cell_type":"code","metadata":{"id":"0_qTijrGspxf","executionInfo":{"status":"ok","timestamp":1642439218986,"user_tz":-60,"elapsed":262,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["def evaluate_classifier(classifier, eval_set):\n","    correct = 0\n","    hypotheses = classifier(eval_set)\n","    for i, example in enumerate(eval_set):\n","        hypothesis = hypotheses[i]\n","        if hypothesis == example['label']:\n","            correct += 1        \n","    return correct / float(len(eval_set))"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cLcqDcjvspxm"},"source":["## Assignments: Building the RNN"]},{"cell_type":"markdown","metadata":{"id":"_uC9N01Mspxq"},"source":["Replace the TODOs in the code below to make RNN work. If it's set up properly, it should reach dev set accuracy of about 0.70 within 500 epochs with the given hyperparameters.\n","\n","You will find 3 TODOs in the code.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","source":["### TODO 1:\n","\n","- You have to define the RNN parameters (attribute *self.dim* sets dimmension of hidden state). \n","\n","- (Hint) The paremters take input's embedding (*self.embedding_dim*) and the previous hidden state (*self.dim*) and provides the current hidden state (*self.dim*)."],"metadata":{"id":"l_cnWrHBAyry"}},{"cell_type":"markdown","source":["### TODO 2:\n","\n","- Write a (very short) Python function that defines one step of an RNN. (Hint) In each step current input and previous hidden states are involved. \n","\n","- Recall from slides: $f(h_{t-1}, p_t) = tanh(W[h_{t-1};p_t])$. Note that input $x$ at time step $t$ is *translated* to its embedding representation. \n","\n","\n","![](https://drive.google.com/uc?id=1VNI--El3renuefGD0R7AOlcxI4ycLj4V)"],"metadata":{"id":"d6CDkFZwAviR"}},{"cell_type":"markdown","source":["### TODO 3:\n","\n","- Unroll the RNN using a *for* loop, and obtain the sentence representation with the final hidden state.\n","\n","- (Hint) Note that we are vectorizing the whole minibatch. That is, in each step we are processing all the examples in the batch together in one go. Try to understand the following two code lines:\n","\n","   $\\rightarrow$ ``self.x_slices = tf.split(self.x, self.sequence_length, 1)``\n","   \n","   $\\rightarrow$ ``self.h_zero = tf.zeros([self.batch_size, self.dim])``\n","   \n","- (Hint) It might be a good idea to reshape (tf.reshape) the tensor at step t in a single tensor. "],"metadata":{"id":"X9wpkJqEAsqk"}},{"cell_type":"code","metadata":{"id":"tG8H7bYgspxw","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1642439345000,"user_tz":-60,"elapsed":2275,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"e0b25928-1dc4-4ce5-b43c-8e9dad8862ae"},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","tf.__version__"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.7.0'"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"piE_Cr6zspx6","executionInfo":{"status":"ok","timestamp":1642444315192,"user_tz":-60,"elapsed":315,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["class RNNSentimentClassifier:\n","    def __init__(self, vocab_size, sequence_length):\n","        # Define the hyperparameters\n","        self.learning_rate = 0.2  # Should be about right\n","        self.training_epochs = 500  # How long to train for - chosen to fit within class time\n","        self.display_epoch_freq = 5  # How often to test and print out statistics\n","        self.dim = 24  # The dimension of the hidden state of the RNN\n","        self.embedding_dim = 8  # The dimension of the learned word embeddings\n","        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n","        self.vocab_size = vocab_size  # Defined by the file reader above\n","        self.sequence_length = sequence_length  # Defined by the file reader above\n","        self.l2_lambda = 0.001\n","        \n","        self.trainable_variables = []\n","\n","        # Define the parameters\n","        self.E = tf.Variable(tf.random.normal([self.vocab_size, self.embedding_dim], stddev=0.1))\n","        self.trainable_variables.append(self.E)\n","        \n","        self.W_cl = tf.Variable(tf.random.normal([self.dim, 2], stddev=0.1))\n","        self.b_cl = tf.Variable(tf.random.normal([2], stddev=0.1))\n","        self.trainable_variables.append(self.W_cl)\n","        self.trainable_variables.append(self.b_cl)\n","        \n","        # TODO 1: Define the RNN parameters\n","        self.W_rnn = tf.Variable(tf.random.normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n","        self.b = tf.Variable(tf.random.normal([self.dim], stddev=0.1))\n","        self.trainable_variables.append(self.W_rnn)\n","        self.trainable_variables.append(self.b)\n","        \n","        \n","    def model(self,x):\n","        # Split up the inputs into individual tensors\n","        self.x_slices = tf.split(x, self.sequence_length, 1)\n","    \n","        # Define the start state of the RNN\n","        self.h_zero = tf.zeros([self.batch_size, self.dim])  \n","        \n","        # TODO 2: Write a (very short) Python function that defines one step of an RNN\n","        def step(x, h_prev):\n","            #add your code here\n","            e = tf.nn.embedding_lookup(self.E, x)\n","            er = tf.reshape(e, [self.batch_size, self.embedding_dim])\n","            c = tf.concat([h_prev, er], axis=1)\n","            h = tf.nn.tanh(tf.matmul(c, self.W_rnn) + self.b)\n","            return h\n","        \n","        # TODO 3: Unroll the RNN using a for loop, and obtain the sentence representation with the final hidden state\n","        h_prev = self.h_zero\n","        for x in self.x_slices:\n","            h_prev = step(x, h_prev)\n","        sentence_representation = h_prev\n","\n","        # Compute the logits using one last linear layer\n","        logits = tf.matmul(sentence_representation, self.W_cl) + self.b_cl\n","        return logits\n","\n","    def train(self, training_data, dev_set):\n","        def get_minibatch(dataset, start_index, end_index):\n","            indices = range(start_index, end_index)\n","            vectors = np.vstack([dataset[i]['index_sequence'] for i in indices])\n","            labels = [dataset[i]['label'] for i in indices]\n","            return vectors, labels\n","      \n","        print('Training.')\n","\n","        # Training cycle\n","        for epoch in range(self.training_epochs):\n","            random.shuffle(training_set)\n","            avg_cost = 0.\n","            total_batch = int(len(training_set) / self.batch_size)\n","            \n","            # Loop over all batches in epoch\n","            for i in range(total_batch):\n","                # Assemble a minibatch of the next B examples\n","                minibatch_vectors, minibatch_labels = get_minibatch(training_set, \n","                                                                    self.batch_size * i, \n","                                                                    self.batch_size * (i + 1))\n","\n","                # Run the optimizer to take a gradient step, and also fetch the value of the \n","                # cost function for logging\n","                with tf.GradientTape() as tape:\n","                  logits = self.model(minibatch_vectors)\n","                \n","                  # Define the L2 cost\n","                  self.l2_cost = self.l2_lambda * (tf.reduce_sum(tf.square(self.W_rnn)) +\n","                                                  tf.reduce_sum(tf.square(self.W_cl)))\n","\n","                  # Define the cost function (here, the softmax exp and sum are built in)\n","                  total_cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=minibatch_labels, logits=logits) + self.l2_cost)\n","        \n","                # This  performs the main SGD update equation with gradient clipping\n","                optimizer = tf.optimizers.SGD(self.learning_rate)\n","                gradients = tape.gradient(total_cost, self.trainable_variables)\n","                gvs = zip(gradients, self.trainable_variables)\n","                capped_gvs = [(tf.clip_by_norm(grad, 5.0), var) for grad, var in gvs if grad is not None]\n","                optimizer.apply_gradients(capped_gvs)\n","                                                                            \n","                # Compute average loss\n","                avg_cost += total_cost / total_batch\n","                \n","            # Display some statistics about the step\n","            # Evaluating only one batch worth of data -- simplifies implementation slightly\n","            if (epoch+1) % self.display_epoch_freq == 0:\n","                tf.print(\"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \\\n","                    \"Dev acc:\", evaluate_classifier(self.classify, dev_set[0:256]), \\\n","                    \"Train acc:\", evaluate_classifier(self.classify, training_set[0:256]))  \n","    \n","    def classify(self, examples):\n","        # This classifies a list of examples\n","        vectors = np.vstack([example['index_sequence'] for example in examples])\n","        logits = self.model(vectors)\n","        return np.argmax(logits, axis=1)"],"execution_count":84,"outputs":[]},{"cell_type":"code","metadata":{"id":"26SBs4iespyE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642444956723,"user_tz":-60,"elapsed":639483,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"e58d6d55-00a5-44ce-ac4a-edf0562bbf65"},"source":["classifier = RNNSentimentClassifier(len(word_indices), 20)\n","classifier.train(training_set, dev_set)"],"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["Training.\n","Epoch: 5 Cost: 0.699828625 Dev acc: 0.5546875 Train acc: 0.55078125\n","Epoch: 10 Cost: 0.698900521 Dev acc: 0.5546875 Train acc: 0.50390625\n","Epoch: 15 Cost: 0.698470056 Dev acc: 0.5546875 Train acc: 0.5546875\n","Epoch: 20 Cost: 0.697694123 Dev acc: 0.5546875 Train acc: 0.5625\n","Epoch: 25 Cost: 0.69688791 Dev acc: 0.5546875 Train acc: 0.55078125\n","Epoch: 30 Cost: 0.696421564 Dev acc: 0.5546875 Train acc: 0.55078125\n","Epoch: 35 Cost: 0.695888758 Dev acc: 0.5703125 Train acc: 0.48046875\n","Epoch: 40 Cost: 0.695547044 Dev acc: 0.5703125 Train acc: 0.50390625\n","Epoch: 45 Cost: 0.694877 Dev acc: 0.5546875 Train acc: 0.46875\n","Epoch: 50 Cost: 0.694469273 Dev acc: 0.546875 Train acc: 0.5546875\n","Epoch: 55 Cost: 0.694353521 Dev acc: 0.5546875 Train acc: 0.5390625\n","Epoch: 60 Cost: 0.693891048 Dev acc: 0.5703125 Train acc: 0.5703125\n","Epoch: 65 Cost: 0.693745077 Dev acc: 0.55078125 Train acc: 0.53515625\n","Epoch: 70 Cost: 0.693454623 Dev acc: 0.5546875 Train acc: 0.51953125\n","Epoch: 75 Cost: 0.692985117 Dev acc: 0.5546875 Train acc: 0.5390625\n","Epoch: 80 Cost: 0.692700446 Dev acc: 0.55078125 Train acc: 0.5546875\n","Epoch: 85 Cost: 0.692410827 Dev acc: 0.5546875 Train acc: 0.5234375\n","Epoch: 90 Cost: 0.692121208 Dev acc: 0.5546875 Train acc: 0.46875\n","Epoch: 95 Cost: 0.691822886 Dev acc: 0.5546875 Train acc: 0.50390625\n","Epoch: 100 Cost: 0.691423714 Dev acc: 0.5546875 Train acc: 0.56640625\n","Epoch: 105 Cost: 0.690931201 Dev acc: 0.5625 Train acc: 0.5703125\n","Epoch: 110 Cost: 0.690528035 Dev acc: 0.5546875 Train acc: 0.52734375\n","Epoch: 115 Cost: 0.689460337 Dev acc: 0.56640625 Train acc: 0.5390625\n","Epoch: 120 Cost: 0.687757552 Dev acc: 0.5703125 Train acc: 0.5859375\n","Epoch: 125 Cost: 0.683608711 Dev acc: 0.578125 Train acc: 0.609375\n","Epoch: 130 Cost: 0.678121626 Dev acc: 0.6015625 Train acc: 0.56640625\n","Epoch: 135 Cost: 0.676316082 Dev acc: 0.57421875 Train acc: 0.5625\n","Epoch: 140 Cost: 0.663686216 Dev acc: 0.5546875 Train acc: 0.60546875\n","Epoch: 145 Cost: 0.661777318 Dev acc: 0.62109375 Train acc: 0.6484375\n","Epoch: 150 Cost: 0.65663451 Dev acc: 0.64453125 Train acc: 0.64453125\n","Epoch: 155 Cost: 0.64871794 Dev acc: 0.5234375 Train acc: 0.56640625\n","Epoch: 160 Cost: 0.644537628 Dev acc: 0.6640625 Train acc: 0.7109375\n","Epoch: 165 Cost: 0.631975 Dev acc: 0.6640625 Train acc: 0.65234375\n","Epoch: 170 Cost: 0.63422519 Dev acc: 0.68359375 Train acc: 0.6953125\n","Epoch: 175 Cost: 0.635665655 Dev acc: 0.6875 Train acc: 0.70703125\n","Epoch: 180 Cost: 0.622003794 Dev acc: 0.58203125 Train acc: 0.625\n","Epoch: 185 Cost: 0.623815298 Dev acc: 0.6640625 Train acc: 0.6640625\n","Epoch: 190 Cost: 0.612411797 Dev acc: 0.70703125 Train acc: 0.734375\n","Epoch: 195 Cost: 0.604988575 Dev acc: 0.6796875 Train acc: 0.68359375\n","Epoch: 200 Cost: 0.603925109 Dev acc: 0.70703125 Train acc: 0.7578125\n","Epoch: 205 Cost: 0.593141437 Dev acc: 0.67578125 Train acc: 0.71484375\n","Epoch: 210 Cost: 0.594235182 Dev acc: 0.68359375 Train acc: 0.7265625\n","Epoch: 215 Cost: 0.593310237 Dev acc: 0.62890625 Train acc: 0.65625\n","Epoch: 220 Cost: 0.583444297 Dev acc: 0.6796875 Train acc: 0.65625\n","Epoch: 225 Cost: 0.580983758 Dev acc: 0.734375 Train acc: 0.734375\n","Epoch: 230 Cost: 0.583169937 Dev acc: 0.5859375 Train acc: 0.66015625\n","Epoch: 235 Cost: 0.583995342 Dev acc: 0.6875 Train acc: 0.69140625\n","Epoch: 240 Cost: 0.582280934 Dev acc: 0.64453125 Train acc: 0.70703125\n","Epoch: 245 Cost: 0.573282063 Dev acc: 0.640625 Train acc: 0.765625\n","Epoch: 250 Cost: 0.576776266 Dev acc: 0.71875 Train acc: 0.78515625\n","Epoch: 255 Cost: 0.574197352 Dev acc: 0.703125 Train acc: 0.75390625\n","Epoch: 260 Cost: 0.565137923 Dev acc: 0.6953125 Train acc: 0.70703125\n","Epoch: 265 Cost: 0.56482482 Dev acc: 0.70703125 Train acc: 0.71875\n","Epoch: 270 Cost: 0.55368 Dev acc: 0.58984375 Train acc: 0.67578125\n","Epoch: 275 Cost: 0.567296565 Dev acc: 0.69140625 Train acc: 0.73046875\n","Epoch: 280 Cost: 0.557710588 Dev acc: 0.7265625 Train acc: 0.78515625\n","Epoch: 285 Cost: 0.553541958 Dev acc: 0.75390625 Train acc: 0.765625\n","Epoch: 290 Cost: 0.565156698 Dev acc: 0.71875 Train acc: 0.8046875\n","Epoch: 295 Cost: 0.560314834 Dev acc: 0.67578125 Train acc: 0.7734375\n","Epoch: 300 Cost: 0.55010128 Dev acc: 0.6953125 Train acc: 0.7578125\n","Epoch: 305 Cost: 0.576847136 Dev acc: 0.703125 Train acc: 0.77734375\n","Epoch: 310 Cost: 0.545953453 Dev acc: 0.74609375 Train acc: 0.828125\n","Epoch: 315 Cost: 0.540531158 Dev acc: 0.69140625 Train acc: 0.7578125\n","Epoch: 320 Cost: 0.561447263 Dev acc: 0.734375 Train acc: 0.8125\n","Epoch: 325 Cost: 0.527414501 Dev acc: 0.6328125 Train acc: 0.6875\n","Epoch: 330 Cost: 0.56690228 Dev acc: 0.7421875 Train acc: 0.77734375\n","Epoch: 335 Cost: 0.535838187 Dev acc: 0.62890625 Train acc: 0.7109375\n","Epoch: 340 Cost: 0.526377261 Dev acc: 0.72265625 Train acc: 0.80078125\n","Epoch: 345 Cost: 0.540034175 Dev acc: 0.74609375 Train acc: 0.80078125\n","Epoch: 350 Cost: 0.528961301 Dev acc: 0.68359375 Train acc: 0.77734375\n","Epoch: 355 Cost: 0.516449451 Dev acc: 0.66796875 Train acc: 0.70703125\n","Epoch: 360 Cost: 0.539512575 Dev acc: 0.73046875 Train acc: 0.765625\n","Epoch: 365 Cost: 0.515768111 Dev acc: 0.671875 Train acc: 0.73828125\n","Epoch: 370 Cost: 0.520738482 Dev acc: 0.72265625 Train acc: 0.78125\n","Epoch: 375 Cost: 0.512063146 Dev acc: 0.734375 Train acc: 0.7890625\n","Epoch: 380 Cost: 0.505467772 Dev acc: 0.70703125 Train acc: 0.76953125\n","Epoch: 385 Cost: 0.507276177 Dev acc: 0.71484375 Train acc: 0.74609375\n","Epoch: 390 Cost: 0.494720459 Dev acc: 0.734375 Train acc: 0.8046875\n","Epoch: 395 Cost: 0.535618246 Dev acc: 0.75 Train acc: 0.82421875\n","Epoch: 400 Cost: 0.505109727 Dev acc: 0.75390625 Train acc: 0.80859375\n","Epoch: 405 Cost: 0.496509761 Dev acc: 0.7421875 Train acc: 0.8046875\n","Epoch: 410 Cost: 0.505317032 Dev acc: 0.75390625 Train acc: 0.79296875\n","Epoch: 415 Cost: 0.502978384 Dev acc: 0.671875 Train acc: 0.74609375\n","Epoch: 420 Cost: 0.492827445 Dev acc: 0.72265625 Train acc: 0.7890625\n","Epoch: 425 Cost: 0.482949972 Dev acc: 0.73828125 Train acc: 0.79296875\n","Epoch: 430 Cost: 0.526049674 Dev acc: 0.74609375 Train acc: 0.83203125\n","Epoch: 435 Cost: 0.493227303 Dev acc: 0.60546875 Train acc: 0.6875\n","Epoch: 440 Cost: 0.520987093 Dev acc: 0.7109375 Train acc: 0.78125\n","Epoch: 445 Cost: 0.482290715 Dev acc: 0.71875 Train acc: 0.76953125\n","Epoch: 450 Cost: 0.473078251 Dev acc: 0.6484375 Train acc: 0.73828125\n","Epoch: 455 Cost: 0.50975728 Dev acc: 0.73046875 Train acc: 0.83984375\n","Epoch: 460 Cost: 0.454049855 Dev acc: 0.671875 Train acc: 0.72265625\n","Epoch: 465 Cost: 0.489855945 Dev acc: 0.73828125 Train acc: 0.7421875\n","Epoch: 470 Cost: 0.478191704 Dev acc: 0.7578125 Train acc: 0.84765625\n","Epoch: 475 Cost: 0.511330664 Dev acc: 0.7734375 Train acc: 0.81640625\n","Epoch: 480 Cost: 0.506306529 Dev acc: 0.76171875 Train acc: 0.84375\n","Epoch: 485 Cost: 0.476500064 Dev acc: 0.74609375 Train acc: 0.84375\n","Epoch: 490 Cost: 0.461194605 Dev acc: 0.71875 Train acc: 0.8046875\n","Epoch: 495 Cost: 0.475455 Dev acc: 0.77734375 Train acc: 0.87890625\n","Epoch: 500 Cost: 0.465999544 Dev acc: 0.75390625 Train acc: 0.85546875\n"]}]},{"cell_type":"markdown","metadata":{"id":"kjBUT-D2spyT"},"source":["# Atribution:\n","Adapted by Oier Lopez de Lacalle, Olatz Perez de Vi√±aspre and Ander Barrena, based on a notebook by Sam Bowman at NYU"]}]}