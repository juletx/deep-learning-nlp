{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6. Transformers_with_TF.ipynb","provenance":[{"file_id":"18vuacGTCCoecm-Qp4wYJEzmoo1eW3wrI","timestamp":1590414998879}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python [default]","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"imyaVBbcIBVq"},"source":["# Lab 6: Sentiment Analysis with fine-tuned Transformers\n","\n","In this lab session, you will fine-tune a **Transformer** based pre-trained language model for sentiment analysis. Transfer learning with large pre-trained language models has been shown to be successful strategy to achieve state-of-the-art performances. In this lab we'll learn how to do transfer learning with large pre-trained neural language models like BERT. \n","\n","More concretely, in this lab session will learn the following:\n","\n","- Deploy and fine-tune transformers from the [Hugging Face library](https://github.com/huggingface/transformers)\n","- Preprocessing data for transformers archicture (word piece tokenizatiin)\n","- Implementation of Transformer-based classifier\n","\n","\n","\n","----\n","\n","## Transfer Learning\n"," \n","Figure below shows how to fine-tune a transformer on a downstream task. Here, the fine-tuning task is sentiment analysis of movie reviews. As learned from theory, we will use the knowledge encoded in the Transformer to learn better our target task. So our sentiment classifier has two main components: 1) the text encoder based on BERT (which doesn't know anything about sentiments, but knows something about English), and 2) the component dedicated to sentiment classification (a simple feed-forward layer). In other words, BERT will generate the sentence embeddings of the input and pass to the classifier layer to the prediction. When we fine-tune our classifier we'll change BERT's parameters as well, and make it to learn specific aspects of the task.\n","\n"," ![](http://ixa2.si.ehu.es/~jibloleo/uc3m_dl4nlp/img/fine-tuning.png)\n","\n","\n","Advantages of these types of architectures and learning:\n","\n","- Unlimited amount of unlabelled text data can be scraped from the web with very little effort to train a large language model.\n","- Transformer is a feed-forward architecture that allows highly parallelized, efficient training on huge datasets, with the objective of simply predicting words based on their context ([check the tutorial on strategy learning for sequence classification](https://colab.research.google.com/drive/1yWaLpCWImXZE2fPV0ZYDdWWI8f52__9A#scrollTo=MGqVkG2-7qfu)).\n","- Although pre-training a language model can be expensive, fine-tuning can be done in a single GPU most of the times, as tipically it requiere few learning epochs. \n"]},{"cell_type":"markdown","metadata":{"id":"yWHDCCqPIBVr"},"source":["## 1. Loading the data\n","We'll use the same data for sentiement analysis used in previous sessions. So first, we need to mount our Drive account in order to get access to the sentiment analysis data ( Stanford Sentiment Treebank)."]},{"cell_type":"code","metadata":{"id":"ihLkOMT9OJDt"},"source":["# Mount Drive files\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xZhrDTCkIBVt"},"source":["import numpy as np\n","import pandas as pd\n","import re\n","\n","import tensorflow as tf\n","from sklearn.utils import shuffle\n","\n","## for replicability of results\n","np.random.seed(1)\n","tf.random.set_seed(2)\n","\n","# Let's do 2-way positive/negative classification instead of 5-way    \n","def load_sst_data(path,\n","                  easy_label_map={0:0, 1:0, 2:None, 3:1, 4:1}):\n","    data = []\n","    with open(path) as f:\n","        for i, line in enumerate(f): \n","            example = {}\n","            example['label'] = easy_label_map[int(line[1])]\n","            if example['label'] is None:\n","                continue\n","            \n","            # Strip out the parse information and the phrase labels---we don't need those here\n","            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n","            example['text'] = text[1:]\n","            data.append(example)\n","    data = pd.DataFrame(data)\n","    return data\n","\n","def pretty_print(example):\n","    print('Label: {}\\nText: {}'.format(example['label'], example['text']))\n","\n","sst_home = 'drive/My Drive/Colab Notebooks/dl4nlp_labs/data/trees/'\n","training_set = load_sst_data(sst_home+'/train.txt')\n","dev_set = load_sst_data(sst_home+'/dev.txt')\n","test_set = load_sst_data(sst_home+'/test.txt')\n","\n","# Shuffle dataset\n","training_set = shuffle(training_set)\n","dev_set = shuffle(dev_set)\n","test_set = shuffle(test_set)\n","\n","# Obtain text and label vectors\n","train_texts = training_set.text\n","train_labels = training_set.label\n","\n","dev_texts = dev_set.text\n","dev_labels = dev_set.label\n","\n","test_texts = test_set.text\n","test_labels = test_set.label\n","\n","\n","print('Training size: {}'.format(len(training_set)))\n","print('Dev size: {}'.format(len(dev_set)))\n","print('Test size: {}'.format(len(test_set)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yZqxg504JNIv"},"source":["## 2. Installing and seting up the Transformers library"]},{"cell_type":"code","metadata":{"id":"8I1LnTA4II71"},"source":["# https://blog.tensorflow.org/2019/11/hugging-face-state-of-art-natural.html\n","!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q4ZJAgAVelDa"},"source":["Once the transformers library is installed, we can use it directly just creating three object of two classes:\n","\n","- **The tokenizer class**: the tokenizer class takes care of converting input  string in tensors of integers which are indices in a model vocabulary. The tokenization varies according to the model, therefore each model has its own tokenizer.\n","\n","- **The model class**: the model class holds the neural network modeling logic itself. When using a TensorFlow model, it inherits from tf.keras.layers. Layer which means it can be used very simply by the Kerasâ€™ fit API or make more complicated stuff. \n","\n","There is also **configuration class** that is also required unless you are not using the default values. With the configuration class we indicate everything related to hyperparaters such as number of layers, dropout and so on. Below is an example of a BERT configuration file, for the pre-trained weights bert-base-cased. \n","\n","```\n","{\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 28996\n","}\n","```\n"]},{"cell_type":"code","metadata":{"id":"G7FLj9htJFJW"},"source":["from transformers import TFBertForSequenceClassification, BertTokenizer\n","\n","model = TFBertForSequenceClassification.from_pretrained(\"bert-base-cased\")\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3U_bMFqMcINb"},"source":["Next we will define two helper function to 1) extract features from the tokenizer (`convert_examples_to_features`) and 2) convert the features to `tf.data.Dataset` object class (`convert_features_to_tf_dataset`). `tf.data.Dataset` is a convinient API that helps managing and iterating in efficient way the input and output data of the model.  For more information you can check the API in tensorflow web page: https://www.tensorflow.org/api_docs/python/tf/data/Dataset."]},{"cell_type":"code","metadata":{"id":"QAolNiqWJmM3"},"source":["from transformers import InputFeatures\n","\n","def convert_examples_to_features(texts, labels):\n","  labels = list(labels)\n","  batch_encoding = tokenizer.batch_encode_plus(texts, max_length=128, pad_to_max_length=True)\n","\n","  features = []\n","  for i in range(len(texts)):\n","      inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n","\n","      feature = InputFeatures(**inputs, label=labels[i])\n","      features.append(feature)\n","\n","  for i, example in enumerate(texts[:5]):\n","      print(\"*** Example ***\")\n","      print(\"text: %s\" % (example))\n","      print(\"features: %s\" % features[i])\n","\n","  return features\n","\n","def convert_features_to_tf_dataset(features):\n","  def gen():\n","      for ex in features:\n","          yield (\n","              {\n","                  \"input_ids\": ex.input_ids,\n","                  \"attention_mask\": ex.attention_mask,\n","                  \"token_type_ids\": ex.token_type_ids,\n","              },\n","              ex.label,\n","          )\n","  dataset = tf.data.Dataset.from_generator(gen, \n","                                           ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n","                                           (\n","                                               {\n","                                                \"input_ids\": tf.TensorShape([None]),\n","                                                \"attention_mask\": tf.TensorShape([None]),\n","                                                \"token_type_ids\": tf.TensorShape([None])\n","                                                },\n","                                            tf.TensorShape([]),\n","                                            ))\n","  return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v2OE1lWbh3P7"},"source":["Let's preprocess the training and development sets. Note that we use the `tf.data.Dataset` API to set the batch size to 32."]},{"cell_type":"code","metadata":{"id":"1r1LPqLIaW4_"},"source":["train_features = convert_examples_to_features(train_texts, train_labels)\n","train_dataset = convert_features_to_tf_dataset(train_features)\n","\n","dev_features = convert_examples_to_features(dev_texts, dev_labels)\n","dev_dataset = convert_features_to_tf_dataset(dev_features)\n","\n","train_dataset = train_dataset.shuffle(100).batch(32)\n","dev_dataset = dev_dataset.batch(32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gZgauVk5jNhd"},"source":["The results of the tokenizer can be seen in the cell below. There are some differences if vary the tokenizer, but most of them provide the following information. \n","\n","- `input_ids`: list of token ids to be fed to a model. Remember that the tokens are subwords, and new tokens are included to indicate sentence separation or ending (`[SEP]`) as well as `[CLS]` token that allow the sentence classification .\n","\n","- `token_type_ids`: list of token type ids to be fed to a model. \n","\n","- `attention_mask`: list of indices specifying which tokens should be attended to by the model\n"]},{"cell_type":"code","metadata":{"id":"aue9cJVx_N65"},"source":["# take one bacth of 32 examples.\n","instance = list(train_dataset.take(1).as_numpy_iterator())\n","print(instance)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GorwYEWOcFWs"},"source":["## 3. Understanding the tokenizer\n","\n","When we preprocess the input text to be fed in BERT like encoder, we tipically make three steps: \n","\n","1. Break words into tokens (subwords). \n","2. Add the special tokens such as `[CLS]` and `[SEP]`. These special tokens are already included in the model's vocabulary, so they have their own token id.\n","3. Substitute the tokens with their corresponding ids. After this step will get the proper shape for BERT. \n","\n","The code cell bellow shows the results of the three steps. \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"HFDfUY1UcWzS"},"source":["sentence1 = \"a visually stunning rumination on love.\"\n","sentence2 = \"There ought to be a directing license, so that Ed Burns can have his revoked.\"\n","\n","# Tokenize sentence\n","sentence1_tokenized = tokenizer.tokenize(sentence1)\n","print('0. INPUT SENTENCE: {}'.format(sentence1))\n","print('1. TOKENIZED SENTENCE: {}'.format(sentence1_tokenized))\n","\n","# Add Special tokens\n","sentence1_tokenized_with_special_tokens = ['[CLS]'] + sentence1_tokenized + ['[SEP]']\n","print('2. ADD [CLS], [SEP]: {}'.format(sentence1_tokenized_with_special_tokens))\n","sentence1_ids = tokenizer.convert_tokens_to_ids(sentence1_tokenized_with_special_tokens)\n","\n","# Convert to ids\n","print('3. SENTENCE IDS: {}'.format(sentence1_ids))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_68U0jC81wFA"},"source":["### Exercise 1:\n","- Can you see what happened to \"rumination\" after the tokenization? \n","- Can you identify the token ids for [CLS] and [SEP]?\n","\n","-----"]},{"cell_type":"markdown","metadata":{"id":"GmPo2_4twYmE"},"source":["The three steps can be done with `encode` or `batch_encode_plus` functions. The first function takes single string and convert is to ids. Then second function is more convinient to preprocess larger input data. It returns all the requiered information (input_ids, token_type_ids, attention_mask, etc) in a python dictionary. "]},{"cell_type":"code","metadata":{"id":"RTYbni402ES0"},"source":["# how tokenize and get the token ids with one funtions\n","sentence1_ids = tokenizer.encode(sentence1, add_special_tokens=True)\n","\n","print('SENTENCE IDS: {}'.format(sentence1_ids))\n","\n","# there are more convinient methods to preprocess the input data. \n","batch_encoding = tokenizer.batch_encode_plus(\n","        [sentence1], max_length=128, pad_to_max_length=True,\n","    )\n","print('ENCODE PLUS: {}'.format(batch_encoding))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xhRQkPRa2KBL"},"source":["### Two sentences as input\n","\n","As you have seen in the theoretical part BERT is a masked language models that learns predicting masked words, and in addition it predicts if next sentence belongs after the first one. That's why BERT's tokenizer is ready to have two sentences as input. This way preprocessing the data is interesting for task like Semantic Textual Similiraty and Natural Language Inference. "]},{"cell_type":"code","metadata":{"id":"V2qZJTkFq8y6"},"source":["# how tokenize and get the token ids with one funtions\n","sentence_pair_ids = tokenizer.encode(text=sentence1, text_pair=sentence2, add_special_tokens=True)\n","\n","\n","# there are more convinient methods to preprocess the input data. \n","batch_encoding = tokenizer.batch_encode_plus(\n","        [(sentence1, sentence2)], max_length=128, pad_to_max_length=True,\n","    )\n","\n","print(\"SENTENCE PAIR IDS: {}\".format(sentence_pair_ids))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fSeX9VzaFOAY"},"source":["### Exercise 2:\n","- From the IDs can you say which ids correspond to the first sentence and which to the second one? "]},{"cell_type":"markdown","metadata":{"id":"D80JtwZbWJ7C"},"source":["## 4. Fine-tune BERT as Sentence Classifier"]},{"cell_type":"code","metadata":{"id":"2vWeYpxdCmtC"},"source":["# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule\n","optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n","\n","# Train and evaluate using tf.keras.Model.fit()\n","history = model.fit(train_dataset, epochs=3, validation_data=dev_dataset)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OMfta2vyKCXt"},"source":["import matplotlib.pyplot as plt\n","\n","# summarize history for accuracy\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'dev'], loc='upper left')\n","plt.show()\n","\n","# summarize history for accuracy\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'dev'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Aeyowf06K11o"},"source":["A nice feature when fine-tuning a large model is that we do not need to train many epochs (the number of epochs depends on the size of the training set). In this case, it looks like just one epoch can be enough. "]},{"cell_type":"markdown","metadata":{"id":"6EsEkO3uG1et"},"source":["Once the model is fine-tuned for sentiment analysis we could evaluate it on the test set. In this we need to tokenized and convert to ids the input too. "]},{"cell_type":"code","metadata":{"id":"4KFUtfXzImAY"},"source":["test_features = convert_examples_to_features(test_texts, test_labels)\n","test_dataset = convert_features_to_tf_dataset(test_features)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j6EjtjScENVw"},"source":["test_dataset = test_dataset.batch(32)\n","instances = list(test_dataset.take(1).as_numpy_iterator())\n","instances"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3iEUS77CIzgA"},"source":["model.evaluate(test_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T4d1Hs-Dc4JE"},"source":["## 5. Fine-tuning BERT for NLI\n","\n","Now that you know how to fine-tune BERT model for sentence classification. We could do something similar to fine-tune BERT to Natural Language Inference task. Recall that NLI consist in determining whether a natural language hypothesis can justifiably be inferred from a natural language premise hus given a pair of premise and hypothesis texts, the task is to classify them into three categories: entailment, contradiction, and neutral.\n","\n","There are many ways to approach the task, but one way is to encode the premise and hypothesis at the same time as shown in the figure below. We separate premise and hypothesis sentences with `[SEP]` and use `[CLS]` token to perform the three-way classification task. \n","\n","\n"," ![](http://ixa2.si.ehu.es/~jibloleo/uc3m_dl4nlp/img/bert_nli.png)\n","\n","----\n","\n"]},{"cell_type":"markdown","metadata":{"id":"d50LA3brYajM"},"source":["Let's start loading the data for NLI. We'll use the same function used in the previous lab."]},{"cell_type":"code","metadata":{"id":"ujAiEm7BL-ZZ"},"source":["import re\n","import random\n","import json\n","import bz2\n","import pandas as pd\n","\n","def load_snli_data(path, label_map = {\"entailment\": 0, \"neutral\": 1,\"contradiction\": 2}):\n","    data = []\n","    with bz2.open(path) as f:\n","        for i, line in enumerate(f):\n","            line = line.decode('utf-8')\n","            if i >= 1000000:  # Edit to use less data for debugging. set to 1000000 for testing.\n","                break\n","            json_line = json.loads(line)\n","            if json_line[\"gold_label\"] not in label_map:\n","                continue\n","            loaded_example = {}\n","            loaded_example[\"label\"] = label_map[json_line[\"gold_label\"]]\n","            loaded_example[\"sentence1\"] = json_line[\"sentence1\"]\n","            loaded_example[\"sentence2\"] = json_line[\"sentence2\"]\n","            data.append(loaded_example)\n","    data = pd.DataFrame(data)\n","    return data\n","\n","snli_home = 'drive/My Drive/Colab Notebooks/2020-2021_labs/data/snli/'  \n","training_set = load_snli_data(snli_home + '/snli_1.0_train.jsonl.bz2')\n","dev_set = load_snli_data(snli_home + '/snli_1.0_dev.jsonl.bz2')\n","test_set = load_snli_data(snli_home + '/snli_1.0_test.jsonl.bz2')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ksshZRh0lx8K"},"source":["We are going to reduce the dataset to speed up the experiments in the lab session, but feel free to use the whole dataset after the lab is completed. Fine-tuning with the whole training set can take many hours, so maybe it is a good idea to run just for one or two epochs.\n"]},{"cell_type":"code","metadata":{"id":"XHHI8N29Vtti"},"source":["training_set = training_set.head(5000)\n","dev_set = dev_set.head(1000)\n","\n","# check if the dataset is still balanced\n","print('Training labels:')\n","print(training_set.label.value_counts())\n","\n","print('Dev labels:')\n","print(dev_set.label.value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OiCVwyaGnFY4"},"source":["Note that when we load the pretrained model (`from_pretrained`) we need to indicate number of labels that contains the dataset. This can be done with the argument `num_labels`."]},{"cell_type":"code","metadata":{"id":"zJdYaECRXN3Y"},"source":["from transformers import TFBertForSequenceClassification, BertTokenizer\n","\n","nli_model = TFBertForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=3)\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UX3s5Yp1rm0a"},"source":["As you can see, the `config` has slightly changed compared to first model. Now it contains the information of the labels (we don't care about the actual id2label mapping for now)."]},{"cell_type":"code","metadata":{"id":"fssORtQtfqwo"},"source":["nli_model.config"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pIFdYyYkTRiF"},"source":["### Exercise 3:\n","For the final exercise you have to re-write the function `convert_examples_to_features` so it is able to extract the token ids of premise and hypothesis in one go. Most of the function is already done for you. You just need to fill a small part to complete it."]},{"cell_type":"code","metadata":{"id":"3zFQ2QNwXLoB"},"source":["import tensorflow as tf\n","from transformers import InputFeatures\n","\n","def convert_nli_examples_to_features(premises, hypotheses, labels):\n","  labels = list(labels)\n","\n","  # TODO: You need to create the code that fills the batch_encoding object. \n","  # Hint: Iterate over premises and hypothesis to get tuples of \n","  #       premises and hypotheses.\n","  batch_encoding = \n","\n","  features = []\n","  for i in range(len(premises)):\n","      inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n","\n","      feature = InputFeatures(**inputs, label=labels[i])\n","      features.append(feature)\n","\n","  for i in range(5):\n","      print(\"*** Example ***\")\n","      print(\"premise: %s\" % (premises[i]))\n","      print(\"hypothesis: %s\" % (hypotheses[i]))\n","      print(\"features: %s\" % features[i])\n","\n","  return features"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sbdPeTBpryJl"},"source":["Once you complete the exercise you can see convert dataset to features first and to `tf.dataset` after that."]},{"cell_type":"code","metadata":{"id":"q6BqLltF32ZU"},"source":["train_features = convert_nli_examples_to_features(training_set.sentence1, training_set.sentence2, training_set.label)\n","dev_features = convert_nli_examples_to_features(dev_set.sentence1, dev_set.sentence2, dev_set.label)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vo2EphiZjeNv"},"source":["train_dataset = convert_features_to_tf_dataset(train_features)\n","dev_dataset = convert_features_to_tf_dataset(dev_features)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ObN05ITRsWKe"},"source":["We use `tf.dataset` API to shuffle the training set and set the batch size. We decrease the batch size to 16 in order to avoid problems with the memory. "]},{"cell_type":"code","metadata":{"id":"WECDX7kMctlT"},"source":["train_dataset = train_dataset.shuffle(100).batch(16)\n","dev_dataset = dev_dataset.batch(32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fxIlEzupXhG1"},"source":["# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule\n","optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","nli_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n","\n","# Train and evaluate using tf.keras.Model.fit()\n","history = nli_model.fit(train_dataset, epochs=3, validation_data=dev_dataset)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g_tYW0T8d_Qa"},"source":["nli_model.predict(dev_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dIgw-1Qot731"},"source":["As in the previous example, we can process the test set in the same way, and evaluate it directly. "]},{"cell_type":"code","metadata":{"id":"NXrIqumjt7XP"},"source":["test_features = convert_nli_examples_to_features(test_set.sentence1,\n","                                                 test_set.sentence2,\n","                                                 test_set.label)\n","test_dataset = convert_features_to_tf_dataset(test_features)\n","test_dataset = test_dataset.batch(32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TrwjUrcKulfv"},"source":["nli_model.evaluate(test_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8qSPPPKqQxCS"},"source":["### Exercise 4\n","If you have time increase the size of the trainig set. Try with 15,000 training examples, and see what happens in the test set. "]},{"cell_type":"markdown","metadata":{"id":"kjBUT-D2spyT"},"source":["# Atribution:\n","Adapted by Oier Lopez de Lacalle and Ander Barrena"]}]}